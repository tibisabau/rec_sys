{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d74f92-57dd-49d5-8e6d-b5ab8c7ee75c",
   "metadata": {},
   "source": [
    "# DSAIT4335 Recommender Systems\n",
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18281d7-d2fc-4a67-9bc9-21d25bad6cfc",
   "metadata": {},
   "source": [
    "In this project, you will work to build different recommendation models and evaluate the effectiveness of these models through offline experiments. The dataset used for the experiments is **MovieLens100K**, a movie recommendation dataset collected by GroupLens: https://grouplens.org/datasets/movielens/100k/. For more details, check the project description on Brightspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cbc07f-b579-4f9b-85b5-dc43c2d7ce48",
   "metadata": {},
   "source": [
    "# Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944993d6-8983-46cf-880f-753f65975811",
   "metadata": {},
   "source": [
    "The MovieLens100K is already splitted into 80% training and 20% test sets. Along with training and test sets, movies metadata as content information is also provided.\n",
    "\n",
    "**Expected file structure** for this assignment:   \n",
    "   \n",
    "   ```\n",
    "   RecSysProject/\n",
    "   ├── training.txt\n",
    "   ├── test.txt\n",
    "   ├── movies.txt\n",
    "   └── codes.ipynb\n",
    "   ```\n",
    "\n",
    "**Note:** Be sure to run all cells in each section sequentially, so that intermediate variables and packages are properly carried over to subsequent cells.\n",
    "\n",
    "**Note** Be sure to run all cells such that the submitted file contains the output of each cell.\n",
    "\n",
    "**Note** Feel free to add cells if you need more for answering a question.\n",
    "\n",
    "**Submission:** Answer all the questions in this jupyter-notebook file. Submit this jupyter-notebook file (your answers included) to Brightspace. Change the name of this jupyter-notebook file to your group number: example, group10 -> 10.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977945fa-a202-49c4-a41d-12ada7b437da",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "302a2b5b-fdf1-41c8-b6a6-bc1cd453425b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: filelock in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: requests in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vcs\\desktop\\uni\\msc_cs\\y2\\q1\\rs\\rec_sys\\venv\\lib\\site-packages (from requests->transformers) (3.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\vcs\\Desktop\\UNI\\MSc_CS\\Y2\\Q1\\RS\\rec_sys\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch  \n",
    "\n",
    "# you can refer https://huggingface.co/docs/transformers/en/model_doc/bert for various versions of the pre-trained model BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e3b0e3c-74d9-436b-b676-56bc2a8528a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the status of BERT installation:\n",
      "BERT libraries loaded successfully!\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# For BERT embeddings (install: pip install transformers torch)\n",
    "print(\"Check the status of BERT installation:\")\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch\n",
    "    BERT_AVAILABLE = True\n",
    "    print(\"BERT libraries loaded successfully!\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "except ImportError:\n",
    "    BERT_AVAILABLE = False\n",
    "    print(\"BERT libraries not available. Install with: pip install transformers torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8055513b-9f14-4d18-b32a-7c2ee386e6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import cosine, correlation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "import re\n",
    "import time, math\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd1eff-8e8b-4f65-b92a-778107a256cc",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d220e9dc-3a45-4d25-b214-23d6555cb34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        1        1       5\n",
       "1        1        2       3\n",
       "2        1        3       4\n",
       "3        1        4       3\n",
       "4        1        5       3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training data: (80000, 4)\n",
      "--------------------------------\n",
      "The test data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        1        6       5\n",
       "1        1       10       3\n",
       "2        1       12       5\n",
       "3        1       14       5\n",
       "4        1       17       3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the test data: (20000, 4)\n"
     ]
    }
   ],
   "source": [
    "# loading the training set and test set\n",
    "columns_name=['user_id','item_id','rating','timestamp']\n",
    "train_data = pd.read_csv('training.txt', sep='\\t', names=columns_name)\n",
    "test_data = pd.read_csv('test.txt', sep='\\t', names=columns_name)\n",
    "\n",
    "print(f'The training data:')\n",
    "display(train_data[['user_id','item_id','rating']].head())\n",
    "print(f'The shape of the training data: {train_data.shape}')\n",
    "print('--------------------------------')\n",
    "print(f'The test data:')\n",
    "display(test_data[['user_id','item_id','rating']].head())\n",
    "print(f'The shape of the test data: {test_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e84160d-ef0e-4e58-8ace-307fb8cd5a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation, Children's, Comedy</td>\n",
       "      <td>A group of sentient toys, who pretend to be li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "      <td>Action, Adventure, Thriller</td>\n",
       "      <td>In 1986, MI6 agents James Bond and Alec Trevel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>On New Year's Eve, bellhop Sam (Marc Lawrence)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Get Shorty (1995)</td>\n",
       "      <td>Action, Comedy, Drama</td>\n",
       "      <td>Chili Palmer is a Miami-based loan shark and m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Copycat (1995)</td>\n",
       "      <td>Crime, Drama, Thriller</td>\n",
       "      <td>After giving a guest lecture on criminal psych...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id              title                         genres  \\\n",
       "0        1   Toy Story (1995)  Animation, Children's, Comedy   \n",
       "1        2   GoldenEye (1995)    Action, Adventure, Thriller   \n",
       "2        3  Four Rooms (1995)                       Thriller   \n",
       "3        4  Get Shorty (1995)          Action, Comedy, Drama   \n",
       "4        5     Copycat (1995)         Crime, Drama, Thriller   \n",
       "\n",
       "                                         description  \n",
       "0  A group of sentient toys, who pretend to be li...  \n",
       "1  In 1986, MI6 agents James Bond and Alec Trevel...  \n",
       "2  On New Year's Eve, bellhop Sam (Marc Lawrence)...  \n",
       "3  Chili Palmer is a Miami-based loan shark and m...  \n",
       "4  After giving a guest lecture on criminal psych...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv('movies.txt',names=['item_id','title','genres','description'],sep='\\t')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d50b57f-b07a-49b0-ad8c-31566a355cc7",
   "metadata": {},
   "source": [
    "# Task 1) Implementation of different recommendation models as well as a hybrid model combining those recommendation models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc2f3d7",
   "metadata": {},
   "source": [
    "### A. Content Based Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5dadd",
   "metadata": {},
   "source": [
    "#### A.1 Deriving Content Representation with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f55e25ab-353d-4a7c-bf68-9deb201bfbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_embeddings(content):\n",
    "    \"\"\"\n",
    "    Generate BERT embeddings for movie content.\n",
    "\n",
    "    Args:\n",
    "        content: Content of items\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: BERT embeddings matrix\n",
    "    \"\"\"\n",
    "    if not BERT_AVAILABLE:\n",
    "        print(\"BERT libraries not available. Install with: pip install transformers torch\")\n",
    "        return None\n",
    "\n",
    "    if content is None:\n",
    "        return None\n",
    "\n",
    "    if isinstance(content, pd.Series):\n",
    "        content = content.fillna(\"\").astype(str).tolist()\n",
    "    elif isinstance(content, np.ndarray):\n",
    "        content = content.astype(str).tolist()\n",
    "\n",
    "    model_name = 'distilbert-base-uncased'\n",
    "\n",
    "    print(f\"Loading BERT model: {model_name}\")\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Set device (GPU if available)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using cuda or cpu: {device}\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Generate embeddings in batches\n",
    "    batch_size = 32  # Adjust based on available memory\n",
    "    emb = []\n",
    "\n",
    "    for i in range(0, len(content), batch_size):\n",
    "        if i % (batch_size * 10) == 0:\n",
    "            print(f\"Processing batch {i//batch_size + 1}/{len(content)//batch_size + 1}\")\n",
    "\n",
    "        batch_texts = content[i:i + batch_size]\n",
    "\n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # Use [CLS] token embedding (first token)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            emb.extend(batch_embeddings)\n",
    "\n",
    "    emb = np.array(emb)\n",
    "\n",
    "    print(f\"BERT embeddings generated: {emb.shape}\")\n",
    "    print(f\"Embedding dimension: {emb.shape[1]}\")\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad8246",
   "metadata": {},
   "source": [
    "#### A.2 Deriving the representation of items for three types of content: \n",
    "- title + genres \n",
    "- description \n",
    "- title + genres + description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25db7c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model: distilbert-base-uncased\n",
      "Using cuda or cpu: cpu\n",
      "Using device: cpu\n",
      "Processing batch 1/53\n",
      "Processing batch 11/53\n",
      "Processing batch 21/53\n",
      "Processing batch 31/53\n",
      "Processing batch 41/53\n",
      "Processing batch 51/53\n",
      "BERT embeddings generated: (1682, 768)\n",
      "Embedding dimension: 768\n",
      "Loading BERT model: distilbert-base-uncased\n",
      "Using cuda or cpu: cpu\n",
      "Using device: cpu\n",
      "Processing batch 1/53\n",
      "Processing batch 11/53\n",
      "Processing batch 21/53\n",
      "Processing batch 31/53\n",
      "Processing batch 41/53\n",
      "Processing batch 51/53\n",
      "BERT embeddings generated: (1682, 768)\n",
      "Embedding dimension: 768\n",
      "Loading BERT model: distilbert-base-uncased\n",
      "Using cuda or cpu: cpu\n",
      "Using device: cpu\n",
      "Processing batch 1/53\n",
      "Processing batch 11/53\n",
      "Processing batch 21/53\n",
      "Processing batch 31/53\n",
      "Processing batch 41/53\n",
      "Processing batch 51/53\n",
      "BERT embeddings generated: (1682, 768)\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# Implement code to derive the content representation for title and genres. Concatenate the two content as: title + ' ' + genres\n",
    "item_emb_titlegenres = None\n",
    "############# Your code here ############\n",
    "title_genres_text = (movies['title'].astype(str) + ' ' + movies['genres'].astype(str)).tolist()\n",
    "item_emb_titlegenres = create_bert_embeddings(title_genres_text)\n",
    "#########################################\n",
    "\n",
    "# Implement code to derive the content representation for description.\n",
    "item_emb_description = None\n",
    "############# Your code here ############\n",
    "description_text = movies['description'].fillna(\"\").astype(str).tolist()\n",
    "item_emb_description = create_bert_embeddings(description_text)\n",
    "#########################################\n",
    "\n",
    "# Implement code to derive the content representation for title, genres, and description. Concatenate the two content as: title + ' ' + genres + '' + description\n",
    "item_emb_full = None\n",
    "############# Your code here ############\n",
    "full_text = (movies['title'].astype(str) + ' ' + \n",
    "             movies['genres'].astype(str) + ' ' + \n",
    "             movies['description'].fillna(\"\").astype(str)).tolist()\n",
    "item_emb_full = create_bert_embeddings(full_text)\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07da03b",
   "metadata": {},
   "source": [
    "#### A.3 Get Item Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38e402f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_emb(item_id, content_type):\n",
    "    # Implement the function that given content type (title+genres, description, or title+genres+description) returns the embedding derived for the corresponding item_id. \n",
    "    # Hint1: keep in mind that item_id in the data starts from 1, but in the embedding variable it starts from 0, e.g., item_id 100 corresponds to index 99 in embedding variable..\n",
    "    # Hint2: use if-else conditions to return the embedding for the requested content types.\n",
    "    # Hint3: use the global variables (embeddings) already computed in previous cells.\n",
    "\n",
    "    emb = None\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    idx = int(item_id) - 1\n",
    "    if content_type == 'title_genres':\n",
    "        emb = item_emb_titlegenres[idx]\n",
    "    elif content_type == 'description':\n",
    "        emb = item_emb_description[idx]\n",
    "    else:\n",
    "        emb = item_emb_full[idx]\n",
    "    #########################################\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca727f",
   "metadata": {},
   "source": [
    "#### A.4 User Profile Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b143580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interacted_items_embs_rating(train_data, user_id, content_type):\n",
    "    # Implement the function that given content type (title+genres, description, or title+genres+description) returns the embeddings and ratings of interacted items by user_id=100. \n",
    "    # Hint1: use train_data to retrieve the item_ids that target user (user_id=100 in this example) interacted, then pass these item_ids to function previously implemented to retrieve the embeddings and ratings.\n",
    "\n",
    "    embs, ratings = [], []\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    user_interactions = train_data[train_data['user_id'] == user_id]\n",
    "\n",
    "    for _, row in user_interactions.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        rating = row['rating']\n",
    "        \n",
    "        # Get embedding for this item\n",
    "        emb = get_item_emb(item_id, content_type)\n",
    "        \n",
    "        if emb is not None:\n",
    "            embs.append(emb)\n",
    "            ratings.append(rating)\n",
    "    #########################################\n",
    "\n",
    "    return embs, ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca90889",
   "metadata": {},
   "source": [
    "#### A.5 Deriving the representation for a user using the following aggregation methods:\n",
    "1. **avg:** Average representation of interacted item \n",
    "2. **weighted_avg:** Weighted average representation of interacted item using rating values\n",
    "3. **avg_pos:** Average representation of positively interacted item (ratings >= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fed87724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_emb(train_data, user_id, content_type, aggregation_method):\n",
    "    # Implement the function that given content type (title+genres, description, or title+genres+description) and aggregation method (avg, weighted_avg, avg_pos) returns the representation of a user. \n",
    "    # Hint1: use the previsouly implemented items for retrieving ratings and representation of interacted items by a user.\n",
    "\n",
    "    emb = []\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    embs, ratings = get_interacted_items_embs_rating(train_data, user_id, content_type)\n",
    "\n",
    "    embs = np.array(embs)\n",
    "    ratings = np.array(ratings)\n",
    "\n",
    "    if aggregation_method == 'avg':\n",
    "        emb = np.mean(embs, axis=0)\n",
    "\n",
    "    elif aggregation_method == 'weighted_avg':\n",
    "        weights = ratings / ratings.sum()\n",
    "        emb = np.average(embs, axis=0, weights=weights)\n",
    "\n",
    "    else: \n",
    "        mask = ratings >= 4\n",
    "        if mask.sum() == 0:\n",
    "            return None  \n",
    "        emb = np.mean(embs[mask], axis=0)\n",
    "    #########################################\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3ad12",
   "metadata": {},
   "source": [
    "#### A.6 Predict Score for User-Item Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25a120c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted score for user_id=100 and item_id=266 for content type full and aggregation method avg:\n",
      "142.68702697753906\n"
     ]
    }
   ],
   "source": [
    "def get_user_item_prediction(train_data, user_id, item_id, content_type, aggregation_method):\n",
    "    # Implement the function that given content type and aggregation method returns the predicted rating for a user-item pair. \n",
    "    # Hint1: use the previsouly implemented functions for retrieving the embeddings and then compute the dot product of user and item embeddings.\n",
    "\n",
    "    pred_rating = 0.0\n",
    "    \n",
    "    ############# Your code here ############\n",
    "    user_emb = get_user_emb(train_data, user_id, content_type, aggregation_method)\n",
    "    item_emb = get_item_emb(item_id, content_type)    \n",
    "    pred_rating = float(np.dot(user_emb, item_emb))\n",
    "    #########################################\n",
    "\n",
    "    return pred_rating\n",
    "    \n",
    "user_id, item_id = 100, 266\n",
    "content_type, aggregation_method = 'full', 'avg' # alternatives are content_type={title_genres,description,full} and aggregation_method={avg,weighted_avg,avg_pos}\n",
    "print('Predicted score for user_id=100 and item_id=266 for content type '+content_type+' and aggregation method '+aggregation_method+':')\n",
    "print(get_user_item_prediction(train_data, user_id, item_id, content_type, aggregation_method))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a42786",
   "metadata": {},
   "source": [
    "### B. User-based neighborhood method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b060b210",
   "metadata": {},
   "source": [
    "#### B.1 Implementing a function that computes the Pearson Correlation between two users. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa01a3",
   "metadata": {},
   "source": [
    "The **Pearson correlation coefficient** between two users \\(x\\) and \\(y\\) is defined as:\n",
    "\n",
    "$$\n",
    "r_{xy} = \\frac{\\sum_{i \\in I_{xy}} (x_i - \\bar{x})(y_i - \\bar{y})}\n",
    "              {\\sqrt{\\sum_{i \\in I_{xy}} (x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i \\in I_{xy}} (y_i - \\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- $I_{xy}$ = set of items rated by both users  \n",
    "- $x_i$, $y_i$ = ratings of users \\(x\\) and \\(y\\) on item \\(i\\)  \n",
    "- $\\bar{x}$, $\\bar{y}$ = mean ratings of users \\(x\\) and \\(y\\) on the common items  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7105c95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation between users 1 and 2 is 0.2697\n"
     ]
    }
   ],
   "source": [
    "def pearson_correlation(user1_ratings: pd.Series, user2_ratings: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Compute Pearson correlation coefficient between two users' rating vectors.\n",
    "    \n",
    "    user1_ratings, user2_ratings: Pandas Series indexed by item IDs. They may contain NaN for unrated items.\n",
    "    Returns: float (correlation between -1 and 1). Returns 0 if not enough data.\n",
    "    \"\"\"\n",
    "    # Find the common items both users have rated\n",
    "    common_items = user1_ratings.index.intersection(user2_ratings.index)\n",
    "    \n",
    "    if len(common_items) < 2:\n",
    "        # Not enough common ratings to compute correlation\n",
    "        return 0.0\n",
    "\n",
    "    # Extract the ratings for the common items\n",
    "    u1 = user1_ratings.loc[common_items]\n",
    "    u2 = user2_ratings.loc[common_items]\n",
    "\n",
    "    # Drop any NaNs just in case\n",
    "    valid = u1.notna() & u2.notna()\n",
    "    u1 = u1[valid]\n",
    "    u2 = u2[valid]\n",
    "\n",
    "    if len(u1) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute mean-centered ratings\n",
    "    u1_mean = u1.mean()\n",
    "    u2_mean = u2.mean()\n",
    "\n",
    "    numerator = ((u1 - u1_mean) * (u2 - u2_mean)).sum()\n",
    "    denominator = ((u1 - u1_mean).pow(2).sum() ** 0.5) * ((u2 - u2_mean).pow(2).sum() ** 0.5)\n",
    "\n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "\n",
    "    result = numerator / denominator\n",
    "    return result\n",
    "\n",
    "user1, user2 = 1, 2\n",
    "user1_ratings = train_data[train_data['user_id'] == user1].set_index('item_id')['rating']\n",
    "user2_ratings = train_data[train_data['user_id'] == user2].set_index('item_id')['rating']\n",
    "print(f\"Pearson Correlation between users {user1} and {user2} is {pearson_correlation(user1_ratings, user2_ratings):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92aaa0",
   "metadata": {},
   "source": [
    "#### B.2 User Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e7d26be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity matrix creation started! This may take around 5-10 minutes...\n",
      "Running time: 780.7737 seconds\n"
     ]
    }
   ],
   "source": [
    "def compute_user_similarity_matrix(train_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute user-user similarity matrix using Pearson correlation.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_data: pd.DataFrame with columns ['user_id', 'item_id', 'rating']\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: user-user similarity matrix (rows & cols = user_ids)\n",
    "    \"\"\"\n",
    "    users = train_data['user_id'].unique()\n",
    "    user_similarity_matrix = pd.DataFrame(np.zeros((len(users), len(users))), index=users, columns=users)\n",
    "    \n",
    "    # Create a user-item rating pivot for faster lookups\n",
    "    user_ratings = {user: train_data[train_data['user_id'] == user].set_index('item_id')['rating']\n",
    "                    for user in users}\n",
    "    \n",
    "    # Compute pairwise Pearson correlations\n",
    "    for i, user1 in enumerate(users):\n",
    "        for j, user2 in enumerate(users):\n",
    "            if i > j:  # Use symmetry to avoid redundant computation\n",
    "                user_similarity_matrix.loc[user1, user2] = user_similarity_matrix.loc[user2, user1]\n",
    "                continue\n",
    "            if user1 == user2:\n",
    "                user_similarity_matrix.loc[user1, user2] = 1.0\n",
    "                continue\n",
    "            \n",
    "            sim = pearson_correlation(user_ratings[user1], user_ratings[user2])\n",
    "            user_similarity_matrix.loc[user1, user2] = sim\n",
    "            user_similarity_matrix.loc[user2, user1] = sim  # symmetric matrix\n",
    "    \n",
    "    return user_similarity_matrix\n",
    "\n",
    "start_time = time.time()\n",
    "print(f'Similarity matrix creation started! This may take around 5-10 minutes...')\n",
    "user_similarity_matrix = compute_user_similarity_matrix(train_data)  \n",
    "end_time = time.time()\n",
    "print(f'Running time: {end_time - start_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234988be",
   "metadata": {},
   "source": [
    "#### B.3 Implementing a function that returns k most similar users along with the similarity values to a target user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bc43616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbors of user 1 are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(289, np.float64(1.0000000000000002)),\n",
       " (656, np.float64(1.0000000000000002)),\n",
       " (926, np.float64(1.0000000000000002)),\n",
       " (29, np.float64(1.0000000000000002)),\n",
       " (46, np.float64(1.0000000000000002)),\n",
       " (920, np.float64(1.0000000000000002)),\n",
       " (485, np.float64(1.0)),\n",
       " (898, np.float64(1.0)),\n",
       " (229, np.float64(1.0)),\n",
       " (824, np.float64(1.0))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_k_user_neighbors(user_similarity_matrix: pd.DataFrame, target_user, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve top-k most similar users to the target user.\n",
    "\n",
    "    Parameters:\n",
    "    - user_similarity_matrix: pd.DataFrame, user-user similarity values (indexed by user IDs)\n",
    "    - target_user: user ID for whom we want neighbors\n",
    "    - k: number of neighbors to retrieve\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples: [(neighbor_user_id, similarity), ...] sorted by similarity descending\n",
    "    \"\"\"\n",
    "    if target_user not in user_similarity_matrix.index:\n",
    "        return []\n",
    "\n",
    "    # Extract similarity scores for the target user\n",
    "    user_similarities = user_similarity_matrix.loc[target_user]\n",
    "\n",
    "    # Drop self-similarity (user with themselves)\n",
    "    user_similarities = user_similarities.drop(target_user, errors='ignore')\n",
    "\n",
    "    # Sort users by similarity (descending order)\n",
    "    sorted_neighbors = user_similarities.sort_values(ascending=False)\n",
    "\n",
    "    # Select top-k most similar users\n",
    "    top_k = sorted_neighbors.head(k)\n",
    "\n",
    "    # Convert to list of tuples\n",
    "    top_k_neighbors = list(zip(top_k.index, top_k.values))\n",
    "\n",
    "    return top_k_neighbors\n",
    "\n",
    "target_user, k = 1, 10\n",
    "print(f\"Neighbors of user {target_user} are:\")\n",
    "get_k_user_neighbors(user_similarity_matrix, target_user, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8ee23c",
   "metadata": {},
   "source": [
    "#### B.4 Implementing a function that predicts the rating for a target user might give to a target item using user-user similarity matrix and the following equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf09cb5",
   "metadata": {},
   "source": [
    "The **predicted rating** for a target user \\(u\\) on item \\(i\\) using mean-centered user-based collaborative filtering is:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{u,i} = \\bar{r}_u + \\frac{\\sum_{v \\in N(u)} s(u,v) \\cdot (r_{v,i} - \\bar{r}_v)}\n",
    "                             {\\sum_{v \\in N(u)} |s(u,v)|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\hat{r}_{u,i}$ = predicted rating for user \\(u\\) on item \\(i\\)  \n",
    "- $\\bar{r}_u$ = mean rating of the target user \\(u\\)  \n",
    "- $N(u)$ = set of top-\\(k\\) neighbors of user \\(u\\) who have rated item \\(i\\)  \n",
    "- $s(u,v)$ = similarity between users \\(u\\) and \\(v\\)  \n",
    "- $r_{v,i}$ = rating of neighbor \\(v\\) on item \\(i\\)  \n",
    "- $\\bar{r}_v$ = mean rating of neighbor \\(v\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbca8f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual rating for user 1 and item 17 is 3. The predicted rating by user-based CF for user 1 and item 17 is 3.4832\n"
     ]
    }
   ],
   "source": [
    "def predict_rating_user_based(train_data: pd.DataFrame, user_similarity_matrix: pd.DataFrame, target_user, target_item, k=5):\n",
    "    \"\"\"\n",
    "    Predict rating for target_user and target_item using mean-centered user-based CF.\n",
    "\n",
    "    Parameters:\n",
    "    - train_data: pd.DataFrame with columns ['user_id', 'item_id', 'rating']\n",
    "    - user_similarity_matrix: pd.DataFrame of user-user similarities\n",
    "    - target_user: user ID\n",
    "    - target_item: item ID\n",
    "    - k: number of neighbors to consider\n",
    "\n",
    "    Returns:\n",
    "    - float: predicted rating, or np.nan if not possible\n",
    "    \"\"\"\n",
    "    # Check if target_user and target_item exist in data\n",
    "    if target_user not in user_similarity_matrix.index:\n",
    "        return np.nan\n",
    "    if target_item not in train_data['item_id'].unique():\n",
    "        return np.nan\n",
    "\n",
    "    # Get mean rating of the target user\n",
    "    target_user_ratings = train_data[train_data['user_id'] == target_user]['rating']\n",
    "    if target_user_ratings.empty:\n",
    "        return np.nan\n",
    "    target_user_mean = target_user_ratings.mean()\n",
    "\n",
    "    # Get all users who rated the target item\n",
    "    item_raters = train_data[train_data['item_id'] == target_item]\n",
    "    if item_raters.empty:\n",
    "        return np.nan\n",
    "\n",
    "    # Get similarities between target_user and those raters\n",
    "    similarities = user_similarity_matrix.loc[target_user, item_raters['user_id']]\n",
    "    \n",
    "    # Combine raters' data and similarity\n",
    "    item_raters = item_raters.copy()\n",
    "    item_raters['similarity'] = similarities.values\n",
    "\n",
    "    # Keep only top-k most similar users\n",
    "    item_raters = item_raters.sort_values(by='similarity', ascending=False).head(k)\n",
    "\n",
    "    # Remove users with non-positive similarity (optional but common)\n",
    "    item_raters = item_raters[item_raters['similarity'] > 0]\n",
    "\n",
    "    if item_raters.empty:\n",
    "        return np.nan\n",
    "\n",
    "    # Compute mean rating for each neighbor\n",
    "    neighbor_means = (\n",
    "        train_data.groupby('user_id')['rating']\n",
    "        .mean()\n",
    "        .reindex(item_raters['user_id'])\n",
    "    )\n",
    "\n",
    "    # Mean-centered ratings: (r_ui - mean_u)\n",
    "    item_raters['mean_centered'] = item_raters['rating'] - neighbor_means.values\n",
    "\n",
    "    numerator = (item_raters['similarity'] * item_raters['mean_centered']).sum()\n",
    "    denominator = item_raters['similarity'].abs().sum()\n",
    "\n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # Predicted rating using mean-centering\n",
    "    predicted_rating = target_user_mean + numerator / denominator\n",
    "    \n",
    "    # Clamp to valid rating scale\n",
    "    return predicted_rating\n",
    "\n",
    "target_user, target_item, k = 1, 17, 50\n",
    "print(f\"The actual rating for user {target_user} and item {target_item} is 3. The predicted rating by user-based CF for user {target_user} and item {target_item} is {predict_rating_user_based(train_data, user_similarity_matrix, target_user, target_item, k):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458adf7",
   "metadata": {},
   "source": [
    "#### B.5 Implementing a function that generates top-K recommendation list for a target user using user-based CF method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d067eee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 recommendations for user 1:\n",
      "Item 12: 5.0000\n",
      "Item 129: 5.0000\n",
      "Item 97: 5.0000\n",
      "Item 201: 5.0000\n",
      "Item 385: 5.0000\n",
      "Item 371: 5.0000\n",
      "Item 309: 5.0000\n",
      "Item 514: 5.0000\n",
      "Item 504: 5.0000\n",
      "Item 435: 5.0000\n",
      "Running time: 0.1420 seconds\n"
     ]
    }
   ],
   "source": [
    "def recommend_topk_user_based(train_data, user_similarity_matrix, target_user, k=30, top_n=10):\n",
    "    \"\"\"\n",
    "    Fast Top-K recommendations for a target user using User-based CF.\n",
    "    \n",
    "    Args:\n",
    "        train_data (pd.DataFrame): ratings data with [user_id, item_id, rating]\n",
    "        user_similarity_matrix (pd.DataFrame): user-user similarity matrix\n",
    "        target_user (int): ID of the target user\n",
    "        k (int): number of most similar users to consider\n",
    "        top_n (int): number of recommendations to return\n",
    "    \n",
    "    Returns:\n",
    "        list of (item_id, predicted_score), sorted by score descending\n",
    "    \"\"\"\n",
    "\n",
    "    # If user not in similarity matrix, return empty\n",
    "    if target_user not in user_similarity_matrix.index:\n",
    "        return []\n",
    "\n",
    "    # Get top-k similar users (excluding the target itself)\n",
    "    sim_scores = user_similarity_matrix.loc[target_user].drop(target_user, errors='ignore')\n",
    "    top_k_users = sim_scores.nlargest(k).index\n",
    "    top_k_sims = sim_scores.loc[top_k_users].values\n",
    "\n",
    "    # Get ratings of top-k users\n",
    "    neighbor_ratings = train_data[train_data['user_id'].isin(top_k_users)]\n",
    "\n",
    "    # Compute weighted mean rating per item\n",
    "    # Merge in similarity scores for each neighbor\n",
    "    neighbor_ratings = neighbor_ratings.merge(\n",
    "        pd.DataFrame({'user_id': top_k_users, 'sim': top_k_sims}),\n",
    "        on='user_id'\n",
    "    )\n",
    "\n",
    "    # Weighted sum for each item\n",
    "    item_scores = neighbor_ratings.groupby('item_id').apply(\n",
    "        lambda x: np.sum(x['rating'] * x['sim']) / np.sum(np.abs(x['sim']))\n",
    "    )\n",
    "\n",
    "    # Remove items already rated by the target user\n",
    "    user_rated_items = set(train_data.loc[train_data['user_id'] == target_user, 'item_id'])\n",
    "    item_scores = item_scores.drop(index=user_rated_items, errors='ignore')\n",
    "\n",
    "    # Sort and select top_n\n",
    "    top_items = item_scores.sort_values(ascending=False).head(top_n)\n",
    "\n",
    "    return list(zip(top_items.index, top_items.values))\n",
    "\n",
    "start_time = time.time()\n",
    "target_user, k = 1, 30\n",
    "recommendations = recommend_topk_user_based(train_data, user_similarity_matrix, target_user, k, top_n=10)\n",
    "\n",
    "print(f\"Top-10 recommendations for user {target_user}:\")\n",
    "for item, score in recommendations:\n",
    "    print(f\"Item {item}: {score:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'Running time: {end_time - start_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e4d8a",
   "metadata": {},
   "source": [
    "### C. Item-based neighborhood method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0421aa",
   "metadata": {},
   "source": [
    "#### C.1 Implementing a function that computes the Cosine similarity between two items. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d443b63",
   "metadata": {},
   "source": [
    "The **cosine similarity** between two items \\(i\\) and \\(j\\) is defined as:\n",
    "\n",
    "$$\n",
    "\\text{sim}(i,j) = \\frac{\\sum_{u \\in U_{ij}} r_{u,i} \\cdot r_{u,j}}\n",
    "                      {\\sqrt{\\sum_{u \\in U_{ij}} r_{u,i}^2} \\cdot \\sqrt{\\sum_{u \\in U_{ij}} r_{u,j}^2}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\(r_{u,i}\\) = rating of user \\(u\\) on item \\(i\\)  \n",
    "- \\(r_{u,j}\\) = rating of user \\(u\\) on item \\(j\\)  \n",
    "- \\(U_{ij}\\) = set of users who have rated both items \\(i\\) and \\(j\\)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "369ce459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between items 1 and 2 is 0.9500\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(item1_ratings: pd.Series, item2_ratings: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two items' rating vectors.\n",
    "    Only common users are considered.\n",
    "    \n",
    "    Parameters:\n",
    "    - item1_ratings, item2_ratings: pd.Series indexed by user_id\n",
    "    \n",
    "    Returns:\n",
    "    - float: cosine similarity between -1 and 1\n",
    "    \"\"\"\n",
    "    # Find common users who rated both items\n",
    "    common_users = item1_ratings.index.intersection(item2_ratings.index)\n",
    "    \n",
    "    if len(common_users) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Extract ratings of common users\n",
    "    v1 = item1_ratings.loc[common_users]\n",
    "    v2 = item2_ratings.loc[common_users]\n",
    "\n",
    "    # Drop any NaN values (just in case)\n",
    "    valid = v1.notna() & v2.notna()\n",
    "    v1 = v1[valid]\n",
    "    v2 = v2[valid]\n",
    "\n",
    "    if len(v1) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    numerator = (v1 * v2).sum()\n",
    "    denominator = np.sqrt((v1 ** 2).sum()) * np.sqrt((v2 ** 2).sum())\n",
    "\n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "\n",
    "    result = numerator / denominator\n",
    "    return result\n",
    "\n",
    "item1, item2 = 1, 2\n",
    "item1_ratings = train_data[train_data['item_id'] == item1].set_index('user_id')['rating']\n",
    "item2_ratings = train_data[train_data['item_id'] == item2].set_index('user_id')['rating']\n",
    "print(f\"Cosine similarity between items {item1} and {item2} is {cosine_similarity(item1_ratings, item2_ratings):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0b880d",
   "metadata": {},
   "source": [
    "#### C.2 Item-Item similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19385545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 1595.0528 seconds\n"
     ]
    }
   ],
   "source": [
    "def compute_item_similarity_matrix(train_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute item-item similarity matrix using cosine similarity.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_data: pd.DataFrame with columns ['user_id', 'item_id', 'rating']\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: item-item similarity matrix (rows & cols = item_ids)\n",
    "    \"\"\"\n",
    "    items = train_data['item_id'].unique()\n",
    "    item_similarity_matrix = pd.DataFrame(np.zeros((len(items), len(items))), index=items, columns=items)\n",
    "    \n",
    "    # Create item-rating mapping for faster lookups\n",
    "    item_ratings = {\n",
    "        item: train_data[train_data['item_id'] == item].set_index('user_id')['rating']\n",
    "        for item in items\n",
    "    }\n",
    "\n",
    "    # Compute pairwise cosine similarities\n",
    "    for i, item1 in enumerate(items):\n",
    "        for j, item2 in enumerate(items):\n",
    "            if i > j:  # Use symmetry to skip redundant computation\n",
    "                item_similarity_matrix.loc[item1, item2] = item_similarity_matrix.loc[item2, item1]\n",
    "                continue\n",
    "            if item1 == item2:\n",
    "                item_similarity_matrix.loc[item1, item2] = 1.0\n",
    "                continue\n",
    "\n",
    "            sim = cosine_similarity(item_ratings[item1], item_ratings[item2])\n",
    "            item_similarity_matrix.loc[item1, item2] = sim\n",
    "            item_similarity_matrix.loc[item2, item1] = sim  # symmetry\n",
    "    \n",
    "    return item_similarity_matrix\n",
    "\n",
    "start_time = time.time()\n",
    "item_similarity_matrix = compute_item_similarity_matrix(train_data)  \n",
    "end_time = time.time()\n",
    "print(f'Running time: {end_time - start_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f37879c",
   "metadata": {},
   "source": [
    "#### C.3 Implementing a function that returns k most similar item along with the similarity values to a target item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffbdb7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbors of item 1 are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1075, np.float64(1.0)),\n",
       " (1080, np.float64(1.0)),\n",
       " (1096, np.float64(1.0)),\n",
       " (1083, np.float64(1.0)),\n",
       " (1144, np.float64(1.0)),\n",
       " (1578, np.float64(1.0)),\n",
       " (1538, np.float64(1.0)),\n",
       " (1554, np.float64(1.0)),\n",
       " (1436, np.float64(1.0)),\n",
       " (1004, np.float64(1.0))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_k_item_neighbors(item_similarity_matrix: pd.DataFrame, target_item, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve top-k most similar items to the target item.\n",
    "    \n",
    "    Parameters:\n",
    "    - item_similarity_matrix: pd.DataFrame, item-item similarity\n",
    "    - target_item: item ID\n",
    "    - k: number of neighbors\n",
    "    \n",
    "    Returns:\n",
    "    - List of tuples: [(neighbor_item_id, similarity), ...]\n",
    "    \"\"\"\n",
    "    if target_item not in item_similarity_matrix.index:\n",
    "        return []\n",
    "\n",
    "    # Extract similarity scores for the target item\n",
    "    item_similarities = item_similarity_matrix.loc[target_item]\n",
    "\n",
    "    # Remove the target item itself\n",
    "    item_similarities = item_similarities.drop(target_item, errors='ignore')\n",
    "\n",
    "    # Sort by similarity (descending)\n",
    "    sorted_neighbors = item_similarities.sort_values(ascending=False)\n",
    "\n",
    "    # Select top-k most similar items\n",
    "    top_k = sorted_neighbors.head(k)\n",
    "\n",
    "    # Convert to list of tuples\n",
    "    top_k_neighbors = list(zip(top_k.index, top_k.values))\n",
    "\n",
    "    return top_k_neighbors\n",
    "\n",
    "target_item, k = 1, 10\n",
    "print(f\"Neighbors of item {target_item} are:\")\n",
    "get_k_item_neighbors(item_similarity_matrix, target_item, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e161805",
   "metadata": {},
   "source": [
    "#### C.4 Implementing a function that predicts the rating for a target user might give to a target item using item-item similarity matrix and the following equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac324c0",
   "metadata": {},
   "source": [
    "The **predicted rating** for a target user \\(u\\) on a target item \\(i\\) using item-based collaborative filtering is:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{u,i} = \\frac{\\sum_{j \\in N(i)} s(i,j) \\cdot r_{u,j}}{\\sum_{j \\in N(i)} |s(i,j)|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\(\\hat{r}_{u,i}\\) = predicted rating of user \\(u\\) on item \\(i\\)  \n",
    "- \\(N(i)\\) = set of top-\\(k\\) most similar items to item \\(i\\) that user \\(u\\) has rated  \n",
    "- \\(s(i,j)\\) = similarity between item \\(i\\) and item \\(j\\)  \n",
    "- \\(r_{u,j}\\) = rating of user \\(u\\) on item \\(j\\)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28235fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual rating for user 1 and item 17 is 3. The predicted rating by item-based CF for user 1 and item 17 is 3.5321\n"
     ]
    }
   ],
   "source": [
    "def predict_rating_item_based(train_data: pd.DataFrame, item_similarity_matrix: pd.DataFrame, target_user, target_item, k=5):\n",
    "    \"\"\"\n",
    "    Predict rating using item-based CF (non-mean centric).\n",
    "    \n",
    "    Parameters:\n",
    "    - train_data: pd.DataFrame ['user_id', 'item_id', 'rating']\n",
    "    - item_similarity_matrix: item-item similarity DataFrame\n",
    "    - target_user: user ID\n",
    "    - target_item: item ID\n",
    "    - k: number of neighbors to use\n",
    "    \n",
    "    Returns:\n",
    "    - float: predicted rating, or np.nan if not enough data\n",
    "    \"\"\"\n",
    "    # Get items the user has already rated\n",
    "    user_ratings = train_data[train_data['user_id'] == target_user][['item_id', 'rating']]\n",
    "    \n",
    "    if user_ratings.empty:\n",
    "        return np.nan\n",
    "\n",
    "    # Get similarities between target item and all items the user rated\n",
    "    sims = item_similarity_matrix.loc[target_item, user_ratings['item_id']]\n",
    "\n",
    "    # Combine with user's ratings\n",
    "    neighbors = pd.DataFrame({\n",
    "        'rating': user_ratings['rating'].values,\n",
    "        'similarity': sims.values\n",
    "    })\n",
    "\n",
    "    # Keep top-k most similar items\n",
    "    neighbors = neighbors.sort_values(by='similarity', ascending=False).head(k)\n",
    "\n",
    "    # Optionally, filter out items with non-positive similarity\n",
    "    neighbors = neighbors[neighbors['similarity'] > 0]\n",
    "\n",
    "    if neighbors.empty:\n",
    "        return np.nan\n",
    "\n",
    "    # Compute weighted average\n",
    "    numerator = (neighbors['rating'] * neighbors['similarity']).sum()\n",
    "    denominator = neighbors['similarity'].sum()\n",
    "\n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "\n",
    "    predicted_rating = numerator / denominator\n",
    "\n",
    "    # Clamp to valid rating range (e.g., 1–5)\n",
    "    predicted_rating = np.clip(predicted_rating, 1, 5)\n",
    "\n",
    "    return predicted_rating\n",
    "\n",
    "\n",
    "target_user, target_item, k = 1, 17, 50\n",
    "print(f\"The actual rating for user {target_user} and item {target_item} is 3. The predicted rating by item-based CF for user {target_user} and item {target_item} is {predict_rating_item_based(train_data, item_similarity_matrix, target_user, target_item, k):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de263b97",
   "metadata": {},
   "source": [
    "#### C.5 Implement a function that generates top-K recommendation list for a target user using item-based CF method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e22d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 recommendations for user 1:\n",
      "Item 1342: 5.0000\n",
      "Item 1414: 4.7500\n",
      "Item 1259: 4.5332\n",
      "Item 1354: 4.5000\n",
      "Item 1654: 4.4375\n",
      "Item 1618: 4.4000\n",
      "Item 1500: 4.4000\n",
      "Item 1332: 4.3529\n",
      "Item 1234: 4.3481\n",
      "Item 1347: 4.3276\n"
     ]
    }
   ],
   "source": [
    "def recommend_topk_item_based(train_data, item_similarity_matrix, target_user, k=50, top_n=10):\n",
    "    \"\"\"\n",
    "    Fast Top-K recommendations for a target user using Item-based Collaborative Filtering.\n",
    "    \n",
    "    Args:\n",
    "        train_data (pd.DataFrame): [user_id, item_id, rating]\n",
    "        item_similarity_matrix (pd.DataFrame): item-item similarity matrix (square)\n",
    "        target_user (int): target user ID\n",
    "        k (int): number of most similar items to use when predicting\n",
    "        top_n (int): number of items to recommend\n",
    "    \n",
    "    Returns:\n",
    "        list of (item_id, predicted_score), sorted by predicted_score descending\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # Ratings given by the target user\n",
    "    user_ratings = train_data[train_data['user_id'] == target_user][['item_id', 'rating']]\n",
    "    if user_ratings.empty:\n",
    "        return []\n",
    "\n",
    "    rated_items = user_ratings['item_id'].tolist()\n",
    "\n",
    "    # Candidate items (not yet rated)\n",
    "    all_items = set(train_data['item_id'])\n",
    "    candidate_items = list(all_items - set(rated_items))\n",
    "\n",
    "    # Extract similarity submatrix (only for candidate vs rated items)\n",
    "    sim_submatrix = item_similarity_matrix.loc[candidate_items, rated_items]\n",
    "\n",
    "    # Compute top-k similarities per candidate (for weighted average)\n",
    "    sim_topk_indices = np.argpartition(-sim_submatrix.values, kth=min(k, len(rated_items)-1), axis=1)[:, :k]\n",
    "\n",
    "    # Get corresponding similarity and rating values\n",
    "    sim_values = np.take_along_axis(sim_submatrix.values, sim_topk_indices, axis=1)\n",
    "    rated_item_ids = np.take_along_axis(np.tile(sim_submatrix.columns.values, (len(candidate_items), 1)),\n",
    "                                        sim_topk_indices, axis=1)\n",
    "    user_ratings_dict = dict(zip(user_ratings['item_id'], user_ratings['rating']))\n",
    "    rating_values = np.vectorize(user_ratings_dict.get)(rated_item_ids)\n",
    "\n",
    "    # Weighted average rating prediction\n",
    "    weighted_sum = np.sum(sim_values * rating_values, axis=1)\n",
    "    norm_factor = np.sum(np.abs(sim_values), axis=1)\n",
    "    pred_scores = np.divide(weighted_sum, norm_factor, out=np.zeros_like(weighted_sum), where=norm_factor != 0)\n",
    "\n",
    "    # Combine with item IDs\n",
    "    preds_df = pd.DataFrame({'item_id': candidate_items, 'pred_score': pred_scores})\n",
    "\n",
    "    # Sort by predicted score\n",
    "    preds_df = preds_df.sort_values('pred_score', ascending=False).head(top_n)\n",
    "\n",
    "    return list(zip(preds_df['item_id'], preds_df['pred_score']))\n",
    "\n",
    "target_user, k = 1, 50\n",
    "recommendations = recommend_topk_item_based(train_data, item_similarity_matrix, target_user, k, top_n=10)\n",
    "print(f\"Top-10 recommendations for user {target_user}:\")\n",
    "for item, score in recommendations:\n",
    "    print(f\"Item {item}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529cfade",
   "metadata": {},
   "source": [
    "### D. Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7c60388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorizationSGD:\n",
    "    \"\"\"\n",
    "    Matrix Factorization for rating prediction using Stochastic Gradient Descent (SGD).\n",
    "    \n",
    "    Rating matrix R ≈ P × Q^T + biases\n",
    "    \"\"\"\n",
    "    def __init__(self, n_factors=20, learning_rate=0.01, regularization=0.02, n_epochs=20, use_bias=True):\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.n_epochs = n_epochs\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        # Model parameters\n",
    "        self.P = None  # User latent factors\n",
    "        self.Q = None  # Item latent factors\n",
    "        self.user_bias = None\n",
    "        self.item_bias = None\n",
    "        self.global_mean = None\n",
    "\n",
    "    def fit(self, ratings, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        \n",
    "        Args:\n",
    "            ratings (pd.DataFrame): dataframe with [user_id, item_id, rating]\n",
    "        \"\"\"\n",
    "        # Map IDs to indices\n",
    "        self.user_mapping = {u: i for i, u in enumerate(ratings['user_id'].unique())}\n",
    "        self.item_mapping = {i: j for j, i in enumerate(ratings['item_id'].unique())}\n",
    "        self.user_inv = {i: u for u, i in self.user_mapping.items()}\n",
    "        self.item_inv = {j: i for i, j in self.item_mapping.items()}\n",
    "\n",
    "        n_users = len(self.user_mapping)\n",
    "        n_items = len(self.item_mapping)\n",
    "\n",
    "        # Initialize factors\n",
    "        self.P = np.random.normal(0, 0.1, (n_users, self.n_factors))\n",
    "        self.Q = np.random.normal(0, 0.1, (n_items, self.n_factors))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.user_bias = np.zeros(n_users)\n",
    "            self.item_bias = np.zeros(n_items)\n",
    "            self.global_mean = ratings['rating'].mean()\n",
    "\n",
    "        # Convert to (user_idx, item_idx, rating) triples\n",
    "        training_data = [(self.user_mapping[u], self.item_mapping[i], r)\n",
    "                         for u, i, r in zip(ratings['user_id'], ratings['item_id'], ratings['rating'])]\n",
    "\n",
    "        # SGD loop\n",
    "        for epoch in range(self.n_epochs):\n",
    "            np.random.shuffle(training_data)\n",
    "            total_error = 0\n",
    "\n",
    "            for u, i, r in training_data:\n",
    "                pred = np.dot(self.P[u], self.Q[i])\n",
    "                if self.use_bias:\n",
    "                    pred += self.global_mean + self.user_bias[u] + self.item_bias[i]\n",
    "\n",
    "                err = r - pred\n",
    "                total_error += err ** 2\n",
    "\n",
    "                # Updates\n",
    "                P_u = self.P[u]\n",
    "                Q_i = self.Q[i]\n",
    "\n",
    "                self.P[u] += self.learning_rate * (err * Q_i - self.regularization * P_u)\n",
    "                self.Q[i] += self.learning_rate * (err * P_u - self.regularization * Q_i)\n",
    "\n",
    "                if self.use_bias:\n",
    "                    self.user_bias[u] += self.learning_rate * (err - self.regularization * self.user_bias[u])\n",
    "                    self.item_bias[i] += self.learning_rate * (err - self.regularization * self.item_bias[i])\n",
    "\n",
    "            rmse = np.sqrt(total_error / len(training_data))\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch+1}/{self.n_epochs} - RMSE: {rmse:.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_single(self, user_id, item_id):\n",
    "        \"\"\"Predict rating for a single (user, item) pair\"\"\"\n",
    "        if user_id not in self.user_mapping or item_id not in self.item_mapping:\n",
    "            return np.nan\n",
    "\n",
    "        u = self.user_mapping[user_id]\n",
    "        i = self.item_mapping[item_id]\n",
    "\n",
    "        pred = np.dot(self.P[u], self.Q[i])\n",
    "        if self.use_bias:\n",
    "            pred += self.global_mean + self.user_bias[u] + self.item_bias[i]\n",
    "        return pred\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        \"\"\"Predict ratings for a test dataframe with [user_id, item_id]\"\"\"\n",
    "        preds = []\n",
    "        for u, i in zip(test_data['user_id'], test_data['item_id']):\n",
    "            preds.append(self.predict_single(u, i))\n",
    "        return np.array(preds)\n",
    "\n",
    "    def recommend_topk(self, user_id, train_data, n=10, exclude_seen=True):\n",
    "        \"\"\"\n",
    "        Generate Top-K recommendations for a given user.\n",
    "\n",
    "        Args:\n",
    "            user_id (int): target user ID (original ID, not index).\n",
    "            train_data (pd.DataFrame): training ratings [user_id, item_id, rating],\n",
    "                                       used to exclude already-seen items.\n",
    "            k (int): number of recommendations.\n",
    "            exclude_seen (bool): whether to exclude items the user already rated.\n",
    "\n",
    "        Returns:\n",
    "            list of (item_id, predicted_score) sorted by score desc.\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_mapping:\n",
    "            return []\n",
    "\n",
    "        u = self.user_mapping[user_id]\n",
    "\n",
    "        # Predict scores for all items\n",
    "        scores = np.dot(self.P[u], self.Q.T)\n",
    "        if self.use_bias:\n",
    "            scores += self.global_mean + self.user_bias[u] + self.item_bias\n",
    "\n",
    "        # Exclude seen items\n",
    "        if exclude_seen:\n",
    "            seen_items = train_data[train_data['user_id'] == user_id]['item_id'].values\n",
    "            seen_idx = [self.item_mapping[i] for i in seen_items if i in self.item_mapping]\n",
    "            scores[seen_idx] = -np.inf\n",
    "\n",
    "        # Get top-K items\n",
    "        top_idx = np.argsort(scores)[::-1][:n]\n",
    "        top_items = [self.item_inv[i] for i in top_idx]\n",
    "        top_scores = scores[top_idx]\n",
    "\n",
    "        return list(zip(top_items, top_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f030b0",
   "metadata": {},
   "source": [
    "### E. Bayesian Probabilistic Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0155e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianPersonalizedRanking:\n",
    "    def __init__(self, n_factors=50, learning_rate=0.01, regularization=0.01, n_epochs=20):\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "    def fit(self, train_data, verbose=True):\n",
    "        \"\"\"\n",
    "        train_data: DataFrame with columns ['user_id', 'item_id']\n",
    "        \"\"\"\n",
    "        users = train_data['user_id'].unique()\n",
    "        items = train_data['item_id'].unique()\n",
    "\n",
    "        self.user_mapping = {u: i for i, u in enumerate(users)}\n",
    "        self.item_mapping = {i: j for j, i in enumerate(items)}\n",
    "        self.user_inv = {i: u for u, i in self.user_mapping.items()}\n",
    "        self.item_inv = {j: i for i, j in self.item_mapping.items()}\n",
    "\n",
    "        n_users = len(users)\n",
    "        n_items = len(items)\n",
    "\n",
    "        # Initialize latent factors\n",
    "        self.P = np.random.normal(0, 0.1, (n_users, self.n_factors))\n",
    "        self.Q = np.random.normal(0, 0.1, (n_items, self.n_factors))\n",
    "\n",
    "        # Convert data to user->positive items\n",
    "        user_pos_items = train_data.groupby('user_id')['item_id'].apply(set).to_dict()\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            total_loss = 0\n",
    "            for _ in range(len(train_data)):\n",
    "                # Sample user, positive item, and negative item\n",
    "                u = np.random.choice(users)\n",
    "                pos_items = user_pos_items[u]\n",
    "                i = np.random.choice(list(pos_items))\n",
    "                j = np.random.choice(list(items - pos_items)) if isinstance(items, set) else np.random.choice([it for it in items if it not in pos_items])\n",
    "\n",
    "                u_idx, i_idx, j_idx = self.user_mapping[u], self.item_mapping[i], self.item_mapping[j]\n",
    "\n",
    "                # Compute predicted preference difference\n",
    "                x_uij = np.dot(self.P[u_idx], self.Q[i_idx] - self.Q[j_idx])\n",
    "                sigmoid = 1 / (1 + np.exp(-x_uij))\n",
    "\n",
    "                # Gradients\n",
    "                dP = (1 - sigmoid) * (self.Q[i_idx] - self.Q[j_idx]) - self.regularization * self.P[u_idx]\n",
    "                dQi = (1 - sigmoid) * self.P[u_idx] - self.regularization * self.Q[i_idx]\n",
    "                dQj = -(1 - sigmoid) * self.P[u_idx] - self.regularization * self.Q[j_idx]\n",
    "\n",
    "                # Updates\n",
    "                self.P[u_idx] += self.learning_rate * dP\n",
    "                self.Q[i_idx] += self.learning_rate * dQi\n",
    "                self.Q[j_idx] += self.learning_rate * dQj\n",
    "\n",
    "                total_loss += -np.log(sigmoid)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch+1}/{self.n_epochs} - Loss: {total_loss/len(train_data):.4f}\")\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        if user_id not in self.user_mapping or item_id not in self.item_mapping:\n",
    "            return np.nan\n",
    "        u, i = self.user_mapping[user_id], self.item_mapping[item_id]\n",
    "        return np.dot(self.P[u], self.Q[i])\n",
    "\n",
    "    def recommend_topk(self, user_id, train_data=None, k=10):\n",
    "        if user_id not in self.user_mapping:\n",
    "            return []\n",
    "\n",
    "        u = self.user_mapping[user_id]\n",
    "        scores = np.dot(self.P[u], self.Q.T)\n",
    "        if train_data is not None:\n",
    "            seen_items = set(train_data[train_data['user_id'] == user_id]['item_id'])\n",
    "            item_indices = [self.item_mapping[i] for i in seen_items if i in self.item_mapping]\n",
    "            scores[item_indices] = -np.inf\n",
    "\n",
    "        top_indices = np.argsort(scores)[::-1][:k]\n",
    "        top_items = [self.item_inv[i] for i in top_indices]\n",
    "        return list(zip(top_items, scores[top_indices]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6cc4c0",
   "metadata": {},
   "source": [
    "### F. Recommender Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b38977",
   "metadata": {},
   "source": [
    "#### F.1 Content-Based Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f3f451d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id  item_id  rating  timestamp  CB_pred_full_avg\n",
      "0            1        6       5  887431973          3.181899\n",
      "1            1       10       3  875693118          3.105166\n",
      "2            1       12       5  878542960          3.073602\n",
      "3            1       14       5  874965706          3.022038\n",
      "4            1       17       3  875073198          3.110272\n",
      "...        ...      ...     ...        ...               ...\n",
      "19995      458      648       4  886395899          3.413348\n",
      "19996      458     1101       4  886397931          3.366637\n",
      "19997      459      934       3  879563639          3.408985\n",
      "19998      460       10       3  882912371          3.146654\n",
      "19999      462      682       5  886365231          3.476901\n",
      "\n",
      "[20000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "def content_based_rating_prediction(train_data, test_data, content_type, aggregation_method):\n",
    "    \"\"\"\n",
    "    Predict ratings for user-item pairs in test_data using a content-based model.\n",
    "    Scales predictions to the [1, 5] interval.\n",
    "    \"\"\"\n",
    "    # Precompute all item embeddings\n",
    "    item_embs = {item_id: get_item_emb(item_id, content_type)\n",
    "                 for item_id in train_data['item_id'].unique()}\n",
    "\n",
    "    # Precompute all user embeddings\n",
    "    user_embs = {user_id: get_user_emb(train_data, user_id, content_type, aggregation_method)\n",
    "                 for user_id in train_data['user_id'].unique()}\n",
    "\n",
    "    # Predict raw ratings\n",
    "    def safe_dot(u, i):\n",
    "        if u not in user_embs or i not in item_embs:\n",
    "            return np.nan\n",
    "        return float(np.dot(user_embs[u], item_embs[i]))\n",
    "\n",
    "    raw_preds = [safe_dot(u, i) for u, i in zip(test_data['user_id'], test_data['item_id'])]\n",
    "    raw_preds = np.array(raw_preds, dtype=float)\n",
    "\n",
    "    # Compute min and max of non-NaN predictions\n",
    "    valid_mask = ~np.isnan(raw_preds)\n",
    "    min_val, max_val = raw_preds[valid_mask].min(), raw_preds[valid_mask].max()\n",
    "\n",
    "    # Normalize to [1, 5] using formula: 1 + (pred - min_val) * 4 / (max_val - min_val)\n",
    "    scaled_preds = raw_preds.copy()\n",
    "    if max_val > min_val:\n",
    "        scaled_preds[valid_mask] = 1 + (raw_preds[valid_mask] - min_val) * (4.0 / (max_val - min_val))\n",
    "    else:\n",
    "        # if all predictions are the same, just clip to 1-5\n",
    "        scaled_preds[valid_mask] = np.clip(raw_preds[valid_mask], 1.0, 5.0)\n",
    "\n",
    "    # Store predictions in test_data\n",
    "    # test_data = test_data.copy()\n",
    "    test_data[f'CB_pred_{content_type}_{aggregation_method}'] = scaled_preds\n",
    "\n",
    "    return test_data\n",
    "\n",
    "\n",
    "content_based_rating_prediction(train_data, test_data, 'full', 'avg')\n",
    "print(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5988f",
   "metadata": {},
   "source": [
    "#### F.2 User-based neighborhood method Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c3736a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting UserCF ratings: 100%|██████████| 20000/20000 [03:04<00:00, 108.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id  item_id  rating  timestamp  CB_pred_full_avg  UserKNN_pred_5\n",
      "0            1        6       5  887431973          3.181899        2.897132\n",
      "1            1       10       3  875693118          3.105166        3.777550\n",
      "2            1       12       5  878542960          3.073602        4.303465\n",
      "3            1       14       5  874965706          3.022038        4.032122\n",
      "4            1       17       3  875073198          3.110272        3.831528\n",
      "...        ...      ...     ...        ...               ...             ...\n",
      "19995      458      648       4  886395899          3.413348        3.687026\n",
      "19996      458     1101       4  886397931          3.366637        3.768772\n",
      "19997      459      934       3  879563639          3.408985        2.999013\n",
      "19998      460       10       3  882912371          3.146654        3.985456\n",
      "19999      462      682       5  886365231          3.476901        3.187263\n",
      "\n",
      "[20000 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def user_based_rating_prediction(train_data, user_similarity_matrix, test_data, k):\n",
    "    \"\"\"\n",
    "    Predict user-based CF ratings for all (user, item) pairs in test_data,\n",
    "    and normalize predictions to [1, 5] using min-max scaling.\n",
    "    \"\"\"\n",
    "    pred_ratings = []\n",
    "\n",
    "    for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Predicting UserCF ratings\"):\n",
    "        target_user = row['user_id']\n",
    "        target_item = row['item_id']\n",
    "\n",
    "        # Check if target_user and target_item exist\n",
    "        if target_user not in user_similarity_matrix.index or target_item not in train_data['item_id'].unique():\n",
    "            pred_ratings.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # Target user's mean rating\n",
    "        target_user_ratings = train_data[train_data['user_id'] == target_user]['rating']\n",
    "        if target_user_ratings.empty:\n",
    "            pred_ratings.append(np.nan)\n",
    "            continue\n",
    "        target_user_mean = target_user_ratings.mean()\n",
    "\n",
    "        # Users who rated target item\n",
    "        item_raters = train_data[train_data['item_id'] == target_item]\n",
    "        if item_raters.empty:\n",
    "            pred_ratings.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # Similarities between target user and raters\n",
    "        similarities = user_similarity_matrix.loc[target_user, item_raters['user_id']]\n",
    "        item_raters = item_raters.copy()\n",
    "        item_raters['similarity'] = similarities.values\n",
    "\n",
    "        # Top-k neighbors with positive similarity\n",
    "        item_raters = item_raters[item_raters['similarity'] > 0].sort_values(by='similarity', ascending=False).head(k)\n",
    "        if item_raters.empty:\n",
    "            pred_ratings.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # Mean-centered ratings\n",
    "        neighbor_means = train_data.groupby('user_id')['rating'].mean().reindex(item_raters['user_id'])\n",
    "        item_raters['mean_centered'] = item_raters['rating'] - neighbor_means.values\n",
    "\n",
    "        numerator = (item_raters['similarity'] * item_raters['mean_centered']).sum()\n",
    "        denominator = item_raters['similarity'].abs().sum()\n",
    "\n",
    "        predicted_rating = target_user_mean + numerator / denominator\n",
    "        pred_ratings.append(predicted_rating)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    pred_ratings = np.array(pred_ratings, dtype=float)\n",
    "\n",
    "    # Min-max normalization over valid predictions\n",
    "    valid_mask = ~np.isnan(pred_ratings)\n",
    "    min_val, max_val = pred_ratings[valid_mask].min(), pred_ratings[valid_mask].max()\n",
    "    if max_val > min_val:\n",
    "        pred_ratings[valid_mask] = 1 + (pred_ratings[valid_mask] - min_val) * (4.0 / (max_val - min_val))\n",
    "    else:\n",
    "        pred_ratings[valid_mask] = np.clip(pred_ratings[valid_mask], 1, 5)\n",
    "\n",
    "    # Add predictions to test_data\n",
    "    test_data[f'UserKNN_pred_{k}'] = pred_ratings\n",
    "\n",
    "    return test_data\n",
    "\n",
    "user_based_rating_prediction(train_data, user_similarity_matrix, test_data, 5)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f0a471",
   "metadata": {},
   "source": [
    "#### F.3 Item-based neighborhood method Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d511de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting ItemCF ratings: 100%|██████████| 20000/20000 [00:18<00:00, 1100.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id  item_id  rating  timestamp  CB_pred_full_avg  UserKNN_pred_5  \\\n",
      "0            1        6       5  887431973          3.181899        2.897132   \n",
      "1            1       10       3  875693118          3.105166        3.777550   \n",
      "2            1       12       5  878542960          3.073602        4.303465   \n",
      "3            1       14       5  874965706          3.022038        4.032122   \n",
      "4            1       17       3  875073198          3.110272        3.831528   \n",
      "...        ...      ...     ...        ...               ...             ...   \n",
      "19995      458      648       4  886395899          3.413348        3.687026   \n",
      "19996      458     1101       4  886397931          3.366637        3.768772   \n",
      "19997      459      934       3  879563639          3.408985        2.999013   \n",
      "19998      460       10       3  882912371          3.146654        3.985456   \n",
      "19999      462      682       5  886365231          3.476901        3.187263   \n",
      "\n",
      "       ItemKNN_pred_5  \n",
      "0            3.000000  \n",
      "1            2.599449  \n",
      "2            2.393287  \n",
      "3            2.200000  \n",
      "4            3.200000  \n",
      "...               ...  \n",
      "19995        2.400000  \n",
      "19996        3.396667  \n",
      "19997        3.600856  \n",
      "19998        3.198217  \n",
      "19999        4.799286  \n",
      "\n",
      "[20000 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def item_based_rating_prediction(train_data, item_similarity_matrix, test_data, k):\n",
    "    \"\"\"\n",
    "    Predict item-based CF ratings for all (user, item) pairs in test_data,\n",
    "    and normalize predictions to [1,5] using min-max scaling.\n",
    "    \"\"\"\n",
    "    pred_ratings = []\n",
    "\n",
    "    for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Predicting ItemCF ratings\"):\n",
    "        target_user = row['user_id']\n",
    "        target_item = row['item_id']\n",
    "\n",
    "        # Users who rated the target item\n",
    "        user_ratings = train_data[train_data['user_id'] == target_user]\n",
    "        if user_ratings.empty or target_item not in item_similarity_matrix.index:\n",
    "            pred_ratings.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # Compute similarities between target item and items rated by user\n",
    "        rated_items = user_ratings['item_id'].values\n",
    "        sims = item_similarity_matrix.loc[target_item, rated_items]\n",
    "        top_k_idx = np.argsort(-sims.values)[:k]  # top-k most similar\n",
    "        top_k_sims = sims.values[top_k_idx]\n",
    "        top_k_ratings = user_ratings['rating'].values[top_k_idx]\n",
    "\n",
    "        # Keep only positive similarities\n",
    "        mask = top_k_sims > 0\n",
    "        if not mask.any():\n",
    "            pred_ratings.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        numerator = (top_k_sims[mask] * top_k_ratings[mask]).sum()\n",
    "        denominator = top_k_sims[mask].sum()\n",
    "        predicted_rating = numerator / denominator\n",
    "        pred_ratings.append(predicted_rating)\n",
    "\n",
    "    # Convert to numpy array and normalize\n",
    "    pred_ratings = np.array(pred_ratings, dtype=float)\n",
    "    valid_mask = ~np.isnan(pred_ratings)\n",
    "    min_val, max_val = pred_ratings[valid_mask].min(), pred_ratings[valid_mask].max()\n",
    "    if max_val > min_val:\n",
    "        pred_ratings[valid_mask] = 1 + (pred_ratings[valid_mask] - min_val) * (4.0 / (max_val - min_val))\n",
    "    else:\n",
    "        pred_ratings[valid_mask] = np.clip(pred_ratings[valid_mask], 1, 5)\n",
    "\n",
    "    # Store in DataFrame\n",
    "    test_data[f'ItemKNN_pred_{k}'] = pred_ratings\n",
    "\n",
    "    return test_data\n",
    "\n",
    "item_based_rating_prediction(train_data, item_similarity_matrix, test_data, 5)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6fd1ac",
   "metadata": {},
   "source": [
    "#### F.4 Matrix Factorization Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab0df0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - RMSE: 1.0965\n",
      "Epoch 2/10 - RMSE: 1.0573\n",
      "Epoch 3/10 - RMSE: 1.0312\n",
      "Epoch 4/10 - RMSE: 1.0125\n",
      "Epoch 5/10 - RMSE: 0.9984\n",
      "Epoch 6/10 - RMSE: 0.9873\n",
      "Epoch 7/10 - RMSE: 0.9783\n",
      "Epoch 8/10 - RMSE: 0.9708\n",
      "Epoch 9/10 - RMSE: 0.9645\n",
      "Epoch 10/10 - RMSE: 0.9590\n",
      "       user_id  item_id  rating  timestamp  CB_pred_full_avg  UserKNN_pred_5  \\\n",
      "0            1        6       5  887431973          3.181899        2.897132   \n",
      "1            1       10       3  875693118          3.105166        3.777550   \n",
      "2            1       12       5  878542960          3.073602        4.303465   \n",
      "3            1       14       5  874965706          3.022038        4.032122   \n",
      "4            1       17       3  875073198          3.110272        3.831528   \n",
      "...        ...      ...     ...        ...               ...             ...   \n",
      "19995      458      648       4  886395899          3.413348        3.687026   \n",
      "19996      458     1101       4  886397931          3.366637        3.768772   \n",
      "19997      459      934       3  879563639          3.408985        2.999013   \n",
      "19998      460       10       3  882912371          3.146654        3.985456   \n",
      "19999      462      682       5  886365231          3.476901        3.187263   \n",
      "\n",
      "       ItemKNN_pred_5  MF_pred_50_0.001_0.001_10  \n",
      "0            3.000000                   3.574589  \n",
      "1            2.599449                   3.798890  \n",
      "2            2.393287                   4.355966  \n",
      "3            2.200000                   3.862171  \n",
      "4            3.200000                   3.498327  \n",
      "...               ...                        ...  \n",
      "19995        2.400000                   3.697353  \n",
      "19996        3.396667                   3.660207  \n",
      "19997        3.600856                   3.107855  \n",
      "19998        3.198217                   3.677318  \n",
      "19999        4.799286                   3.479231  \n",
      "\n",
      "[20000 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "def mf_rating_prediction(mf_model, test_data):\n",
    "    \"\"\"\n",
    "    Predict ratings using trained Matrix Factorization model.\n",
    "    Normalize predictions to [1,5] using min-max scaling.\n",
    "    \n",
    "    Parameters:\n",
    "    - mf_model: trained MF model with predict(user_id, item_id) method\n",
    "    - test_data: DataFrame with 'user_id' and 'item_id'\n",
    "    \"\"\"\n",
    "    raw_preds = mf_model.predict(test_data)\n",
    "    raw_preds = np.array(raw_preds, dtype=float)\n",
    "\n",
    "    # # Compute min-max on valid predictions\n",
    "    # valid_mask = ~np.isnan(raw_preds)\n",
    "    # min_val, max_val = raw_preds[valid_mask].min(), raw_preds[valid_mask].max()\n",
    "\n",
    "    # # Scale to [1,5]\n",
    "    # scaled_preds = raw_preds.copy()\n",
    "    # if max_val > min_val:\n",
    "    #     scaled_preds[valid_mask] = 1 + (raw_preds[valid_mask] - min_val) * (4.0 / (max_val - min_val))\n",
    "    # else:\n",
    "    #     scaled_preds[valid_mask] = np.clip(raw_preds[valid_mask], 1, 5)\n",
    "\n",
    "    # test_data['MF_pred'] = scaled_preds\n",
    "    test_data[f'MF_pred_{mf_model.n_factors}_{mf_model.learning_rate}_{mf_model.regularization}_{mf_model.n_epochs}'] = raw_preds\n",
    "    return test_data\n",
    "\n",
    "# test_data = test_data.drop('MF_pred_RAW', axis=1)\n",
    "mf = MatrixFactorizationSGD(n_factors=50, learning_rate=0.001, regularization=0.001, n_epochs=10)\n",
    "mf.fit(train_data, verbose=True)\n",
    "mf_rating_prediction(mf, test_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca98dbe2",
   "metadata": {},
   "source": [
    "### G. Hybrid Recommender Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c3a82",
   "metadata": {},
   "source": [
    "$$ score(u, i) = \\sum_{j=1}^k \\alpha_j score_{C_j} (u, i) $$\n",
    "\n",
    "Where:\n",
    "-  $\\alpha_j$ = Coefficient weight, the degree to which\n",
    "component 𝐶𝑗 contributes to final prediction\n",
    "- $score_{C_j} (u, i)$ = Prediction Score for user u and item i by algorithm j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a9d85fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_prediction(train_data, test_data, cb_setting, user_k, item_k, mf_setting):\n",
    "    # Content-Based\n",
    "    test_data = content_based_rating_prediction(train_data, test_data, cb_setting['content_type'], cb_setting['method'])\n",
    "\n",
    "    # UserKNN\n",
    "    test_data = user_based_rating_prediction(train_data, user_similarity_matrix, test_data, user_k)\n",
    "\n",
    "    # ItemKNN\n",
    "    test_data = item_based_rating_prediction(train_data, item_similarity_matrix, test_data, item_k)\n",
    "\n",
    "    # Matrix Factorization\n",
    "    mf = MatrixFactorizationSGD(mf_setting['n_factors'], mf_setting['lr'], mf_setting['reg'], mf_setting['ep'])\n",
    "    mf.fit(train_data, verbose=True)\n",
    "    test_data = mf_rating_prediction(mf, test_data)\n",
    "\n",
    "    # Content-Based\n",
    "    train_data = content_based_rating_prediction(train_data, train_data, cb_setting['content_type'], cb_setting['method'])\n",
    "\n",
    "    # UserKNN\n",
    "    train_data = user_based_rating_prediction(train_data, user_similarity_matrix, train_data, user_k)\n",
    "\n",
    "    # ItemKNN\n",
    "    train_data = item_based_rating_prediction(train_data, item_similarity_matrix, train_data, item_k)\n",
    "\n",
    "    # Matrix Factorization\n",
    "    mf = MatrixFactorizationSGD(mf_setting['n_factors'], mf_setting['lr'], mf_setting['reg'], mf_setting['ep'])\n",
    "    mf.fit(train_data, verbose=True)\n",
    "    train_data = mf_rating_prediction(mf, train_data)\n",
    "\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    # # Keep only rows where all predictions are valid\n",
    "    valid_mask = train_data[\n",
    "    [f\"CB_pred_{cb_setting['content_type']}_{cb_setting['method']}\",\n",
    "     f\"UserKNN_pred_{user_k}\",\n",
    "     f\"ItemKNN_pred_{item_k}\",\n",
    "     f\"MF_pred_{mf_setting['n_factors']}_{mf_setting['lr']}_{mf_setting['reg']}_{mf_setting['ep']}\"]\n",
    "    ].notna().all(axis=1)\n",
    "    df_train_valid = train_data[valid_mask]\n",
    "\n",
    "    X_train = df_train_valid[[f\"CB_pred_{cb_setting['content_type']}_{cb_setting['method']}\",\n",
    "     f\"UserKNN_pred_{user_k}\",\n",
    "     f\"ItemKNN_pred_{item_k}\",\n",
    "     f\"MF_pred_{mf_setting['n_factors']}_{mf_setting['lr']}_{mf_setting['reg']}_{mf_setting['ep']}\"]].values\n",
    "    y_train = train_data['rating'].values\n",
    "\n",
    "    reg = LinearRegression(fit_intercept=True)\n",
    "    reg.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Learned weights (α1, α2, α3, α4):\", reg.coef_)\n",
    "    print(\"Intercept:\", reg.intercept_)\n",
    "\n",
    "    valid_mask = test_data[[f\"CB_pred_{cb_setting['content_type']}_{cb_setting['method']}\",\n",
    "     f\"UserKNN_pred_{user_k}\",\n",
    "     f\"ItemKNN_pred_{item_k}\",\n",
    "          f\"MF_pred_{mf_setting['n_factors']}_{mf_setting['lr']}_{mf_setting['reg']}_{mf_setting['ep']}\"]].notna().all(axis=1)\n",
    "    df_test_valid = test_data[valid_mask]\n",
    "\n",
    "    X_test = df_test_valid[[f\"CB_pred_{cb_setting['content_type']}_{cb_setting['method']}\",\n",
    "     f\"UserKNN_pred_{user_k}\",\n",
    "     f\"ItemKNN_pred_{item_k}\",\n",
    "     f\"MF_pred_{mf_setting['n_factors']}_{mf_setting['lr']}_{mf_setting['reg']}_{mf_setting['ep']}\"]].values\n",
    "    df_test_valid['Hybrid_pred'] = reg.predict(X_test)\n",
    "\n",
    "    return df_test_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f712c-2895-4962-ad06-85da032fd597",
   "metadata": {},
   "source": [
    "# Task 2) Experiments for both rating prediction and ranking tasks, and conducting offline evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eee40a",
   "metadata": {},
   "source": [
    "### A. Rating Prediction Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56177635-1c91-4ca6-845e-5ae874726b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(title_genres, avg) - RMSE=1.37016705324228\n",
      "(title_genres, weighted_avg) - RMSE=1.3145654235158457\n",
      "(title_genres, avg_pos) - RMSE=1.2990971267934248\n",
      "(description, avg) - RMSE=1.261705772936097\n",
      "(description, weighted_avg) - RMSE=1.2981158215980482\n",
      "(description, avg_pos) - RMSE=1.311742561511312\n",
      "(full, avg) - RMSE=1.215107569248853\n",
      "(full, weighted_avg) - RMSE=1.2407087523281377\n",
      "(full, avg_pos) - RMSE=1.2489307147571487\n",
      "\n",
      "Best content-based RMSE: 1.215107569248853\n",
      "Best setting: {'content_type': 'full', 'method': 'avg'} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting UserCF ratings: 100%|██████████| 20000/20000 [03:02<00:00, 109.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=5 - RMSE=1.008678469139694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting UserCF ratings: 100%|██████████| 20000/20000 [03:06<00:00, 107.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=10 - RMSE=0.9946470831925149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting UserCF ratings: 100%|██████████| 20000/20000 [03:06<00:00, 107.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=20 - RMSE=0.9874592224010168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting UserCF ratings: 100%|██████████| 20000/20000 [03:05<00:00, 107.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=30 - RMSE=0.9857246313577047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting UserCF ratings: 100%|██████████| 20000/20000 [03:05<00:00, 107.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=40 - RMSE=0.9851546506274138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting UserCF ratings: 100%|██████████| 20000/20000 [03:07<00:00, 106.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=50 - RMSE=0.9849192918517446\n",
      "\n",
      "Best user-based KNN RMSE: 0.9849192918517446 at k=50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting ItemCF ratings: 100%|██████████| 20000/20000 [00:19<00:00, 1017.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=5 - RMSE=1.1857756548025105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting ItemCF ratings: 100%|██████████| 20000/20000 [00:19<00:00, 1016.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=10 - RMSE=1.1068561518245235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting ItemCF ratings: 100%|██████████| 20000/20000 [00:19<00:00, 1018.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=20 - RMSE=1.0679920305104755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting ItemCF ratings: 100%|██████████| 20000/20000 [00:19<00:00, 1024.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=25 - RMSE=1.0591903869946089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting ItemCF ratings: 100%|██████████| 20000/20000 [00:19<00:00, 1020.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=30 - RMSE=1.0560021124713046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting ItemCF ratings: 100%|██████████| 20000/20000 [00:19<00:00, 1023.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=40 - RMSE=1.0569881117804045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting ItemCF ratings: 100%|██████████| 20000/20000 [00:19<00:00, 1012.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=50 - RMSE=1.0601438267605028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting ItemCF ratings: 100%|██████████| 20000/20000 [00:19<00:00, 1013.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=100 - RMSE=1.0603196802081505\n",
      "\n",
      "Best item-based KNN RMSE: 1.0560021124713046 at k=30\n",
      "\n",
      "Epoch 1/10 - RMSE: 1.0945\n",
      "Epoch 2/10 - RMSE: 1.0559\n",
      "Epoch 3/10 - RMSE: 1.0304\n",
      "Epoch 4/10 - RMSE: 1.0122\n",
      "Epoch 5/10 - RMSE: 0.9986\n",
      "Epoch 6/10 - RMSE: 0.9881\n",
      "Epoch 7/10 - RMSE: 0.9797\n",
      "Epoch 8/10 - RMSE: 0.9727\n",
      "Epoch 9/10 - RMSE: 0.9669\n",
      "Epoch 10/10 - RMSE: 0.9619\n",
      "(factors=20, lr=0.001, reg=0.001, ep=10) - RMSE=0.999161015820945\n",
      "Epoch 1/20 - RMSE: 1.0947\n",
      "Epoch 2/20 - RMSE: 1.0561\n",
      "Epoch 3/20 - RMSE: 1.0306\n",
      "Epoch 4/20 - RMSE: 1.0124\n",
      "Epoch 5/20 - RMSE: 0.9989\n",
      "Epoch 6/20 - RMSE: 0.9883\n",
      "Epoch 7/20 - RMSE: 0.9799\n",
      "Epoch 8/20 - RMSE: 0.9729\n",
      "Epoch 9/20 - RMSE: 0.9671\n",
      "Epoch 10/20 - RMSE: 0.9621\n",
      "Epoch 11/20 - RMSE: 0.9578\n",
      "Epoch 12/20 - RMSE: 0.9540\n",
      "Epoch 13/20 - RMSE: 0.9506\n",
      "Epoch 14/20 - RMSE: 0.9476\n",
      "Epoch 15/20 - RMSE: 0.9448\n",
      "Epoch 16/20 - RMSE: 0.9423\n",
      "Epoch 17/20 - RMSE: 0.9400\n",
      "Epoch 18/20 - RMSE: 0.9378\n",
      "Epoch 19/20 - RMSE: 0.9358\n",
      "Epoch 20/20 - RMSE: 0.9339\n",
      "(factors=20, lr=0.001, reg=0.001, ep=20) - RMSE=0.9778784318350882\n",
      "Epoch 1/30 - RMSE: 1.0949\n",
      "Epoch 2/30 - RMSE: 1.0563\n",
      "Epoch 3/30 - RMSE: 1.0307\n",
      "Epoch 4/30 - RMSE: 1.0125\n",
      "Epoch 5/30 - RMSE: 0.9989\n",
      "Epoch 6/30 - RMSE: 0.9884\n",
      "Epoch 7/30 - RMSE: 0.9799\n",
      "Epoch 8/30 - RMSE: 0.9730\n",
      "Epoch 9/30 - RMSE: 0.9671\n",
      "Epoch 10/30 - RMSE: 0.9621\n",
      "Epoch 11/30 - RMSE: 0.9578\n",
      "Epoch 12/30 - RMSE: 0.9540\n",
      "Epoch 13/30 - RMSE: 0.9506\n",
      "Epoch 14/30 - RMSE: 0.9475\n",
      "Epoch 15/30 - RMSE: 0.9447\n",
      "Epoch 16/30 - RMSE: 0.9422\n",
      "Epoch 17/30 - RMSE: 0.9398\n",
      "Epoch 18/30 - RMSE: 0.9377\n",
      "Epoch 19/30 - RMSE: 0.9356\n",
      "Epoch 20/30 - RMSE: 0.9337\n",
      "Epoch 21/30 - RMSE: 0.9320\n",
      "Epoch 22/30 - RMSE: 0.9303\n",
      "Epoch 23/30 - RMSE: 0.9287\n",
      "Epoch 24/30 - RMSE: 0.9271\n",
      "Epoch 25/30 - RMSE: 0.9257\n",
      "Epoch 26/30 - RMSE: 0.9243\n",
      "Epoch 27/30 - RMSE: 0.9229\n",
      "Epoch 28/30 - RMSE: 0.9216\n",
      "Epoch 29/30 - RMSE: 0.9203\n",
      "Epoch 30/30 - RMSE: 0.9191\n",
      "(factors=20, lr=0.001, reg=0.001, ep=30) - RMSE=0.9692040743491196\n",
      "Epoch 1/40 - RMSE: 1.0945\n",
      "Epoch 2/40 - RMSE: 1.0559\n",
      "Epoch 3/40 - RMSE: 1.0304\n",
      "Epoch 4/40 - RMSE: 1.0122\n",
      "Epoch 5/40 - RMSE: 0.9987\n",
      "Epoch 6/40 - RMSE: 0.9882\n",
      "Epoch 7/40 - RMSE: 0.9797\n",
      "Epoch 8/40 - RMSE: 0.9728\n",
      "Epoch 9/40 - RMSE: 0.9670\n",
      "Epoch 10/40 - RMSE: 0.9620\n",
      "Epoch 11/40 - RMSE: 0.9577\n",
      "Epoch 12/40 - RMSE: 0.9539\n",
      "Epoch 13/40 - RMSE: 0.9505\n",
      "Epoch 14/40 - RMSE: 0.9475\n",
      "Epoch 15/40 - RMSE: 0.9447\n",
      "Epoch 16/40 - RMSE: 0.9422\n",
      "Epoch 17/40 - RMSE: 0.9399\n",
      "Epoch 18/40 - RMSE: 0.9377\n",
      "Epoch 19/40 - RMSE: 0.9357\n",
      "Epoch 20/40 - RMSE: 0.9338\n",
      "Epoch 21/40 - RMSE: 0.9321\n",
      "Epoch 22/40 - RMSE: 0.9304\n",
      "Epoch 23/40 - RMSE: 0.9288\n",
      "Epoch 24/40 - RMSE: 0.9273\n",
      "Epoch 25/40 - RMSE: 0.9259\n",
      "Epoch 26/40 - RMSE: 0.9245\n",
      "Epoch 27/40 - RMSE: 0.9232\n",
      "Epoch 28/40 - RMSE: 0.9220\n",
      "Epoch 29/40 - RMSE: 0.9207\n",
      "Epoch 30/40 - RMSE: 0.9195\n",
      "Epoch 31/40 - RMSE: 0.9184\n",
      "Epoch 32/40 - RMSE: 0.9173\n",
      "Epoch 33/40 - RMSE: 0.9162\n",
      "Epoch 34/40 - RMSE: 0.9151\n",
      "Epoch 35/40 - RMSE: 0.9140\n",
      "Epoch 36/40 - RMSE: 0.9130\n",
      "Epoch 37/40 - RMSE: 0.9120\n",
      "Epoch 38/40 - RMSE: 0.9109\n",
      "Epoch 39/40 - RMSE: 0.9099\n",
      "Epoch 40/40 - RMSE: 0.9089\n",
      "(factors=20, lr=0.001, reg=0.001, ep=40) - RMSE=0.964934911524312\n",
      "Epoch 1/50 - RMSE: 1.0949\n",
      "Epoch 2/50 - RMSE: 1.0563\n",
      "Epoch 3/50 - RMSE: 1.0307\n",
      "Epoch 4/50 - RMSE: 1.0125\n",
      "Epoch 5/50 - RMSE: 0.9990\n",
      "Epoch 6/50 - RMSE: 0.9884\n",
      "Epoch 7/50 - RMSE: 0.9799\n",
      "Epoch 8/50 - RMSE: 0.9730\n",
      "Epoch 9/50 - RMSE: 0.9672\n",
      "Epoch 10/50 - RMSE: 0.9622\n",
      "Epoch 11/50 - RMSE: 0.9579\n",
      "Epoch 12/50 - RMSE: 0.9541\n",
      "Epoch 13/50 - RMSE: 0.9507\n",
      "Epoch 14/50 - RMSE: 0.9476\n",
      "Epoch 15/50 - RMSE: 0.9448\n",
      "Epoch 16/50 - RMSE: 0.9423\n",
      "Epoch 17/50 - RMSE: 0.9400\n",
      "Epoch 18/50 - RMSE: 0.9378\n",
      "Epoch 19/50 - RMSE: 0.9358\n",
      "Epoch 20/50 - RMSE: 0.9339\n",
      "Epoch 21/50 - RMSE: 0.9321\n",
      "Epoch 22/50 - RMSE: 0.9305\n",
      "Epoch 23/50 - RMSE: 0.9289\n",
      "Epoch 24/50 - RMSE: 0.9273\n",
      "Epoch 25/50 - RMSE: 0.9259\n",
      "Epoch 26/50 - RMSE: 0.9245\n",
      "Epoch 27/50 - RMSE: 0.9232\n",
      "Epoch 28/50 - RMSE: 0.9219\n",
      "Epoch 29/50 - RMSE: 0.9206\n",
      "Epoch 30/50 - RMSE: 0.9194\n",
      "Epoch 31/50 - RMSE: 0.9182\n",
      "Epoch 32/50 - RMSE: 0.9171\n",
      "Epoch 33/50 - RMSE: 0.9160\n",
      "Epoch 34/50 - RMSE: 0.9149\n",
      "Epoch 35/50 - RMSE: 0.9138\n",
      "Epoch 36/50 - RMSE: 0.9127\n",
      "Epoch 37/50 - RMSE: 0.9117\n",
      "Epoch 38/50 - RMSE: 0.9106\n",
      "Epoch 39/50 - RMSE: 0.9096\n",
      "Epoch 40/50 - RMSE: 0.9085\n",
      "Epoch 41/50 - RMSE: 0.9075\n",
      "Epoch 42/50 - RMSE: 0.9065\n",
      "Epoch 43/50 - RMSE: 0.9055\n",
      "Epoch 44/50 - RMSE: 0.9044\n",
      "Epoch 45/50 - RMSE: 0.9034\n",
      "Epoch 46/50 - RMSE: 0.9024\n",
      "Epoch 47/50 - RMSE: 0.9014\n",
      "Epoch 48/50 - RMSE: 0.9003\n",
      "Epoch 49/50 - RMSE: 0.8993\n",
      "Epoch 50/50 - RMSE: 0.8982\n",
      "(factors=20, lr=0.001, reg=0.001, ep=50) - RMSE=0.960573926504925\n",
      "Epoch 1/10 - RMSE: 1.0948\n",
      "Epoch 2/10 - RMSE: 1.0563\n",
      "Epoch 3/10 - RMSE: 1.0308\n",
      "Epoch 4/10 - RMSE: 1.0126\n",
      "Epoch 5/10 - RMSE: 0.9991\n",
      "Epoch 6/10 - RMSE: 0.9885\n",
      "Epoch 7/10 - RMSE: 0.9801\n",
      "Epoch 8/10 - RMSE: 0.9731\n",
      "Epoch 9/10 - RMSE: 0.9673\n",
      "Epoch 10/10 - RMSE: 0.9623\n",
      "(factors=20, lr=0.001, reg=0.01, ep=10) - RMSE=0.9987963287658704\n",
      "Epoch 1/20 - RMSE: 1.0949\n",
      "Epoch 2/20 - RMSE: 1.0564\n",
      "Epoch 3/20 - RMSE: 1.0309\n",
      "Epoch 4/20 - RMSE: 1.0128\n",
      "Epoch 5/20 - RMSE: 0.9992\n",
      "Epoch 6/20 - RMSE: 0.9887\n",
      "Epoch 7/20 - RMSE: 0.9803\n",
      "Epoch 8/20 - RMSE: 0.9733\n",
      "Epoch 9/20 - RMSE: 0.9675\n",
      "Epoch 10/20 - RMSE: 0.9626\n",
      "Epoch 11/20 - RMSE: 0.9583\n",
      "Epoch 12/20 - RMSE: 0.9545\n",
      "Epoch 13/20 - RMSE: 0.9511\n",
      "Epoch 14/20 - RMSE: 0.9481\n",
      "Epoch 15/20 - RMSE: 0.9454\n",
      "Epoch 16/20 - RMSE: 0.9428\n",
      "Epoch 17/20 - RMSE: 0.9405\n",
      "Epoch 18/20 - RMSE: 0.9384\n",
      "Epoch 19/20 - RMSE: 0.9364\n",
      "Epoch 20/20 - RMSE: 0.9346\n",
      "(factors=20, lr=0.001, reg=0.01, ep=20) - RMSE=0.9782028542881397\n",
      "Epoch 1/30 - RMSE: 1.0947\n",
      "Epoch 2/30 - RMSE: 1.0562\n",
      "Epoch 3/30 - RMSE: 1.0307\n",
      "Epoch 4/30 - RMSE: 1.0126\n",
      "Epoch 5/30 - RMSE: 0.9991\n",
      "Epoch 6/30 - RMSE: 0.9886\n",
      "Epoch 7/30 - RMSE: 0.9802\n",
      "Epoch 8/30 - RMSE: 0.9733\n",
      "Epoch 9/30 - RMSE: 0.9675\n",
      "Epoch 10/30 - RMSE: 0.9625\n",
      "Epoch 11/30 - RMSE: 0.9582\n",
      "Epoch 12/30 - RMSE: 0.9544\n",
      "Epoch 13/30 - RMSE: 0.9511\n",
      "Epoch 14/30 - RMSE: 0.9481\n",
      "Epoch 15/30 - RMSE: 0.9453\n",
      "Epoch 16/30 - RMSE: 0.9428\n",
      "Epoch 17/30 - RMSE: 0.9405\n",
      "Epoch 18/30 - RMSE: 0.9384\n",
      "Epoch 19/30 - RMSE: 0.9364\n",
      "Epoch 20/30 - RMSE: 0.9346\n",
      "Epoch 21/30 - RMSE: 0.9329\n",
      "Epoch 22/30 - RMSE: 0.9312\n",
      "Epoch 23/30 - RMSE: 0.9297\n",
      "Epoch 24/30 - RMSE: 0.9282\n",
      "Epoch 25/30 - RMSE: 0.9268\n",
      "Epoch 26/30 - RMSE: 0.9255\n",
      "Epoch 27/30 - RMSE: 0.9242\n",
      "Epoch 28/30 - RMSE: 0.9230\n",
      "Epoch 29/30 - RMSE: 0.9218\n",
      "Epoch 30/30 - RMSE: 0.9207\n",
      "(factors=20, lr=0.001, reg=0.01, ep=30) - RMSE=0.969771033068033\n",
      "Epoch 1/40 - RMSE: 1.0948\n",
      "Epoch 2/40 - RMSE: 1.0562\n",
      "Epoch 3/40 - RMSE: 1.0307\n",
      "Epoch 4/40 - RMSE: 1.0126\n",
      "Epoch 5/40 - RMSE: 0.9990\n",
      "Epoch 6/40 - RMSE: 0.9885\n",
      "Epoch 7/40 - RMSE: 0.9800\n",
      "Epoch 8/40 - RMSE: 0.9731\n",
      "Epoch 9/40 - RMSE: 0.9673\n",
      "Epoch 10/40 - RMSE: 0.9623\n",
      "Epoch 11/40 - RMSE: 0.9580\n",
      "Epoch 12/40 - RMSE: 0.9542\n",
      "Epoch 13/40 - RMSE: 0.9508\n",
      "Epoch 14/40 - RMSE: 0.9478\n",
      "Epoch 15/40 - RMSE: 0.9450\n",
      "Epoch 16/40 - RMSE: 0.9425\n",
      "Epoch 17/40 - RMSE: 0.9402\n",
      "Epoch 18/40 - RMSE: 0.9380\n",
      "Epoch 19/40 - RMSE: 0.9360\n",
      "Epoch 20/40 - RMSE: 0.9341\n",
      "Epoch 21/40 - RMSE: 0.9324\n",
      "Epoch 22/40 - RMSE: 0.9307\n",
      "Epoch 23/40 - RMSE: 0.9292\n",
      "Epoch 24/40 - RMSE: 0.9277\n",
      "Epoch 25/40 - RMSE: 0.9263\n",
      "Epoch 26/40 - RMSE: 0.9249\n",
      "Epoch 27/40 - RMSE: 0.9236\n",
      "Epoch 28/40 - RMSE: 0.9223\n",
      "Epoch 29/40 - RMSE: 0.9211\n",
      "Epoch 30/40 - RMSE: 0.9200\n",
      "Epoch 31/40 - RMSE: 0.9188\n",
      "Epoch 32/40 - RMSE: 0.9177\n",
      "Epoch 33/40 - RMSE: 0.9166\n",
      "Epoch 34/40 - RMSE: 0.9156\n",
      "Epoch 35/40 - RMSE: 0.9145\n",
      "Epoch 36/40 - RMSE: 0.9135\n",
      "Epoch 37/40 - RMSE: 0.9125\n",
      "Epoch 38/40 - RMSE: 0.9115\n",
      "Epoch 39/40 - RMSE: 0.9105\n",
      "Epoch 40/40 - RMSE: 0.9096\n",
      "(factors=20, lr=0.001, reg=0.01, ep=40) - RMSE=0.9647746322251729\n",
      "Epoch 1/50 - RMSE: 1.0948\n",
      "Epoch 2/50 - RMSE: 1.0563\n",
      "Epoch 3/50 - RMSE: 1.0308\n",
      "Epoch 4/50 - RMSE: 1.0126\n",
      "Epoch 5/50 - RMSE: 0.9991\n",
      "Epoch 6/50 - RMSE: 0.9886\n",
      "Epoch 7/50 - RMSE: 0.9801\n",
      "Epoch 8/50 - RMSE: 0.9732\n",
      "Epoch 9/50 - RMSE: 0.9674\n",
      "Epoch 10/50 - RMSE: 0.9624\n",
      "Epoch 11/50 - RMSE: 0.9581\n",
      "Epoch 12/50 - RMSE: 0.9543\n",
      "Epoch 13/50 - RMSE: 0.9509\n",
      "Epoch 14/50 - RMSE: 0.9479\n",
      "Epoch 15/50 - RMSE: 0.9451\n",
      "Epoch 16/50 - RMSE: 0.9426\n",
      "Epoch 17/50 - RMSE: 0.9403\n",
      "Epoch 18/50 - RMSE: 0.9382\n",
      "Epoch 19/50 - RMSE: 0.9362\n",
      "Epoch 20/50 - RMSE: 0.9343\n",
      "Epoch 21/50 - RMSE: 0.9326\n",
      "Epoch 22/50 - RMSE: 0.9309\n",
      "Epoch 23/50 - RMSE: 0.9294\n",
      "Epoch 24/50 - RMSE: 0.9279\n",
      "Epoch 25/50 - RMSE: 0.9265\n",
      "Epoch 26/50 - RMSE: 0.9252\n",
      "Epoch 27/50 - RMSE: 0.9239\n",
      "Epoch 28/50 - RMSE: 0.9226\n",
      "Epoch 29/50 - RMSE: 0.9214\n",
      "Epoch 30/50 - RMSE: 0.9203\n",
      "Epoch 31/50 - RMSE: 0.9192\n",
      "Epoch 32/50 - RMSE: 0.9181\n",
      "Epoch 33/50 - RMSE: 0.9170\n",
      "Epoch 34/50 - RMSE: 0.9160\n",
      "Epoch 35/50 - RMSE: 0.9150\n",
      "Epoch 36/50 - RMSE: 0.9140\n",
      "Epoch 37/50 - RMSE: 0.9130\n",
      "Epoch 38/50 - RMSE: 0.9120\n",
      "Epoch 39/50 - RMSE: 0.9110\n",
      "Epoch 40/50 - RMSE: 0.9101\n",
      "Epoch 41/50 - RMSE: 0.9092\n",
      "Epoch 42/50 - RMSE: 0.9082\n",
      "Epoch 43/50 - RMSE: 0.9073\n",
      "Epoch 44/50 - RMSE: 0.9064\n",
      "Epoch 45/50 - RMSE: 0.9055\n",
      "Epoch 46/50 - RMSE: 0.9045\n",
      "Epoch 47/50 - RMSE: 0.9036\n",
      "Epoch 48/50 - RMSE: 0.9027\n",
      "Epoch 49/50 - RMSE: 0.9017\n",
      "Epoch 50/50 - RMSE: 0.9008\n",
      "(factors=20, lr=0.001, reg=0.01, ep=50) - RMSE=0.9608176741430784\n",
      "Epoch 1/10 - RMSE: 1.0948\n",
      "Epoch 2/10 - RMSE: 1.0568\n",
      "Epoch 3/10 - RMSE: 1.0319\n",
      "Epoch 4/10 - RMSE: 1.0141\n",
      "Epoch 5/10 - RMSE: 1.0009\n",
      "Epoch 6/10 - RMSE: 0.9907\n",
      "Epoch 7/10 - RMSE: 0.9825\n",
      "Epoch 8/10 - RMSE: 0.9758\n",
      "Epoch 9/10 - RMSE: 0.9701\n",
      "Epoch 10/10 - RMSE: 0.9654\n",
      "(factors=20, lr=0.001, reg=0.1, ep=10) - RMSE=1.0018711055334222\n",
      "Epoch 1/20 - RMSE: 1.0951\n",
      "Epoch 2/20 - RMSE: 1.0571\n",
      "Epoch 3/20 - RMSE: 1.0322\n",
      "Epoch 4/20 - RMSE: 1.0145\n",
      "Epoch 5/20 - RMSE: 1.0013\n",
      "Epoch 6/20 - RMSE: 0.9910\n",
      "Epoch 7/20 - RMSE: 0.9828\n",
      "Epoch 8/20 - RMSE: 0.9761\n",
      "Epoch 9/20 - RMSE: 0.9705\n",
      "Epoch 10/20 - RMSE: 0.9657\n",
      "Epoch 11/20 - RMSE: 0.9615\n",
      "Epoch 12/20 - RMSE: 0.9579\n",
      "Epoch 13/20 - RMSE: 0.9547\n",
      "Epoch 14/20 - RMSE: 0.9519\n",
      "Epoch 15/20 - RMSE: 0.9493\n",
      "Epoch 16/20 - RMSE: 0.9469\n",
      "Epoch 17/20 - RMSE: 0.9448\n",
      "Epoch 18/20 - RMSE: 0.9429\n",
      "Epoch 19/20 - RMSE: 0.9411\n",
      "Epoch 20/20 - RMSE: 0.9395\n",
      "(factors=20, lr=0.001, reg=0.1, ep=20) - RMSE=0.9800043507919164\n",
      "Epoch 1/30 - RMSE: 1.0947\n",
      "Epoch 2/30 - RMSE: 1.0567\n",
      "Epoch 3/30 - RMSE: 1.0318\n",
      "Epoch 4/30 - RMSE: 1.0141\n",
      "Epoch 5/30 - RMSE: 1.0009\n",
      "Epoch 6/30 - RMSE: 0.9907\n",
      "Epoch 7/30 - RMSE: 0.9825\n",
      "Epoch 8/30 - RMSE: 0.9757\n",
      "Epoch 9/30 - RMSE: 0.9701\n",
      "Epoch 10/30 - RMSE: 0.9653\n",
      "Epoch 11/30 - RMSE: 0.9612\n",
      "Epoch 12/30 - RMSE: 0.9575\n",
      "Epoch 13/30 - RMSE: 0.9544\n",
      "Epoch 14/30 - RMSE: 0.9515\n",
      "Epoch 15/30 - RMSE: 0.9489\n",
      "Epoch 16/30 - RMSE: 0.9466\n",
      "Epoch 17/30 - RMSE: 0.9445\n",
      "Epoch 18/30 - RMSE: 0.9426\n",
      "Epoch 19/30 - RMSE: 0.9408\n",
      "Epoch 20/30 - RMSE: 0.9392\n",
      "Epoch 21/30 - RMSE: 0.9377\n",
      "Epoch 22/30 - RMSE: 0.9363\n",
      "Epoch 23/30 - RMSE: 0.9350\n",
      "Epoch 24/30 - RMSE: 0.9338\n",
      "Epoch 25/30 - RMSE: 0.9326\n",
      "Epoch 26/30 - RMSE: 0.9316\n",
      "Epoch 27/30 - RMSE: 0.9306\n",
      "Epoch 28/30 - RMSE: 0.9296\n",
      "Epoch 29/30 - RMSE: 0.9287\n",
      "Epoch 30/30 - RMSE: 0.9279\n",
      "(factors=20, lr=0.001, reg=0.1, ep=30) - RMSE=0.9713821366364321\n",
      "Epoch 1/40 - RMSE: 1.0952\n",
      "Epoch 2/40 - RMSE: 1.0572\n",
      "Epoch 3/40 - RMSE: 1.0322\n",
      "Epoch 4/40 - RMSE: 1.0145\n",
      "Epoch 5/40 - RMSE: 1.0013\n",
      "Epoch 6/40 - RMSE: 0.9910\n",
      "Epoch 7/40 - RMSE: 0.9828\n",
      "Epoch 8/40 - RMSE: 0.9761\n",
      "Epoch 9/40 - RMSE: 0.9704\n",
      "Epoch 10/40 - RMSE: 0.9656\n",
      "Epoch 11/40 - RMSE: 0.9615\n",
      "Epoch 12/40 - RMSE: 0.9579\n",
      "Epoch 13/40 - RMSE: 0.9546\n",
      "Epoch 14/40 - RMSE: 0.9518\n",
      "Epoch 15/40 - RMSE: 0.9492\n",
      "Epoch 16/40 - RMSE: 0.9469\n",
      "Epoch 17/40 - RMSE: 0.9448\n",
      "Epoch 18/40 - RMSE: 0.9429\n",
      "Epoch 19/40 - RMSE: 0.9411\n",
      "Epoch 20/40 - RMSE: 0.9394\n",
      "Epoch 21/40 - RMSE: 0.9379\n",
      "Epoch 22/40 - RMSE: 0.9365\n",
      "Epoch 23/40 - RMSE: 0.9352\n",
      "Epoch 24/40 - RMSE: 0.9340\n",
      "Epoch 25/40 - RMSE: 0.9329\n",
      "Epoch 26/40 - RMSE: 0.9318\n",
      "Epoch 27/40 - RMSE: 0.9308\n",
      "Epoch 28/40 - RMSE: 0.9298\n",
      "Epoch 29/40 - RMSE: 0.9289\n",
      "Epoch 30/40 - RMSE: 0.9281\n",
      "Epoch 31/40 - RMSE: 0.9273\n",
      "Epoch 32/40 - RMSE: 0.9265\n",
      "Epoch 33/40 - RMSE: 0.9258\n",
      "Epoch 34/40 - RMSE: 0.9251\n",
      "Epoch 35/40 - RMSE: 0.9245\n",
      "Epoch 36/40 - RMSE: 0.9238\n",
      "Epoch 37/40 - RMSE: 0.9232\n",
      "Epoch 38/40 - RMSE: 0.9226\n",
      "Epoch 39/40 - RMSE: 0.9221\n",
      "Epoch 40/40 - RMSE: 0.9216\n",
      "(factors=20, lr=0.001, reg=0.1, ep=40) - RMSE=0.9661685631213605\n",
      "Epoch 1/50 - RMSE: 1.0951\n",
      "Epoch 2/50 - RMSE: 1.0571\n",
      "Epoch 3/50 - RMSE: 1.0322\n",
      "Epoch 4/50 - RMSE: 1.0145\n",
      "Epoch 5/50 - RMSE: 1.0013\n",
      "Epoch 6/50 - RMSE: 0.9910\n",
      "Epoch 7/50 - RMSE: 0.9828\n",
      "Epoch 8/50 - RMSE: 0.9761\n",
      "Epoch 9/50 - RMSE: 0.9704\n",
      "Epoch 10/50 - RMSE: 0.9656\n",
      "Epoch 11/50 - RMSE: 0.9615\n",
      "Epoch 12/50 - RMSE: 0.9578\n",
      "Epoch 13/50 - RMSE: 0.9546\n",
      "Epoch 14/50 - RMSE: 0.9518\n",
      "Epoch 15/50 - RMSE: 0.9492\n",
      "Epoch 16/50 - RMSE: 0.9469\n",
      "Epoch 17/50 - RMSE: 0.9448\n",
      "Epoch 18/50 - RMSE: 0.9429\n",
      "Epoch 19/50 - RMSE: 0.9411\n",
      "Epoch 20/50 - RMSE: 0.9395\n",
      "Epoch 21/50 - RMSE: 0.9379\n",
      "Epoch 22/50 - RMSE: 0.9365\n",
      "Epoch 23/50 - RMSE: 0.9353\n",
      "Epoch 24/50 - RMSE: 0.9340\n",
      "Epoch 25/50 - RMSE: 0.9329\n",
      "Epoch 26/50 - RMSE: 0.9318\n",
      "Epoch 27/50 - RMSE: 0.9308\n",
      "Epoch 28/50 - RMSE: 0.9299\n",
      "Epoch 29/50 - RMSE: 0.9290\n",
      "Epoch 30/50 - RMSE: 0.9281\n",
      "Epoch 31/50 - RMSE: 0.9273\n",
      "Epoch 32/50 - RMSE: 0.9266\n",
      "Epoch 33/50 - RMSE: 0.9258\n",
      "Epoch 34/50 - RMSE: 0.9251\n",
      "Epoch 35/50 - RMSE: 0.9245\n",
      "Epoch 36/50 - RMSE: 0.9239\n",
      "Epoch 37/50 - RMSE: 0.9233\n",
      "Epoch 38/50 - RMSE: 0.9227\n",
      "Epoch 39/50 - RMSE: 0.9221\n",
      "Epoch 40/50 - RMSE: 0.9216\n",
      "Epoch 41/50 - RMSE: 0.9211\n",
      "Epoch 42/50 - RMSE: 0.9206\n",
      "Epoch 43/50 - RMSE: 0.9201\n",
      "Epoch 44/50 - RMSE: 0.9197\n",
      "Epoch 45/50 - RMSE: 0.9193\n",
      "Epoch 46/50 - RMSE: 0.9188\n",
      "Epoch 47/50 - RMSE: 0.9185\n",
      "Epoch 48/50 - RMSE: 0.9181\n",
      "Epoch 49/50 - RMSE: 0.9177\n",
      "Epoch 50/50 - RMSE: 0.9173\n",
      "(factors=20, lr=0.001, reg=0.1, ep=50) - RMSE=0.9632402804374971\n",
      "Epoch 1/10 - RMSE: 1.0436\n",
      "Epoch 2/10 - RMSE: 0.9782\n",
      "Epoch 3/10 - RMSE: 0.9550\n",
      "Epoch 4/10 - RMSE: 0.9420\n",
      "Epoch 5/10 - RMSE: 0.9330\n",
      "Epoch 6/10 - RMSE: 0.9261\n",
      "Epoch 7/10 - RMSE: 0.9203\n",
      "Epoch 8/10 - RMSE: 0.9151\n",
      "Epoch 9/10 - RMSE: 0.9101\n",
      "Epoch 10/10 - RMSE: 0.9052\n",
      "(factors=20, lr=0.005, reg=0.001, ep=10) - RMSE=0.9611374075225195\n",
      "Epoch 1/20 - RMSE: 1.0431\n",
      "Epoch 2/20 - RMSE: 0.9778\n",
      "Epoch 3/20 - RMSE: 0.9545\n",
      "Epoch 4/20 - RMSE: 0.9414\n",
      "Epoch 5/20 - RMSE: 0.9323\n",
      "Epoch 6/20 - RMSE: 0.9251\n",
      "Epoch 7/20 - RMSE: 0.9191\n",
      "Epoch 8/20 - RMSE: 0.9135\n",
      "Epoch 9/20 - RMSE: 0.9082\n",
      "Epoch 10/20 - RMSE: 0.9028\n",
      "Epoch 11/20 - RMSE: 0.8972\n",
      "Epoch 12/20 - RMSE: 0.8911\n",
      "Epoch 13/20 - RMSE: 0.8845\n",
      "Epoch 14/20 - RMSE: 0.8774\n",
      "Epoch 15/20 - RMSE: 0.8697\n",
      "Epoch 16/20 - RMSE: 0.8615\n",
      "Epoch 17/20 - RMSE: 0.8528\n",
      "Epoch 18/20 - RMSE: 0.8439\n",
      "Epoch 19/20 - RMSE: 0.8347\n",
      "Epoch 20/20 - RMSE: 0.8255\n",
      "(factors=20, lr=0.005, reg=0.001, ep=20) - RMSE=0.9522435448500597\n",
      "Epoch 1/30 - RMSE: 1.0434\n",
      "Epoch 2/30 - RMSE: 0.9780\n",
      "Epoch 3/30 - RMSE: 0.9548\n",
      "Epoch 4/30 - RMSE: 0.9417\n",
      "Epoch 5/30 - RMSE: 0.9327\n",
      "Epoch 6/30 - RMSE: 0.9257\n",
      "Epoch 7/30 - RMSE: 0.9198\n",
      "Epoch 8/30 - RMSE: 0.9144\n",
      "Epoch 9/30 - RMSE: 0.9093\n",
      "Epoch 10/30 - RMSE: 0.9043\n",
      "Epoch 11/30 - RMSE: 0.8991\n",
      "Epoch 12/30 - RMSE: 0.8935\n",
      "Epoch 13/30 - RMSE: 0.8875\n",
      "Epoch 14/30 - RMSE: 0.8809\n",
      "Epoch 15/30 - RMSE: 0.8738\n",
      "Epoch 16/30 - RMSE: 0.8659\n",
      "Epoch 17/30 - RMSE: 0.8575\n",
      "Epoch 18/30 - RMSE: 0.8486\n",
      "Epoch 19/30 - RMSE: 0.8393\n",
      "Epoch 20/30 - RMSE: 0.8297\n",
      "Epoch 21/30 - RMSE: 0.8201\n",
      "Epoch 22/30 - RMSE: 0.8104\n",
      "Epoch 23/30 - RMSE: 0.8009\n",
      "Epoch 24/30 - RMSE: 0.7915\n",
      "Epoch 25/30 - RMSE: 0.7824\n",
      "Epoch 26/30 - RMSE: 0.7736\n",
      "Epoch 27/30 - RMSE: 0.7650\n",
      "Epoch 28/30 - RMSE: 0.7567\n",
      "Epoch 29/30 - RMSE: 0.7488\n",
      "Epoch 30/30 - RMSE: 0.7411\n",
      "(factors=20, lr=0.005, reg=0.001, ep=30) - RMSE=0.9681557725180289\n",
      "Epoch 1/40 - RMSE: 1.0434\n",
      "Epoch 2/40 - RMSE: 0.9781\n",
      "Epoch 3/40 - RMSE: 0.9549\n",
      "Epoch 4/40 - RMSE: 0.9419\n",
      "Epoch 5/40 - RMSE: 0.9329\n",
      "Epoch 6/40 - RMSE: 0.9259\n",
      "Epoch 7/40 - RMSE: 0.9201\n",
      "Epoch 8/40 - RMSE: 0.9149\n",
      "Epoch 9/40 - RMSE: 0.9100\n",
      "Epoch 10/40 - RMSE: 0.9051\n",
      "Epoch 11/40 - RMSE: 0.9001\n",
      "Epoch 12/40 - RMSE: 0.8949\n",
      "Epoch 13/40 - RMSE: 0.8893\n",
      "Epoch 14/40 - RMSE: 0.8831\n",
      "Epoch 15/40 - RMSE: 0.8764\n",
      "Epoch 16/40 - RMSE: 0.8690\n",
      "Epoch 17/40 - RMSE: 0.8610\n",
      "Epoch 18/40 - RMSE: 0.8524\n",
      "Epoch 19/40 - RMSE: 0.8433\n",
      "Epoch 20/40 - RMSE: 0.8338\n",
      "Epoch 21/40 - RMSE: 0.8241\n",
      "Epoch 22/40 - RMSE: 0.8142\n",
      "Epoch 23/40 - RMSE: 0.8045\n",
      "Epoch 24/40 - RMSE: 0.7949\n",
      "Epoch 25/40 - RMSE: 0.7856\n",
      "Epoch 26/40 - RMSE: 0.7766\n",
      "Epoch 27/40 - RMSE: 0.7677\n",
      "Epoch 28/40 - RMSE: 0.7593\n",
      "Epoch 29/40 - RMSE: 0.7513\n",
      "Epoch 30/40 - RMSE: 0.7435\n",
      "Epoch 31/40 - RMSE: 0.7361\n",
      "Epoch 32/40 - RMSE: 0.7289\n",
      "Epoch 33/40 - RMSE: 0.7221\n",
      "Epoch 34/40 - RMSE: 0.7156\n",
      "Epoch 35/40 - RMSE: 0.7093\n",
      "Epoch 36/40 - RMSE: 0.7034\n",
      "Epoch 37/40 - RMSE: 0.6976\n",
      "Epoch 38/40 - RMSE: 0.6921\n",
      "Epoch 39/40 - RMSE: 0.6869\n",
      "Epoch 40/40 - RMSE: 0.6818\n",
      "(factors=20, lr=0.005, reg=0.001, ep=40) - RMSE=0.9902365133163831\n",
      "Epoch 1/50 - RMSE: 1.0433\n",
      "Epoch 2/50 - RMSE: 0.9781\n",
      "Epoch 3/50 - RMSE: 0.9549\n",
      "Epoch 4/50 - RMSE: 0.9420\n",
      "Epoch 5/50 - RMSE: 0.9331\n",
      "Epoch 6/50 - RMSE: 0.9263\n",
      "Epoch 7/50 - RMSE: 0.9205\n",
      "Epoch 8/50 - RMSE: 0.9153\n",
      "Epoch 9/50 - RMSE: 0.9104\n",
      "Epoch 10/50 - RMSE: 0.9055\n",
      "Epoch 11/50 - RMSE: 0.9005\n",
      "Epoch 12/50 - RMSE: 0.8952\n",
      "Epoch 13/50 - RMSE: 0.8895\n",
      "Epoch 14/50 - RMSE: 0.8831\n",
      "Epoch 15/50 - RMSE: 0.8763\n",
      "Epoch 16/50 - RMSE: 0.8688\n",
      "Epoch 17/50 - RMSE: 0.8607\n",
      "Epoch 18/50 - RMSE: 0.8521\n",
      "Epoch 19/50 - RMSE: 0.8431\n",
      "Epoch 20/50 - RMSE: 0.8337\n",
      "Epoch 21/50 - RMSE: 0.8243\n",
      "Epoch 22/50 - RMSE: 0.8148\n",
      "Epoch 23/50 - RMSE: 0.8055\n",
      "Epoch 24/50 - RMSE: 0.7962\n",
      "Epoch 25/50 - RMSE: 0.7872\n",
      "Epoch 26/50 - RMSE: 0.7784\n",
      "Epoch 27/50 - RMSE: 0.7699\n",
      "Epoch 28/50 - RMSE: 0.7617\n",
      "Epoch 29/50 - RMSE: 0.7538\n",
      "Epoch 30/50 - RMSE: 0.7461\n",
      "Epoch 31/50 - RMSE: 0.7389\n",
      "Epoch 32/50 - RMSE: 0.7319\n",
      "Epoch 33/50 - RMSE: 0.7252\n",
      "Epoch 34/50 - RMSE: 0.7187\n",
      "Epoch 35/50 - RMSE: 0.7126\n",
      "Epoch 36/50 - RMSE: 0.7067\n",
      "Epoch 37/50 - RMSE: 0.7011\n",
      "Epoch 38/50 - RMSE: 0.6957\n",
      "Epoch 39/50 - RMSE: 0.6906\n",
      "Epoch 40/50 - RMSE: 0.6856\n",
      "Epoch 41/50 - RMSE: 0.6809\n",
      "Epoch 42/50 - RMSE: 0.6763\n",
      "Epoch 43/50 - RMSE: 0.6719\n",
      "Epoch 44/50 - RMSE: 0.6677\n",
      "Epoch 45/50 - RMSE: 0.6637\n",
      "Epoch 46/50 - RMSE: 0.6598\n",
      "Epoch 47/50 - RMSE: 0.6561\n",
      "Epoch 48/50 - RMSE: 0.6525\n",
      "Epoch 49/50 - RMSE: 0.6490\n",
      "Epoch 50/50 - RMSE: 0.6457\n",
      "(factors=20, lr=0.005, reg=0.001, ep=50) - RMSE=1.0204350889153302\n",
      "Epoch 1/10 - RMSE: 1.0434\n",
      "Epoch 2/10 - RMSE: 0.9781\n",
      "Epoch 3/10 - RMSE: 0.9550\n",
      "Epoch 4/10 - RMSE: 0.9421\n",
      "Epoch 5/10 - RMSE: 0.9332\n",
      "Epoch 6/10 - RMSE: 0.9265\n",
      "Epoch 7/10 - RMSE: 0.9209\n",
      "Epoch 8/10 - RMSE: 0.9159\n",
      "Epoch 9/10 - RMSE: 0.9113\n",
      "Epoch 10/10 - RMSE: 0.9069\n",
      "(factors=20, lr=0.005, reg=0.01, ep=10) - RMSE=0.9613943919276212\n",
      "Epoch 1/20 - RMSE: 1.0435\n",
      "Epoch 2/20 - RMSE: 0.9784\n",
      "Epoch 3/20 - RMSE: 0.9552\n",
      "Epoch 4/20 - RMSE: 0.9421\n",
      "Epoch 5/20 - RMSE: 0.9333\n",
      "Epoch 6/20 - RMSE: 0.9265\n",
      "Epoch 7/20 - RMSE: 0.9208\n",
      "Epoch 8/20 - RMSE: 0.9158\n",
      "Epoch 9/20 - RMSE: 0.9111\n",
      "Epoch 10/20 - RMSE: 0.9065\n",
      "Epoch 11/20 - RMSE: 0.9018\n",
      "Epoch 12/20 - RMSE: 0.8969\n",
      "Epoch 13/20 - RMSE: 0.8916\n",
      "Epoch 14/20 - RMSE: 0.8859\n",
      "Epoch 15/20 - RMSE: 0.8798\n",
      "Epoch 16/20 - RMSE: 0.8730\n",
      "Epoch 17/20 - RMSE: 0.8658\n",
      "Epoch 18/20 - RMSE: 0.8580\n",
      "Epoch 19/20 - RMSE: 0.8500\n",
      "Epoch 20/20 - RMSE: 0.8416\n",
      "(factors=20, lr=0.005, reg=0.01, ep=20) - RMSE=0.9506124547075236\n",
      "Epoch 1/30 - RMSE: 1.0435\n",
      "Epoch 2/30 - RMSE: 0.9783\n",
      "Epoch 3/30 - RMSE: 0.9551\n",
      "Epoch 4/30 - RMSE: 0.9422\n",
      "Epoch 5/30 - RMSE: 0.9333\n",
      "Epoch 6/30 - RMSE: 0.9265\n",
      "Epoch 7/30 - RMSE: 0.9209\n",
      "Epoch 8/30 - RMSE: 0.9159\n",
      "Epoch 9/30 - RMSE: 0.9112\n",
      "Epoch 10/30 - RMSE: 0.9067\n",
      "Epoch 11/30 - RMSE: 0.9020\n",
      "Epoch 12/30 - RMSE: 0.8971\n",
      "Epoch 13/30 - RMSE: 0.8918\n",
      "Epoch 14/30 - RMSE: 0.8861\n",
      "Epoch 15/30 - RMSE: 0.8798\n",
      "Epoch 16/30 - RMSE: 0.8731\n",
      "Epoch 17/30 - RMSE: 0.8658\n",
      "Epoch 18/30 - RMSE: 0.8580\n",
      "Epoch 19/30 - RMSE: 0.8498\n",
      "Epoch 20/30 - RMSE: 0.8414\n",
      "Epoch 21/30 - RMSE: 0.8327\n",
      "Epoch 22/30 - RMSE: 0.8239\n",
      "Epoch 23/30 - RMSE: 0.8150\n",
      "Epoch 24/30 - RMSE: 0.8062\n",
      "Epoch 25/30 - RMSE: 0.7975\n",
      "Epoch 26/30 - RMSE: 0.7889\n",
      "Epoch 27/30 - RMSE: 0.7804\n",
      "Epoch 28/30 - RMSE: 0.7722\n",
      "Epoch 29/30 - RMSE: 0.7641\n",
      "Epoch 30/30 - RMSE: 0.7563\n",
      "(factors=20, lr=0.005, reg=0.01, ep=30) - RMSE=0.9503960488259813\n",
      "Epoch 1/40 - RMSE: 1.0436\n",
      "Epoch 2/40 - RMSE: 0.9785\n",
      "Epoch 3/40 - RMSE: 0.9554\n",
      "Epoch 4/40 - RMSE: 0.9425\n",
      "Epoch 5/40 - RMSE: 0.9337\n",
      "Epoch 6/40 - RMSE: 0.9270\n",
      "Epoch 7/40 - RMSE: 0.9214\n",
      "Epoch 8/40 - RMSE: 0.9165\n",
      "Epoch 9/40 - RMSE: 0.9121\n",
      "Epoch 10/40 - RMSE: 0.9076\n",
      "Epoch 11/40 - RMSE: 0.9031\n",
      "Epoch 12/40 - RMSE: 0.8985\n",
      "Epoch 13/40 - RMSE: 0.8934\n",
      "Epoch 14/40 - RMSE: 0.8880\n",
      "Epoch 15/40 - RMSE: 0.8822\n",
      "Epoch 16/40 - RMSE: 0.8757\n",
      "Epoch 17/40 - RMSE: 0.8687\n",
      "Epoch 18/40 - RMSE: 0.8612\n",
      "Epoch 19/40 - RMSE: 0.8533\n",
      "Epoch 20/40 - RMSE: 0.8450\n",
      "Epoch 21/40 - RMSE: 0.8365\n",
      "Epoch 22/40 - RMSE: 0.8278\n",
      "Epoch 23/40 - RMSE: 0.8190\n",
      "Epoch 24/40 - RMSE: 0.8103\n",
      "Epoch 25/40 - RMSE: 0.8016\n",
      "Epoch 26/40 - RMSE: 0.7929\n",
      "Epoch 27/40 - RMSE: 0.7845\n",
      "Epoch 28/40 - RMSE: 0.7763\n",
      "Epoch 29/40 - RMSE: 0.7682\n",
      "Epoch 30/40 - RMSE: 0.7603\n",
      "Epoch 31/40 - RMSE: 0.7527\n",
      "Epoch 32/40 - RMSE: 0.7454\n",
      "Epoch 33/40 - RMSE: 0.7383\n",
      "Epoch 34/40 - RMSE: 0.7315\n",
      "Epoch 35/40 - RMSE: 0.7250\n",
      "Epoch 36/40 - RMSE: 0.7186\n",
      "Epoch 37/40 - RMSE: 0.7126\n",
      "Epoch 38/40 - RMSE: 0.7068\n",
      "Epoch 39/40 - RMSE: 0.7012\n",
      "Epoch 40/40 - RMSE: 0.6959\n",
      "(factors=20, lr=0.005, reg=0.01, ep=40) - RMSE=0.9662677047985185\n",
      "Epoch 1/50 - RMSE: 1.0434\n",
      "Epoch 2/50 - RMSE: 0.9781\n",
      "Epoch 3/50 - RMSE: 0.9549\n",
      "Epoch 4/50 - RMSE: 0.9419\n",
      "Epoch 5/50 - RMSE: 0.9331\n",
      "Epoch 6/50 - RMSE: 0.9263\n",
      "Epoch 7/50 - RMSE: 0.9207\n",
      "Epoch 8/50 - RMSE: 0.9158\n",
      "Epoch 9/50 - RMSE: 0.9111\n",
      "Epoch 10/50 - RMSE: 0.9066\n",
      "Epoch 11/50 - RMSE: 0.9020\n",
      "Epoch 12/50 - RMSE: 0.8973\n",
      "Epoch 13/50 - RMSE: 0.8922\n",
      "Epoch 14/50 - RMSE: 0.8866\n",
      "Epoch 15/50 - RMSE: 0.8806\n",
      "Epoch 16/50 - RMSE: 0.8740\n",
      "Epoch 17/50 - RMSE: 0.8668\n",
      "Epoch 18/50 - RMSE: 0.8591\n",
      "Epoch 19/50 - RMSE: 0.8509\n",
      "Epoch 20/50 - RMSE: 0.8424\n",
      "Epoch 21/50 - RMSE: 0.8336\n",
      "Epoch 22/50 - RMSE: 0.8246\n",
      "Epoch 23/50 - RMSE: 0.8155\n",
      "Epoch 24/50 - RMSE: 0.8065\n",
      "Epoch 25/50 - RMSE: 0.7975\n",
      "Epoch 26/50 - RMSE: 0.7887\n",
      "Epoch 27/50 - RMSE: 0.7801\n",
      "Epoch 28/50 - RMSE: 0.7717\n",
      "Epoch 29/50 - RMSE: 0.7636\n",
      "Epoch 30/50 - RMSE: 0.7556\n",
      "Epoch 31/50 - RMSE: 0.7480\n",
      "Epoch 32/50 - RMSE: 0.7406\n",
      "Epoch 33/50 - RMSE: 0.7334\n",
      "Epoch 34/50 - RMSE: 0.7266\n",
      "Epoch 35/50 - RMSE: 0.7201\n",
      "Epoch 36/50 - RMSE: 0.7138\n",
      "Epoch 37/50 - RMSE: 0.7077\n",
      "Epoch 38/50 - RMSE: 0.7019\n",
      "Epoch 39/50 - RMSE: 0.6963\n",
      "Epoch 40/50 - RMSE: 0.6910\n",
      "Epoch 41/50 - RMSE: 0.6858\n",
      "Epoch 42/50 - RMSE: 0.6809\n",
      "Epoch 43/50 - RMSE: 0.6762\n",
      "Epoch 44/50 - RMSE: 0.6717\n",
      "Epoch 45/50 - RMSE: 0.6673\n",
      "Epoch 46/50 - RMSE: 0.6632\n",
      "Epoch 47/50 - RMSE: 0.6591\n",
      "Epoch 48/50 - RMSE: 0.6553\n",
      "Epoch 49/50 - RMSE: 0.6516\n",
      "Epoch 50/50 - RMSE: 0.6480\n",
      "(factors=20, lr=0.005, reg=0.01, ep=50) - RMSE=0.988336924205299\n",
      "Epoch 1/10 - RMSE: 1.0445\n",
      "Epoch 2/10 - RMSE: 0.9808\n",
      "Epoch 3/10 - RMSE: 0.9584\n",
      "Epoch 4/10 - RMSE: 0.9464\n",
      "Epoch 5/10 - RMSE: 0.9386\n",
      "Epoch 6/10 - RMSE: 0.9331\n",
      "Epoch 7/10 - RMSE: 0.9290\n",
      "Epoch 8/10 - RMSE: 0.9259\n",
      "Epoch 9/10 - RMSE: 0.9232\n",
      "Epoch 10/10 - RMSE: 0.9211\n",
      "(factors=20, lr=0.005, reg=0.1, ep=10) - RMSE=0.96330056749972\n",
      "Epoch 1/20 - RMSE: 1.0445\n",
      "Epoch 2/20 - RMSE: 0.9809\n",
      "Epoch 3/20 - RMSE: 0.9586\n",
      "Epoch 4/20 - RMSE: 0.9466\n",
      "Epoch 5/20 - RMSE: 0.9387\n",
      "Epoch 6/20 - RMSE: 0.9333\n",
      "Epoch 7/20 - RMSE: 0.9293\n",
      "Epoch 8/20 - RMSE: 0.9261\n",
      "Epoch 9/20 - RMSE: 0.9234\n",
      "Epoch 10/20 - RMSE: 0.9215\n",
      "Epoch 11/20 - RMSE: 0.9195\n",
      "Epoch 12/20 - RMSE: 0.9179\n",
      "Epoch 13/20 - RMSE: 0.9165\n",
      "Epoch 14/20 - RMSE: 0.9153\n",
      "Epoch 15/20 - RMSE: 0.9140\n",
      "Epoch 16/20 - RMSE: 0.9128\n",
      "Epoch 17/20 - RMSE: 0.9117\n",
      "Epoch 18/20 - RMSE: 0.9105\n",
      "Epoch 19/20 - RMSE: 0.9095\n",
      "Epoch 20/20 - RMSE: 0.9083\n",
      "(factors=20, lr=0.005, reg=0.1, ep=20) - RMSE=0.9557111754306621\n",
      "Epoch 1/30 - RMSE: 1.0444\n",
      "Epoch 2/30 - RMSE: 0.9809\n",
      "Epoch 3/30 - RMSE: 0.9586\n",
      "Epoch 4/30 - RMSE: 0.9465\n",
      "Epoch 5/30 - RMSE: 0.9389\n",
      "Epoch 6/30 - RMSE: 0.9334\n",
      "Epoch 7/30 - RMSE: 0.9294\n",
      "Epoch 8/30 - RMSE: 0.9261\n",
      "Epoch 9/30 - RMSE: 0.9237\n",
      "Epoch 10/30 - RMSE: 0.9215\n",
      "Epoch 11/30 - RMSE: 0.9197\n",
      "Epoch 12/30 - RMSE: 0.9181\n",
      "Epoch 13/30 - RMSE: 0.9167\n",
      "Epoch 14/30 - RMSE: 0.9154\n",
      "Epoch 15/30 - RMSE: 0.9143\n",
      "Epoch 16/30 - RMSE: 0.9131\n",
      "Epoch 17/30 - RMSE: 0.9120\n",
      "Epoch 18/30 - RMSE: 0.9109\n",
      "Epoch 19/30 - RMSE: 0.9099\n",
      "Epoch 20/30 - RMSE: 0.9089\n",
      "Epoch 21/30 - RMSE: 0.9077\n",
      "Epoch 22/30 - RMSE: 0.9065\n",
      "Epoch 23/30 - RMSE: 0.9054\n",
      "Epoch 24/30 - RMSE: 0.9040\n",
      "Epoch 25/30 - RMSE: 0.9028\n",
      "Epoch 26/30 - RMSE: 0.9014\n",
      "Epoch 27/30 - RMSE: 0.8998\n",
      "Epoch 28/30 - RMSE: 0.8982\n",
      "Epoch 29/30 - RMSE: 0.8965\n",
      "Epoch 30/30 - RMSE: 0.8946\n",
      "(factors=20, lr=0.005, reg=0.1, ep=30) - RMSE=0.9505411068239796\n",
      "Epoch 1/40 - RMSE: 1.0443\n",
      "Epoch 2/40 - RMSE: 0.9805\n",
      "Epoch 3/40 - RMSE: 0.9582\n",
      "Epoch 4/40 - RMSE: 0.9462\n",
      "Epoch 5/40 - RMSE: 0.9385\n",
      "Epoch 6/40 - RMSE: 0.9330\n",
      "Epoch 7/40 - RMSE: 0.9289\n",
      "Epoch 8/40 - RMSE: 0.9259\n",
      "Epoch 9/40 - RMSE: 0.9232\n",
      "Epoch 10/40 - RMSE: 0.9211\n",
      "Epoch 11/40 - RMSE: 0.9193\n",
      "Epoch 12/40 - RMSE: 0.9177\n",
      "Epoch 13/40 - RMSE: 0.9163\n",
      "Epoch 14/40 - RMSE: 0.9149\n",
      "Epoch 15/40 - RMSE: 0.9138\n",
      "Epoch 16/40 - RMSE: 0.9126\n",
      "Epoch 17/40 - RMSE: 0.9113\n",
      "Epoch 18/40 - RMSE: 0.9103\n",
      "Epoch 19/40 - RMSE: 0.9091\n",
      "Epoch 20/40 - RMSE: 0.9080\n",
      "Epoch 21/40 - RMSE: 0.9068\n",
      "Epoch 22/40 - RMSE: 0.9055\n",
      "Epoch 23/40 - RMSE: 0.9043\n",
      "Epoch 24/40 - RMSE: 0.9029\n",
      "Epoch 25/40 - RMSE: 0.9014\n",
      "Epoch 26/40 - RMSE: 0.8999\n",
      "Epoch 27/40 - RMSE: 0.8982\n",
      "Epoch 28/40 - RMSE: 0.8965\n",
      "Epoch 29/40 - RMSE: 0.8947\n",
      "Epoch 30/40 - RMSE: 0.8927\n",
      "Epoch 31/40 - RMSE: 0.8906\n",
      "Epoch 32/40 - RMSE: 0.8885\n",
      "Epoch 33/40 - RMSE: 0.8862\n",
      "Epoch 34/40 - RMSE: 0.8839\n",
      "Epoch 35/40 - RMSE: 0.8816\n",
      "Epoch 36/40 - RMSE: 0.8792\n",
      "Epoch 37/40 - RMSE: 0.8767\n",
      "Epoch 38/40 - RMSE: 0.8742\n",
      "Epoch 39/40 - RMSE: 0.8718\n",
      "Epoch 40/40 - RMSE: 0.8692\n",
      "(factors=20, lr=0.005, reg=0.1, ep=40) - RMSE=0.9398470285710832\n",
      "Epoch 1/50 - RMSE: 1.0445\n",
      "Epoch 2/50 - RMSE: 0.9807\n",
      "Epoch 3/50 - RMSE: 0.9585\n",
      "Epoch 4/50 - RMSE: 0.9463\n",
      "Epoch 5/50 - RMSE: 0.9386\n",
      "Epoch 6/50 - RMSE: 0.9332\n",
      "Epoch 7/50 - RMSE: 0.9292\n",
      "Epoch 8/50 - RMSE: 0.9259\n",
      "Epoch 9/50 - RMSE: 0.9235\n",
      "Epoch 10/50 - RMSE: 0.9212\n",
      "Epoch 11/50 - RMSE: 0.9194\n",
      "Epoch 12/50 - RMSE: 0.9180\n",
      "Epoch 13/50 - RMSE: 0.9164\n",
      "Epoch 14/50 - RMSE: 0.9153\n",
      "Epoch 15/50 - RMSE: 0.9139\n",
      "Epoch 16/50 - RMSE: 0.9129\n",
      "Epoch 17/50 - RMSE: 0.9117\n",
      "Epoch 18/50 - RMSE: 0.9106\n",
      "Epoch 19/50 - RMSE: 0.9096\n",
      "Epoch 20/50 - RMSE: 0.9084\n",
      "Epoch 21/50 - RMSE: 0.9074\n",
      "Epoch 22/50 - RMSE: 0.9062\n",
      "Epoch 23/50 - RMSE: 0.9050\n",
      "Epoch 24/50 - RMSE: 0.9037\n",
      "Epoch 25/50 - RMSE: 0.9023\n",
      "Epoch 26/50 - RMSE: 0.9010\n",
      "Epoch 27/50 - RMSE: 0.8994\n",
      "Epoch 28/50 - RMSE: 0.8977\n",
      "Epoch 29/50 - RMSE: 0.8961\n",
      "Epoch 30/50 - RMSE: 0.8941\n",
      "Epoch 31/50 - RMSE: 0.8922\n",
      "Epoch 32/50 - RMSE: 0.8901\n",
      "Epoch 33/50 - RMSE: 0.8881\n",
      "Epoch 34/50 - RMSE: 0.8858\n",
      "Epoch 35/50 - RMSE: 0.8835\n",
      "Epoch 36/50 - RMSE: 0.8812\n",
      "Epoch 37/50 - RMSE: 0.8788\n",
      "Epoch 38/50 - RMSE: 0.8763\n",
      "Epoch 39/50 - RMSE: 0.8738\n",
      "Epoch 40/50 - RMSE: 0.8715\n",
      "Epoch 41/50 - RMSE: 0.8688\n",
      "Epoch 42/50 - RMSE: 0.8664\n",
      "Epoch 43/50 - RMSE: 0.8639\n",
      "Epoch 44/50 - RMSE: 0.8615\n",
      "Epoch 45/50 - RMSE: 0.8589\n",
      "Epoch 46/50 - RMSE: 0.8565\n",
      "Epoch 47/50 - RMSE: 0.8541\n",
      "Epoch 48/50 - RMSE: 0.8516\n",
      "Epoch 49/50 - RMSE: 0.8491\n",
      "Epoch 50/50 - RMSE: 0.8468\n",
      "(factors=20, lr=0.005, reg=0.1, ep=50) - RMSE=0.9334005574656222\n",
      "Epoch 1/10 - RMSE: 1.0160\n",
      "Epoch 2/10 - RMSE: 0.9531\n",
      "Epoch 3/10 - RMSE: 0.9341\n",
      "Epoch 4/10 - RMSE: 0.9222\n",
      "Epoch 5/10 - RMSE: 0.9123\n",
      "Epoch 6/10 - RMSE: 0.9020\n",
      "Epoch 7/10 - RMSE: 0.8903\n",
      "Epoch 8/10 - RMSE: 0.8765\n",
      "Epoch 9/10 - RMSE: 0.8605\n",
      "Epoch 10/10 - RMSE: 0.8427\n",
      "(factors=20, lr=0.01, reg=0.001, ep=10) - RMSE=0.9526050316756329\n",
      "Epoch 1/20 - RMSE: 1.0158\n",
      "Epoch 2/20 - RMSE: 0.9529\n",
      "Epoch 3/20 - RMSE: 0.9339\n",
      "Epoch 4/20 - RMSE: 0.9218\n",
      "Epoch 5/20 - RMSE: 0.9117\n",
      "Epoch 6/20 - RMSE: 0.9013\n",
      "Epoch 7/20 - RMSE: 0.8894\n",
      "Epoch 8/20 - RMSE: 0.8754\n",
      "Epoch 9/20 - RMSE: 0.8592\n",
      "Epoch 10/20 - RMSE: 0.8411\n",
      "Epoch 11/20 - RMSE: 0.8224\n",
      "Epoch 12/20 - RMSE: 0.8038\n",
      "Epoch 13/20 - RMSE: 0.7857\n",
      "Epoch 14/20 - RMSE: 0.7688\n",
      "Epoch 15/20 - RMSE: 0.7530\n",
      "Epoch 16/20 - RMSE: 0.7386\n",
      "Epoch 17/20 - RMSE: 0.7253\n",
      "Epoch 18/20 - RMSE: 0.7131\n",
      "Epoch 19/20 - RMSE: 0.7020\n",
      "Epoch 20/20 - RMSE: 0.6918\n",
      "(factors=20, lr=0.01, reg=0.001, ep=20) - RMSE=0.9906904009722826\n",
      "Epoch 1/30 - RMSE: 1.0160\n",
      "Epoch 2/30 - RMSE: 0.9529\n",
      "Epoch 3/30 - RMSE: 0.9338\n",
      "Epoch 4/30 - RMSE: 0.9217\n",
      "Epoch 5/30 - RMSE: 0.9117\n",
      "Epoch 6/30 - RMSE: 0.9013\n",
      "Epoch 7/30 - RMSE: 0.8893\n",
      "Epoch 8/30 - RMSE: 0.8751\n",
      "Epoch 9/30 - RMSE: 0.8588\n",
      "Epoch 10/30 - RMSE: 0.8407\n",
      "Epoch 11/30 - RMSE: 0.8220\n",
      "Epoch 12/30 - RMSE: 0.8035\n",
      "Epoch 13/30 - RMSE: 0.7856\n",
      "Epoch 14/30 - RMSE: 0.7688\n",
      "Epoch 15/30 - RMSE: 0.7531\n",
      "Epoch 16/30 - RMSE: 0.7388\n",
      "Epoch 17/30 - RMSE: 0.7254\n",
      "Epoch 18/30 - RMSE: 0.7132\n",
      "Epoch 19/30 - RMSE: 0.7020\n",
      "Epoch 20/30 - RMSE: 0.6918\n",
      "Epoch 21/30 - RMSE: 0.6822\n",
      "Epoch 22/30 - RMSE: 0.6735\n",
      "Epoch 23/30 - RMSE: 0.6654\n",
      "Epoch 24/30 - RMSE: 0.6580\n",
      "Epoch 25/30 - RMSE: 0.6509\n",
      "Epoch 26/30 - RMSE: 0.6445\n",
      "Epoch 27/30 - RMSE: 0.6384\n",
      "Epoch 28/30 - RMSE: 0.6329\n",
      "Epoch 29/30 - RMSE: 0.6277\n",
      "Epoch 30/30 - RMSE: 0.6228\n",
      "(factors=20, lr=0.01, reg=0.001, ep=30) - RMSE=1.0422158090320945\n",
      "Epoch 1/40 - RMSE: 1.0165\n",
      "Epoch 2/40 - RMSE: 0.9531\n",
      "Epoch 3/40 - RMSE: 0.9340\n",
      "Epoch 4/40 - RMSE: 0.9223\n",
      "Epoch 5/40 - RMSE: 0.9122\n",
      "Epoch 6/40 - RMSE: 0.9020\n",
      "Epoch 7/40 - RMSE: 0.8904\n",
      "Epoch 8/40 - RMSE: 0.8765\n",
      "Epoch 9/40 - RMSE: 0.8603\n",
      "Epoch 10/40 - RMSE: 0.8422\n",
      "Epoch 11/40 - RMSE: 0.8232\n",
      "Epoch 12/40 - RMSE: 0.8043\n",
      "Epoch 13/40 - RMSE: 0.7861\n",
      "Epoch 14/40 - RMSE: 0.7690\n",
      "Epoch 15/40 - RMSE: 0.7530\n",
      "Epoch 16/40 - RMSE: 0.7384\n",
      "Epoch 17/40 - RMSE: 0.7249\n",
      "Epoch 18/40 - RMSE: 0.7127\n",
      "Epoch 19/40 - RMSE: 0.7014\n",
      "Epoch 20/40 - RMSE: 0.6911\n",
      "Epoch 21/40 - RMSE: 0.6817\n",
      "Epoch 22/40 - RMSE: 0.6729\n",
      "Epoch 23/40 - RMSE: 0.6649\n",
      "Epoch 24/40 - RMSE: 0.6575\n",
      "Epoch 25/40 - RMSE: 0.6506\n",
      "Epoch 26/40 - RMSE: 0.6442\n",
      "Epoch 27/40 - RMSE: 0.6382\n",
      "Epoch 28/40 - RMSE: 0.6326\n",
      "Epoch 29/40 - RMSE: 0.6274\n",
      "Epoch 30/40 - RMSE: 0.6225\n",
      "Epoch 31/40 - RMSE: 0.6180\n",
      "Epoch 32/40 - RMSE: 0.6136\n",
      "Epoch 33/40 - RMSE: 0.6096\n",
      "Epoch 34/40 - RMSE: 0.6058\n",
      "Epoch 35/40 - RMSE: 0.6022\n",
      "Epoch 36/40 - RMSE: 0.5987\n",
      "Epoch 37/40 - RMSE: 0.5956\n",
      "Epoch 38/40 - RMSE: 0.5925\n",
      "Epoch 39/40 - RMSE: 0.5896\n",
      "Epoch 40/40 - RMSE: 0.5867\n",
      "(factors=20, lr=0.01, reg=0.001, ep=40) - RMSE=1.0744215907853116\n",
      "Epoch 1/50 - RMSE: 1.0161\n",
      "Epoch 2/50 - RMSE: 0.9528\n",
      "Epoch 3/50 - RMSE: 0.9338\n",
      "Epoch 4/50 - RMSE: 0.9219\n",
      "Epoch 5/50 - RMSE: 0.9121\n",
      "Epoch 6/50 - RMSE: 0.9020\n",
      "Epoch 7/50 - RMSE: 0.8907\n",
      "Epoch 8/50 - RMSE: 0.8772\n",
      "Epoch 9/50 - RMSE: 0.8618\n",
      "Epoch 10/50 - RMSE: 0.8443\n",
      "Epoch 11/50 - RMSE: 0.8260\n",
      "Epoch 12/50 - RMSE: 0.8077\n",
      "Epoch 13/50 - RMSE: 0.7900\n",
      "Epoch 14/50 - RMSE: 0.7734\n",
      "Epoch 15/50 - RMSE: 0.7579\n",
      "Epoch 16/50 - RMSE: 0.7434\n",
      "Epoch 17/50 - RMSE: 0.7302\n",
      "Epoch 18/50 - RMSE: 0.7180\n",
      "Epoch 19/50 - RMSE: 0.7069\n",
      "Epoch 20/50 - RMSE: 0.6966\n",
      "Epoch 21/50 - RMSE: 0.6872\n",
      "Epoch 22/50 - RMSE: 0.6784\n",
      "Epoch 23/50 - RMSE: 0.6702\n",
      "Epoch 24/50 - RMSE: 0.6629\n",
      "Epoch 25/50 - RMSE: 0.6559\n",
      "Epoch 26/50 - RMSE: 0.6494\n",
      "Epoch 27/50 - RMSE: 0.6433\n",
      "Epoch 28/50 - RMSE: 0.6377\n",
      "Epoch 29/50 - RMSE: 0.6323\n",
      "Epoch 30/50 - RMSE: 0.6273\n",
      "Epoch 31/50 - RMSE: 0.6227\n",
      "Epoch 32/50 - RMSE: 0.6183\n",
      "Epoch 33/50 - RMSE: 0.6141\n",
      "Epoch 34/50 - RMSE: 0.6103\n",
      "Epoch 35/50 - RMSE: 0.6066\n",
      "Epoch 36/50 - RMSE: 0.6031\n",
      "Epoch 37/50 - RMSE: 0.5998\n",
      "Epoch 38/50 - RMSE: 0.5966\n",
      "Epoch 39/50 - RMSE: 0.5937\n",
      "Epoch 40/50 - RMSE: 0.5908\n",
      "Epoch 41/50 - RMSE: 0.5881\n",
      "Epoch 42/50 - RMSE: 0.5855\n",
      "Epoch 43/50 - RMSE: 0.5830\n",
      "Epoch 44/50 - RMSE: 0.5808\n",
      "Epoch 45/50 - RMSE: 0.5783\n",
      "Epoch 46/50 - RMSE: 0.5762\n",
      "Epoch 47/50 - RMSE: 0.5741\n",
      "Epoch 48/50 - RMSE: 0.5722\n",
      "Epoch 49/50 - RMSE: 0.5703\n",
      "Epoch 50/50 - RMSE: 0.5685\n",
      "(factors=20, lr=0.01, reg=0.001, ep=50) - RMSE=1.0962448166951906\n",
      "Epoch 1/10 - RMSE: 1.0162\n",
      "Epoch 2/10 - RMSE: 0.9531\n",
      "Epoch 3/10 - RMSE: 0.9344\n",
      "Epoch 4/10 - RMSE: 0.9230\n",
      "Epoch 5/10 - RMSE: 0.9137\n",
      "Epoch 6/10 - RMSE: 0.9046\n",
      "Epoch 7/10 - RMSE: 0.8942\n",
      "Epoch 8/10 - RMSE: 0.8822\n",
      "Epoch 9/10 - RMSE: 0.8680\n",
      "Epoch 10/10 - RMSE: 0.8519\n",
      "(factors=20, lr=0.01, reg=0.01, ep=10) - RMSE=0.9498400741881876\n",
      "Epoch 1/20 - RMSE: 1.0159\n",
      "Epoch 2/20 - RMSE: 0.9531\n",
      "Epoch 3/20 - RMSE: 0.9340\n",
      "Epoch 4/20 - RMSE: 0.9227\n",
      "Epoch 5/20 - RMSE: 0.9134\n",
      "Epoch 6/20 - RMSE: 0.9041\n",
      "Epoch 7/20 - RMSE: 0.8940\n",
      "Epoch 8/20 - RMSE: 0.8824\n",
      "Epoch 9/20 - RMSE: 0.8684\n",
      "Epoch 10/20 - RMSE: 0.8529\n",
      "Epoch 11/20 - RMSE: 0.8357\n",
      "Epoch 12/20 - RMSE: 0.8180\n",
      "Epoch 13/20 - RMSE: 0.8005\n",
      "Epoch 14/20 - RMSE: 0.7835\n",
      "Epoch 15/20 - RMSE: 0.7677\n",
      "Epoch 16/20 - RMSE: 0.7526\n",
      "Epoch 17/20 - RMSE: 0.7389\n",
      "Epoch 18/20 - RMSE: 0.7262\n",
      "Epoch 19/20 - RMSE: 0.7146\n",
      "Epoch 20/20 - RMSE: 0.7037\n",
      "(factors=20, lr=0.01, reg=0.01, ep=20) - RMSE=0.9770279445178081\n",
      "Epoch 1/30 - RMSE: 1.0165\n",
      "Epoch 2/30 - RMSE: 0.9533\n",
      "Epoch 3/30 - RMSE: 0.9343\n",
      "Epoch 4/30 - RMSE: 0.9227\n",
      "Epoch 5/30 - RMSE: 0.9133\n",
      "Epoch 6/30 - RMSE: 0.9038\n",
      "Epoch 7/30 - RMSE: 0.8934\n",
      "Epoch 8/30 - RMSE: 0.8810\n",
      "Epoch 9/30 - RMSE: 0.8670\n",
      "Epoch 10/30 - RMSE: 0.8510\n",
      "Epoch 11/30 - RMSE: 0.8339\n",
      "Epoch 12/30 - RMSE: 0.8166\n",
      "Epoch 13/30 - RMSE: 0.7996\n",
      "Epoch 14/30 - RMSE: 0.7832\n",
      "Epoch 15/30 - RMSE: 0.7676\n",
      "Epoch 16/30 - RMSE: 0.7529\n",
      "Epoch 17/30 - RMSE: 0.7393\n",
      "Epoch 18/30 - RMSE: 0.7267\n",
      "Epoch 19/30 - RMSE: 0.7153\n",
      "Epoch 20/30 - RMSE: 0.7045\n",
      "Epoch 21/30 - RMSE: 0.6947\n",
      "Epoch 22/30 - RMSE: 0.6857\n",
      "Epoch 23/30 - RMSE: 0.6772\n",
      "Epoch 24/30 - RMSE: 0.6695\n",
      "Epoch 25/30 - RMSE: 0.6623\n",
      "Epoch 26/30 - RMSE: 0.6555\n",
      "Epoch 27/30 - RMSE: 0.6493\n",
      "Epoch 28/30 - RMSE: 0.6435\n",
      "Epoch 29/30 - RMSE: 0.6379\n",
      "Epoch 30/30 - RMSE: 0.6329\n",
      "(factors=20, lr=0.01, reg=0.01, ep=30) - RMSE=1.0089590204011343\n",
      "Epoch 1/40 - RMSE: 1.0162\n",
      "Epoch 2/40 - RMSE: 0.9533\n",
      "Epoch 3/40 - RMSE: 0.9343\n",
      "Epoch 4/40 - RMSE: 0.9229\n",
      "Epoch 5/40 - RMSE: 0.9135\n",
      "Epoch 6/40 - RMSE: 0.9044\n",
      "Epoch 7/40 - RMSE: 0.8942\n",
      "Epoch 8/40 - RMSE: 0.8821\n",
      "Epoch 9/40 - RMSE: 0.8679\n",
      "Epoch 10/40 - RMSE: 0.8520\n",
      "Epoch 11/40 - RMSE: 0.8345\n",
      "Epoch 12/40 - RMSE: 0.8169\n",
      "Epoch 13/40 - RMSE: 0.7995\n",
      "Epoch 14/40 - RMSE: 0.7827\n",
      "Epoch 15/40 - RMSE: 0.7669\n",
      "Epoch 16/40 - RMSE: 0.7519\n",
      "Epoch 17/40 - RMSE: 0.7381\n",
      "Epoch 18/40 - RMSE: 0.7252\n",
      "Epoch 19/40 - RMSE: 0.7134\n",
      "Epoch 20/40 - RMSE: 0.7026\n",
      "Epoch 21/40 - RMSE: 0.6926\n",
      "Epoch 22/40 - RMSE: 0.6835\n",
      "Epoch 23/40 - RMSE: 0.6750\n",
      "Epoch 24/40 - RMSE: 0.6672\n",
      "Epoch 25/40 - RMSE: 0.6598\n",
      "Epoch 26/40 - RMSE: 0.6531\n",
      "Epoch 27/40 - RMSE: 0.6469\n",
      "Epoch 28/40 - RMSE: 0.6412\n",
      "Epoch 29/40 - RMSE: 0.6357\n",
      "Epoch 30/40 - RMSE: 0.6307\n",
      "Epoch 31/40 - RMSE: 0.6259\n",
      "Epoch 32/40 - RMSE: 0.6214\n",
      "Epoch 33/40 - RMSE: 0.6172\n",
      "Epoch 34/40 - RMSE: 0.6133\n",
      "Epoch 35/40 - RMSE: 0.6095\n",
      "Epoch 36/40 - RMSE: 0.6060\n",
      "Epoch 37/40 - RMSE: 0.6027\n",
      "Epoch 38/40 - RMSE: 0.5995\n",
      "Epoch 39/40 - RMSE: 0.5966\n",
      "Epoch 40/40 - RMSE: 0.5937\n",
      "(factors=20, lr=0.01, reg=0.01, ep=40) - RMSE=1.037021026342014\n",
      "Epoch 1/50 - RMSE: 1.0159\n",
      "Epoch 2/50 - RMSE: 0.9531\n",
      "Epoch 3/50 - RMSE: 0.9343\n",
      "Epoch 4/50 - RMSE: 0.9229\n",
      "Epoch 5/50 - RMSE: 0.9136\n",
      "Epoch 6/50 - RMSE: 0.9044\n",
      "Epoch 7/50 - RMSE: 0.8942\n",
      "Epoch 8/50 - RMSE: 0.8822\n",
      "Epoch 9/50 - RMSE: 0.8681\n",
      "Epoch 10/50 - RMSE: 0.8523\n",
      "Epoch 11/50 - RMSE: 0.8350\n",
      "Epoch 12/50 - RMSE: 0.8174\n",
      "Epoch 13/50 - RMSE: 0.8001\n",
      "Epoch 14/50 - RMSE: 0.7834\n",
      "Epoch 15/50 - RMSE: 0.7676\n",
      "Epoch 16/50 - RMSE: 0.7528\n",
      "Epoch 17/50 - RMSE: 0.7390\n",
      "Epoch 18/50 - RMSE: 0.7265\n",
      "Epoch 19/50 - RMSE: 0.7148\n",
      "Epoch 20/50 - RMSE: 0.7040\n",
      "Epoch 21/50 - RMSE: 0.6941\n",
      "Epoch 22/50 - RMSE: 0.6849\n",
      "Epoch 23/50 - RMSE: 0.6765\n",
      "Epoch 24/50 - RMSE: 0.6685\n",
      "Epoch 25/50 - RMSE: 0.6614\n",
      "Epoch 26/50 - RMSE: 0.6547\n",
      "Epoch 27/50 - RMSE: 0.6485\n",
      "Epoch 28/50 - RMSE: 0.6425\n",
      "Epoch 29/50 - RMSE: 0.6370\n",
      "Epoch 30/50 - RMSE: 0.6321\n",
      "Epoch 31/50 - RMSE: 0.6273\n",
      "Epoch 32/50 - RMSE: 0.6227\n",
      "Epoch 33/50 - RMSE: 0.6185\n",
      "Epoch 34/50 - RMSE: 0.6146\n",
      "Epoch 35/50 - RMSE: 0.6108\n",
      "Epoch 36/50 - RMSE: 0.6074\n",
      "Epoch 37/50 - RMSE: 0.6040\n",
      "Epoch 38/50 - RMSE: 0.6009\n",
      "Epoch 39/50 - RMSE: 0.5979\n",
      "Epoch 40/50 - RMSE: 0.5951\n",
      "Epoch 41/50 - RMSE: 0.5923\n",
      "Epoch 42/50 - RMSE: 0.5899\n",
      "Epoch 43/50 - RMSE: 0.5874\n",
      "Epoch 44/50 - RMSE: 0.5851\n",
      "Epoch 45/50 - RMSE: 0.5829\n",
      "Epoch 46/50 - RMSE: 0.5807\n",
      "Epoch 47/50 - RMSE: 0.5786\n",
      "Epoch 48/50 - RMSE: 0.5768\n",
      "Epoch 49/50 - RMSE: 0.5750\n",
      "Epoch 50/50 - RMSE: 0.5731\n",
      "(factors=20, lr=0.01, reg=0.01, ep=50) - RMSE=1.0622606517738231\n",
      "Epoch 1/10 - RMSE: 1.0175\n",
      "Epoch 2/10 - RMSE: 0.9564\n",
      "Epoch 3/10 - RMSE: 0.9397\n",
      "Epoch 4/10 - RMSE: 0.9312\n",
      "Epoch 5/10 - RMSE: 0.9259\n",
      "Epoch 6/10 - RMSE: 0.9219\n",
      "Epoch 7/10 - RMSE: 0.9190\n",
      "Epoch 8/10 - RMSE: 0.9163\n",
      "Epoch 9/10 - RMSE: 0.9139\n",
      "Epoch 10/10 - RMSE: 0.9111\n",
      "(factors=20, lr=0.01, reg=0.1, ep=10) - RMSE=0.9556377413846733\n",
      "Epoch 1/20 - RMSE: 1.0174\n",
      "Epoch 2/20 - RMSE: 0.9563\n",
      "Epoch 3/20 - RMSE: 0.9397\n",
      "Epoch 4/20 - RMSE: 0.9312\n",
      "Epoch 5/20 - RMSE: 0.9257\n",
      "Epoch 6/20 - RMSE: 0.9219\n",
      "Epoch 7/20 - RMSE: 0.9191\n",
      "Epoch 8/20 - RMSE: 0.9163\n",
      "Epoch 9/20 - RMSE: 0.9138\n",
      "Epoch 10/20 - RMSE: 0.9113\n",
      "Epoch 11/20 - RMSE: 0.9085\n",
      "Epoch 12/20 - RMSE: 0.9056\n",
      "Epoch 13/20 - RMSE: 0.9022\n",
      "Epoch 14/20 - RMSE: 0.8985\n",
      "Epoch 15/20 - RMSE: 0.8945\n",
      "Epoch 16/20 - RMSE: 0.8901\n",
      "Epoch 17/20 - RMSE: 0.8857\n",
      "Epoch 18/20 - RMSE: 0.8810\n",
      "Epoch 19/20 - RMSE: 0.8765\n",
      "Epoch 20/20 - RMSE: 0.8717\n",
      "(factors=20, lr=0.01, reg=0.1, ep=20) - RMSE=0.9381453871266079\n",
      "Epoch 1/30 - RMSE: 1.0177\n",
      "Epoch 2/30 - RMSE: 0.9565\n",
      "Epoch 3/30 - RMSE: 0.9399\n",
      "Epoch 4/30 - RMSE: 0.9311\n",
      "Epoch 5/30 - RMSE: 0.9260\n",
      "Epoch 6/30 - RMSE: 0.9221\n",
      "Epoch 7/30 - RMSE: 0.9190\n",
      "Epoch 8/30 - RMSE: 0.9165\n",
      "Epoch 9/30 - RMSE: 0.9140\n",
      "Epoch 10/30 - RMSE: 0.9115\n",
      "Epoch 11/30 - RMSE: 0.9088\n",
      "Epoch 12/30 - RMSE: 0.9060\n",
      "Epoch 13/30 - RMSE: 0.9028\n",
      "Epoch 14/30 - RMSE: 0.8992\n",
      "Epoch 15/30 - RMSE: 0.8952\n",
      "Epoch 16/30 - RMSE: 0.8911\n",
      "Epoch 17/30 - RMSE: 0.8866\n",
      "Epoch 18/30 - RMSE: 0.8822\n",
      "Epoch 19/30 - RMSE: 0.8776\n",
      "Epoch 20/30 - RMSE: 0.8729\n",
      "Epoch 21/30 - RMSE: 0.8682\n",
      "Epoch 22/30 - RMSE: 0.8635\n",
      "Epoch 23/30 - RMSE: 0.8589\n",
      "Epoch 24/30 - RMSE: 0.8541\n",
      "Epoch 25/30 - RMSE: 0.8494\n",
      "Epoch 26/30 - RMSE: 0.8445\n",
      "Epoch 27/30 - RMSE: 0.8398\n",
      "Epoch 28/30 - RMSE: 0.8350\n",
      "Epoch 29/30 - RMSE: 0.8302\n",
      "Epoch 30/30 - RMSE: 0.8255\n",
      "(factors=20, lr=0.01, reg=0.1, ep=30) - RMSE=0.9303345900359742\n",
      "Epoch 1/40 - RMSE: 1.0174\n",
      "Epoch 2/40 - RMSE: 0.9563\n",
      "Epoch 3/40 - RMSE: 0.9399\n",
      "Epoch 4/40 - RMSE: 0.9314\n",
      "Epoch 5/40 - RMSE: 0.9261\n",
      "Epoch 6/40 - RMSE: 0.9223\n",
      "Epoch 7/40 - RMSE: 0.9193\n",
      "Epoch 8/40 - RMSE: 0.9169\n",
      "Epoch 9/40 - RMSE: 0.9146\n",
      "Epoch 10/40 - RMSE: 0.9123\n",
      "Epoch 11/40 - RMSE: 0.9099\n",
      "Epoch 12/40 - RMSE: 0.9075\n",
      "Epoch 13/40 - RMSE: 0.9046\n",
      "Epoch 14/40 - RMSE: 0.9015\n",
      "Epoch 15/40 - RMSE: 0.8976\n",
      "Epoch 16/40 - RMSE: 0.8937\n",
      "Epoch 17/40 - RMSE: 0.8893\n",
      "Epoch 18/40 - RMSE: 0.8842\n",
      "Epoch 19/40 - RMSE: 0.8794\n",
      "Epoch 20/40 - RMSE: 0.8741\n",
      "Epoch 21/40 - RMSE: 0.8690\n",
      "Epoch 22/40 - RMSE: 0.8637\n",
      "Epoch 23/40 - RMSE: 0.8585\n",
      "Epoch 24/40 - RMSE: 0.8533\n",
      "Epoch 25/40 - RMSE: 0.8480\n",
      "Epoch 26/40 - RMSE: 0.8428\n",
      "Epoch 27/40 - RMSE: 0.8378\n",
      "Epoch 28/40 - RMSE: 0.8329\n",
      "Epoch 29/40 - RMSE: 0.8277\n",
      "Epoch 30/40 - RMSE: 0.8230\n",
      "Epoch 31/40 - RMSE: 0.8180\n",
      "Epoch 32/40 - RMSE: 0.8136\n",
      "Epoch 33/40 - RMSE: 0.8090\n",
      "Epoch 34/40 - RMSE: 0.8046\n",
      "Epoch 35/40 - RMSE: 0.8004\n",
      "Epoch 36/40 - RMSE: 0.7963\n",
      "Epoch 37/40 - RMSE: 0.7920\n",
      "Epoch 38/40 - RMSE: 0.7882\n",
      "Epoch 39/40 - RMSE: 0.7844\n",
      "Epoch 40/40 - RMSE: 0.7809\n",
      "(factors=20, lr=0.01, reg=0.1, ep=40) - RMSE=0.9254962085740289\n",
      "Epoch 1/50 - RMSE: 1.0175\n",
      "Epoch 2/50 - RMSE: 0.9566\n",
      "Epoch 3/50 - RMSE: 0.9401\n",
      "Epoch 4/50 - RMSE: 0.9316\n",
      "Epoch 5/50 - RMSE: 0.9262\n",
      "Epoch 6/50 - RMSE: 0.9227\n",
      "Epoch 7/50 - RMSE: 0.9197\n",
      "Epoch 8/50 - RMSE: 0.9174\n",
      "Epoch 9/50 - RMSE: 0.9152\n",
      "Epoch 10/50 - RMSE: 0.9130\n",
      "Epoch 11/50 - RMSE: 0.9108\n",
      "Epoch 12/50 - RMSE: 0.9085\n",
      "Epoch 13/50 - RMSE: 0.9057\n",
      "Epoch 14/50 - RMSE: 0.9027\n",
      "Epoch 15/50 - RMSE: 0.8994\n",
      "Epoch 16/50 - RMSE: 0.8956\n",
      "Epoch 17/50 - RMSE: 0.8915\n",
      "Epoch 18/50 - RMSE: 0.8866\n",
      "Epoch 19/50 - RMSE: 0.8819\n",
      "Epoch 20/50 - RMSE: 0.8770\n",
      "Epoch 21/50 - RMSE: 0.8721\n",
      "Epoch 22/50 - RMSE: 0.8669\n",
      "Epoch 23/50 - RMSE: 0.8617\n",
      "Epoch 24/50 - RMSE: 0.8567\n",
      "Epoch 25/50 - RMSE: 0.8518\n",
      "Epoch 26/50 - RMSE: 0.8465\n",
      "Epoch 27/50 - RMSE: 0.8415\n",
      "Epoch 28/50 - RMSE: 0.8363\n",
      "Epoch 29/50 - RMSE: 0.8313\n",
      "Epoch 30/50 - RMSE: 0.8263\n",
      "Epoch 31/50 - RMSE: 0.8213\n",
      "Epoch 32/50 - RMSE: 0.8166\n",
      "Epoch 33/50 - RMSE: 0.8118\n",
      "Epoch 34/50 - RMSE: 0.8070\n",
      "Epoch 35/50 - RMSE: 0.8026\n",
      "Epoch 36/50 - RMSE: 0.7982\n",
      "Epoch 37/50 - RMSE: 0.7943\n",
      "Epoch 38/50 - RMSE: 0.7901\n",
      "Epoch 39/50 - RMSE: 0.7861\n",
      "Epoch 40/50 - RMSE: 0.7822\n",
      "Epoch 41/50 - RMSE: 0.7787\n",
      "Epoch 42/50 - RMSE: 0.7753\n",
      "Epoch 43/50 - RMSE: 0.7721\n",
      "Epoch 44/50 - RMSE: 0.7688\n",
      "Epoch 45/50 - RMSE: 0.7656\n",
      "Epoch 46/50 - RMSE: 0.7629\n",
      "Epoch 47/50 - RMSE: 0.7599\n",
      "Epoch 48/50 - RMSE: 0.7573\n",
      "Epoch 49/50 - RMSE: 0.7548\n",
      "Epoch 50/50 - RMSE: 0.7521\n",
      "(factors=20, lr=0.01, reg=0.1, ep=50) - RMSE=0.9238453873378558\n",
      "Epoch 1/10 - RMSE: nan\n",
      "Epoch 2/10 - RMSE: nan\n",
      "Epoch 3/10 - RMSE: nan\n",
      "Epoch 4/10 - RMSE: nan\n",
      "Epoch 5/10 - RMSE: nan\n",
      "Epoch 6/10 - RMSE: nan\n",
      "Epoch 7/10 - RMSE: nan\n",
      "Epoch 8/10 - RMSE: nan\n",
      "Epoch 9/10 - RMSE: nan\n",
      "Epoch 10/10 - RMSE: nan\n",
      "(factors=20, lr=0.2, reg=0.001, ep=10) - RMSE=nan\n",
      "Epoch 1/20 - RMSE: nan\n",
      "Epoch 2/20 - RMSE: nan\n",
      "Epoch 3/20 - RMSE: nan\n",
      "Epoch 4/20 - RMSE: nan\n",
      "Epoch 5/20 - RMSE: nan\n",
      "Epoch 6/20 - RMSE: nan\n",
      "Epoch 7/20 - RMSE: nan\n",
      "Epoch 8/20 - RMSE: nan\n",
      "Epoch 9/20 - RMSE: nan\n",
      "Epoch 10/20 - RMSE: nan\n",
      "Epoch 11/20 - RMSE: nan\n",
      "Epoch 12/20 - RMSE: nan\n",
      "Epoch 13/20 - RMSE: nan\n",
      "Epoch 14/20 - RMSE: nan\n",
      "Epoch 15/20 - RMSE: nan\n",
      "Epoch 16/20 - RMSE: nan\n",
      "Epoch 17/20 - RMSE: nan\n",
      "Epoch 18/20 - RMSE: nan\n",
      "Epoch 19/20 - RMSE: nan\n",
      "Epoch 20/20 - RMSE: nan\n",
      "(factors=20, lr=0.2, reg=0.001, ep=20) - RMSE=nan\n",
      "Epoch 1/30 - RMSE: nan\n",
      "Epoch 2/30 - RMSE: nan\n",
      "Epoch 3/30 - RMSE: nan\n",
      "Epoch 4/30 - RMSE: nan\n",
      "Epoch 5/30 - RMSE: nan\n",
      "Epoch 6/30 - RMSE: nan\n",
      "Epoch 7/30 - RMSE: nan\n",
      "Epoch 8/30 - RMSE: nan\n",
      "Epoch 9/30 - RMSE: nan\n",
      "Epoch 10/30 - RMSE: nan\n",
      "Epoch 11/30 - RMSE: nan\n",
      "Epoch 12/30 - RMSE: nan\n",
      "Epoch 13/30 - RMSE: nan\n",
      "Epoch 14/30 - RMSE: nan\n",
      "Epoch 15/30 - RMSE: nan\n",
      "Epoch 16/30 - RMSE: nan\n",
      "Epoch 17/30 - RMSE: nan\n",
      "Epoch 18/30 - RMSE: nan\n",
      "Epoch 19/30 - RMSE: nan\n",
      "Epoch 20/30 - RMSE: nan\n",
      "Epoch 21/30 - RMSE: nan\n",
      "Epoch 22/30 - RMSE: nan\n",
      "Epoch 23/30 - RMSE: nan\n",
      "Epoch 24/30 - RMSE: nan\n",
      "Epoch 25/30 - RMSE: nan\n",
      "Epoch 26/30 - RMSE: nan\n",
      "Epoch 27/30 - RMSE: nan\n",
      "Epoch 28/30 - RMSE: nan\n",
      "Epoch 29/30 - RMSE: nan\n",
      "Epoch 30/30 - RMSE: nan\n",
      "(factors=20, lr=0.2, reg=0.001, ep=30) - RMSE=nan\n",
      "Epoch 1/40 - RMSE: nan\n",
      "Epoch 2/40 - RMSE: nan\n",
      "Epoch 3/40 - RMSE: nan\n",
      "Epoch 4/40 - RMSE: nan\n",
      "Epoch 5/40 - RMSE: nan\n",
      "Epoch 6/40 - RMSE: nan\n",
      "Epoch 7/40 - RMSE: nan\n",
      "Epoch 8/40 - RMSE: nan\n",
      "Epoch 9/40 - RMSE: nan\n",
      "Epoch 10/40 - RMSE: nan\n",
      "Epoch 11/40 - RMSE: nan\n",
      "Epoch 12/40 - RMSE: nan\n",
      "Epoch 13/40 - RMSE: nan\n",
      "Epoch 14/40 - RMSE: nan\n",
      "Epoch 15/40 - RMSE: nan\n",
      "Epoch 16/40 - RMSE: nan\n",
      "Epoch 17/40 - RMSE: nan\n",
      "Epoch 18/40 - RMSE: nan\n",
      "Epoch 19/40 - RMSE: nan\n",
      "Epoch 20/40 - RMSE: nan\n",
      "Epoch 21/40 - RMSE: nan\n",
      "Epoch 22/40 - RMSE: nan\n",
      "Epoch 23/40 - RMSE: nan\n",
      "Epoch 24/40 - RMSE: nan\n",
      "Epoch 25/40 - RMSE: nan\n",
      "Epoch 26/40 - RMSE: nan\n",
      "Epoch 27/40 - RMSE: nan\n",
      "Epoch 28/40 - RMSE: nan\n",
      "Epoch 29/40 - RMSE: nan\n",
      "Epoch 30/40 - RMSE: nan\n",
      "Epoch 31/40 - RMSE: nan\n",
      "Epoch 32/40 - RMSE: nan\n",
      "Epoch 33/40 - RMSE: nan\n",
      "Epoch 34/40 - RMSE: nan\n",
      "Epoch 35/40 - RMSE: nan\n",
      "Epoch 36/40 - RMSE: nan\n",
      "Epoch 37/40 - RMSE: nan\n",
      "Epoch 38/40 - RMSE: nan\n",
      "Epoch 39/40 - RMSE: nan\n",
      "Epoch 40/40 - RMSE: nan\n",
      "(factors=20, lr=0.2, reg=0.001, ep=40) - RMSE=nan\n",
      "Epoch 1/50 - RMSE: nan\n",
      "Epoch 2/50 - RMSE: nan\n",
      "Epoch 3/50 - RMSE: nan\n",
      "Epoch 4/50 - RMSE: nan\n",
      "Epoch 5/50 - RMSE: nan\n",
      "Epoch 6/50 - RMSE: nan\n",
      "Epoch 7/50 - RMSE: nan\n",
      "Epoch 8/50 - RMSE: nan\n",
      "Epoch 9/50 - RMSE: nan\n",
      "Epoch 10/50 - RMSE: nan\n",
      "Epoch 11/50 - RMSE: nan\n",
      "Epoch 12/50 - RMSE: nan\n",
      "Epoch 13/50 - RMSE: nan\n",
      "Epoch 14/50 - RMSE: nan\n",
      "Epoch 15/50 - RMSE: nan\n",
      "Epoch 16/50 - RMSE: nan\n",
      "Epoch 17/50 - RMSE: nan\n",
      "Epoch 18/50 - RMSE: nan\n",
      "Epoch 19/50 - RMSE: nan\n",
      "Epoch 20/50 - RMSE: nan\n",
      "Epoch 21/50 - RMSE: nan\n",
      "Epoch 22/50 - RMSE: nan\n",
      "Epoch 23/50 - RMSE: nan\n",
      "Epoch 24/50 - RMSE: nan\n",
      "Epoch 25/50 - RMSE: nan\n",
      "Epoch 26/50 - RMSE: nan\n",
      "Epoch 27/50 - RMSE: nan\n",
      "Epoch 28/50 - RMSE: nan\n",
      "Epoch 29/50 - RMSE: nan\n",
      "Epoch 30/50 - RMSE: nan\n",
      "Epoch 31/50 - RMSE: nan\n",
      "Epoch 32/50 - RMSE: nan\n",
      "Epoch 33/50 - RMSE: nan\n",
      "Epoch 34/50 - RMSE: nan\n",
      "Epoch 35/50 - RMSE: nan\n",
      "Epoch 36/50 - RMSE: nan\n",
      "Epoch 37/50 - RMSE: nan\n",
      "Epoch 38/50 - RMSE: nan\n",
      "Epoch 39/50 - RMSE: nan\n",
      "Epoch 40/50 - RMSE: nan\n",
      "Epoch 41/50 - RMSE: nan\n",
      "Epoch 42/50 - RMSE: nan\n",
      "Epoch 43/50 - RMSE: nan\n",
      "Epoch 44/50 - RMSE: nan\n",
      "Epoch 45/50 - RMSE: nan\n",
      "Epoch 46/50 - RMSE: nan\n",
      "Epoch 47/50 - RMSE: nan\n",
      "Epoch 48/50 - RMSE: nan\n",
      "Epoch 49/50 - RMSE: nan\n",
      "Epoch 50/50 - RMSE: nan\n",
      "(factors=20, lr=0.2, reg=0.001, ep=50) - RMSE=nan\n",
      "Epoch 1/10 - RMSE: nan\n",
      "Epoch 2/10 - RMSE: nan\n",
      "Epoch 3/10 - RMSE: nan\n",
      "Epoch 4/10 - RMSE: nan\n",
      "Epoch 5/10 - RMSE: nan\n",
      "Epoch 6/10 - RMSE: nan\n",
      "Epoch 7/10 - RMSE: nan\n",
      "Epoch 8/10 - RMSE: nan\n",
      "Epoch 9/10 - RMSE: nan\n",
      "Epoch 10/10 - RMSE: nan\n",
      "(factors=20, lr=0.2, reg=0.01, ep=10) - RMSE=nan\n",
      "Epoch 1/20 - RMSE: nan\n",
      "Epoch 2/20 - RMSE: nan\n",
      "Epoch 3/20 - RMSE: nan\n",
      "Epoch 4/20 - RMSE: nan\n",
      "Epoch 5/20 - RMSE: nan\n",
      "Epoch 6/20 - RMSE: nan\n",
      "Epoch 7/20 - RMSE: nan\n",
      "Epoch 8/20 - RMSE: nan\n",
      "Epoch 9/20 - RMSE: nan\n",
      "Epoch 10/20 - RMSE: nan\n",
      "Epoch 11/20 - RMSE: nan\n",
      "Epoch 12/20 - RMSE: nan\n",
      "Epoch 13/20 - RMSE: nan\n",
      "Epoch 14/20 - RMSE: nan\n",
      "Epoch 15/20 - RMSE: nan\n",
      "Epoch 16/20 - RMSE: nan\n",
      "Epoch 17/20 - RMSE: nan\n",
      "Epoch 18/20 - RMSE: nan\n",
      "Epoch 19/20 - RMSE: nan\n",
      "Epoch 20/20 - RMSE: nan\n",
      "(factors=20, lr=0.2, reg=0.01, ep=20) - RMSE=nan\n",
      "Epoch 1/30 - RMSE: nan\n",
      "Epoch 2/30 - RMSE: nan\n",
      "Epoch 3/30 - RMSE: nan\n",
      "Epoch 4/30 - RMSE: nan\n",
      "Epoch 5/30 - RMSE: nan\n",
      "Epoch 6/30 - RMSE: nan\n",
      "Epoch 7/30 - RMSE: nan\n",
      "Epoch 8/30 - RMSE: nan\n",
      "Epoch 9/30 - RMSE: nan\n",
      "Epoch 10/30 - RMSE: nan\n",
      "Epoch 11/30 - RMSE: nan\n",
      "Epoch 12/30 - RMSE: nan\n",
      "Epoch 13/30 - RMSE: nan\n",
      "Epoch 14/30 - RMSE: nan\n",
      "Epoch 15/30 - RMSE: nan\n",
      "Epoch 16/30 - RMSE: nan\n",
      "Epoch 17/30 - RMSE: nan\n",
      "Epoch 18/30 - RMSE: nan\n",
      "Epoch 19/30 - RMSE: nan\n",
      "Epoch 20/30 - RMSE: nan\n",
      "Epoch 21/30 - RMSE: nan\n",
      "Epoch 22/30 - RMSE: nan\n",
      "Epoch 23/30 - RMSE: nan\n",
      "Epoch 24/30 - RMSE: nan\n",
      "Epoch 25/30 - RMSE: nan\n",
      "Epoch 26/30 - RMSE: nan\n",
      "Epoch 27/30 - RMSE: nan\n",
      "Epoch 28/30 - RMSE: nan\n",
      "Epoch 29/30 - RMSE: nan\n",
      "Epoch 30/30 - RMSE: nan\n",
      "(factors=20, lr=0.2, reg=0.01, ep=30) - RMSE=nan\n",
      "Epoch 1/40 - RMSE: nan\n",
      "Epoch 2/40 - RMSE: nan\n",
      "Epoch 3/40 - RMSE: nan\n",
      "Epoch 4/40 - RMSE: nan\n",
      "Epoch 5/40 - RMSE: nan\n",
      "Epoch 6/40 - RMSE: nan\n",
      "Epoch 7/40 - RMSE: nan\n",
      "Epoch 8/40 - RMSE: nan\n",
      "Epoch 9/40 - RMSE: nan\n",
      "Epoch 10/40 - RMSE: nan\n",
      "Epoch 11/40 - RMSE: nan\n",
      "Epoch 12/40 - RMSE: nan\n",
      "Epoch 13/40 - RMSE: nan\n",
      "Epoch 14/40 - RMSE: nan\n",
      "Epoch 15/40 - RMSE: nan\n",
      "Epoch 16/40 - RMSE: nan\n",
      "Epoch 17/40 - RMSE: nan\n",
      "Epoch 18/40 - RMSE: nan\n",
      "Epoch 19/40 - RMSE: nan\n",
      "Epoch 20/40 - RMSE: nan\n",
      "Epoch 21/40 - RMSE: nan\n",
      "Epoch 22/40 - RMSE: nan\n",
      "Epoch 23/40 - RMSE: nan\n",
      "Epoch 24/40 - RMSE: nan\n",
      "Epoch 25/40 - RMSE: nan\n",
      "Epoch 26/40 - RMSE: nan\n",
      "Epoch 27/40 - RMSE: nan\n",
      "Epoch 28/40 - RMSE: nan\n",
      "Epoch 29/40 - RMSE: nan\n",
      "Epoch 30/40 - RMSE: nan\n",
      "Epoch 31/40 - RMSE: nan\n",
      "Epoch 32/40 - RMSE: nan\n",
      "Epoch 33/40 - RMSE: nan\n",
      "Epoch 34/40 - RMSE: nan\n",
      "Epoch 35/40 - RMSE: nan\n",
      "Epoch 36/40 - RMSE: nan\n",
      "Epoch 37/40 - RMSE: nan\n",
      "Epoch 38/40 - RMSE: nan\n",
      "Epoch 39/40 - RMSE: nan\n",
      "Epoch 40/40 - RMSE: nan\n",
      "(factors=20, lr=0.2, reg=0.01, ep=40) - RMSE=nan\n",
      "Epoch 1/50 - RMSE: nan\n",
      "Epoch 2/50 - RMSE: nan\n",
      "Epoch 3/50 - RMSE: nan\n",
      "Epoch 4/50 - RMSE: nan\n",
      "Epoch 5/50 - RMSE: nan\n",
      "Epoch 6/50 - RMSE: nan\n",
      "Epoch 7/50 - RMSE: nan\n",
      "Epoch 8/50 - RMSE: nan\n",
      "Epoch 9/50 - RMSE: nan\n",
      "Epoch 10/50 - RMSE: nan\n",
      "Epoch 11/50 - RMSE: nan\n",
      "Epoch 12/50 - RMSE: nan\n",
      "Epoch 13/50 - RMSE: nan\n",
      "Epoch 14/50 - RMSE: nan\n",
      "Epoch 15/50 - RMSE: nan\n",
      "Epoch 16/50 - RMSE: nan\n",
      "Epoch 17/50 - RMSE: nan\n",
      "Epoch 18/50 - RMSE: nan\n",
      "Epoch 19/50 - RMSE: nan\n",
      "Epoch 20/50 - RMSE: nan\n",
      "Epoch 21/50 - RMSE: nan\n",
      "Epoch 22/50 - RMSE: nan\n",
      "Epoch 23/50 - RMSE: nan\n",
      "Epoch 24/50 - RMSE: nan\n",
      "Epoch 25/50 - RMSE: nan\n",
      "Epoch 26/50 - RMSE: nan\n",
      "Epoch 27/50 - RMSE: nan\n",
      "Epoch 28/50 - RMSE: nan\n",
      "Epoch 29/50 - RMSE: nan\n",
      "Epoch 30/50 - RMSE: nan\n",
      "Epoch 31/50 - RMSE: nan\n",
      "Epoch 32/50 - RMSE: nan\n",
      "Epoch 33/50 - RMSE: nan\n",
      "Epoch 34/50 - RMSE: nan\n",
      "Epoch 35/50 - RMSE: nan\n",
      "Epoch 36/50 - RMSE: nan\n",
      "Epoch 37/50 - RMSE: nan\n",
      "Epoch 38/50 - RMSE: nan\n",
      "Epoch 39/50 - RMSE: nan\n",
      "Epoch 40/50 - RMSE: nan\n",
      "Epoch 41/50 - RMSE: nan\n",
      "Epoch 42/50 - RMSE: nan\n",
      "Epoch 43/50 - RMSE: nan\n",
      "Epoch 44/50 - RMSE: nan\n",
      "Epoch 45/50 - RMSE: nan\n",
      "Epoch 46/50 - RMSE: nan\n",
      "Epoch 47/50 - RMSE: nan\n",
      "Epoch 48/50 - RMSE: nan\n",
      "Epoch 49/50 - RMSE: nan\n",
      "Epoch 50/50 - RMSE: nan\n",
      "(factors=20, lr=0.2, reg=0.01, ep=50) - RMSE=nan\n",
      "Epoch 1/10 - RMSE: nan\n",
      "Epoch 2/10 - RMSE: nan\n",
      "Epoch 3/10 - RMSE: nan\n",
      "Epoch 4/10 - RMSE: nan\n",
      "Epoch 5/10 - RMSE: nan\n",
      "Epoch 6/10 - RMSE: nan\n",
      "Epoch 7/10 - RMSE: nan\n",
      "Epoch 8/10 - RMSE: nan\n",
      "Epoch 9/10 - RMSE: nan\n",
      "Epoch 10/10 - RMSE: nan\n",
      "(factors=20, lr=0.2, reg=0.1, ep=10) - RMSE=nan\n",
      "Epoch 1/20 - RMSE: nan\n",
      "Epoch 2/20 - RMSE: nan\n",
      "Epoch 3/20 - RMSE: nan\n",
      "Epoch 4/20 - RMSE: nan\n",
      "Epoch 5/20 - RMSE: nan\n",
      "Epoch 6/20 - RMSE: nan\n",
      "Epoch 7/20 - RMSE: nan\n",
      "Epoch 8/20 - RMSE: nan\n",
      "Epoch 9/20 - RMSE: nan\n",
      "Epoch 10/20 - RMSE: nan\n",
      "Epoch 11/20 - RMSE: nan\n",
      "Epoch 12/20 - RMSE: nan\n",
      "Epoch 13/20 - RMSE: nan\n",
      "Epoch 14/20 - RMSE: nan\n",
      "Epoch 15/20 - RMSE: nan\n",
      "Epoch 16/20 - RMSE: nan\n",
      "Epoch 17/20 - RMSE: nan\n",
      "Epoch 18/20 - RMSE: nan\n",
      "Epoch 19/20 - RMSE: nan\n",
      "Epoch 20/20 - RMSE: nan\n",
      "(factors=20, lr=0.2, reg=0.1, ep=20) - RMSE=nan\n",
      "Epoch 1/30 - RMSE: nan\n",
      "Epoch 2/30 - RMSE: nan\n",
      "Epoch 3/30 - RMSE: nan\n",
      "Epoch 4/30 - RMSE: nan\n",
      "Epoch 5/30 - RMSE: nan\n",
      "Epoch 6/30 - RMSE: nan\n",
      "Epoch 7/30 - RMSE: nan\n",
      "Epoch 8/30 - RMSE: nan\n",
      "Epoch 9/30 - RMSE: nan\n",
      "Epoch 10/30 - RMSE: nan\n",
      "Epoch 11/30 - RMSE: nan\n",
      "Epoch 12/30 - RMSE: nan\n",
      "Epoch 13/30 - RMSE: nan\n",
      "Epoch 14/30 - RMSE: nan\n",
      "Epoch 15/30 - RMSE: nan\n",
      "Epoch 16/30 - RMSE: nan\n",
      "Epoch 17/30 - RMSE: nan\n",
      "Epoch 18/30 - RMSE: nan\n",
      "Epoch 19/30 - RMSE: nan\n",
      "Epoch 20/30 - RMSE: nan\n",
      "Epoch 21/30 - RMSE: nan\n",
      "Epoch 22/30 - RMSE: nan\n",
      "Epoch 23/30 - RMSE: nan\n",
      "Epoch 24/30 - RMSE: nan\n",
      "Epoch 25/30 - RMSE: nan\n",
      "Epoch 26/30 - RMSE: nan\n",
      "Epoch 27/30 - RMSE: nan\n",
      "Epoch 28/30 - RMSE: nan\n",
      "Epoch 29/30 - RMSE: nan\n",
      "Epoch 30/30 - RMSE: nan\n",
      "(factors=20, lr=0.2, reg=0.1, ep=30) - RMSE=nan\n",
      "Epoch 1/40 - RMSE: nan\n",
      "Epoch 2/40 - RMSE: nan\n",
      "Epoch 3/40 - RMSE: nan\n",
      "Epoch 4/40 - RMSE: nan\n",
      "Epoch 5/40 - RMSE: nan\n",
      "Epoch 6/40 - RMSE: nan\n",
      "Epoch 7/40 - RMSE: nan\n",
      "Epoch 8/40 - RMSE: nan\n",
      "Epoch 9/40 - RMSE: nan\n",
      "Epoch 10/40 - RMSE: nan\n",
      "Epoch 11/40 - RMSE: nan\n",
      "Epoch 12/40 - RMSE: nan\n",
      "Epoch 13/40 - RMSE: nan\n",
      "Epoch 14/40 - RMSE: nan\n",
      "Epoch 15/40 - RMSE: nan\n",
      "Epoch 16/40 - RMSE: nan\n",
      "Epoch 17/40 - RMSE: nan\n",
      "Epoch 18/40 - RMSE: nan\n",
      "Epoch 19/40 - RMSE: nan\n",
      "Epoch 20/40 - RMSE: nan\n",
      "Epoch 21/40 - RMSE: nan\n",
      "Epoch 22/40 - RMSE: nan\n",
      "Epoch 23/40 - RMSE: nan\n",
      "Epoch 24/40 - RMSE: nan\n",
      "Epoch 25/40 - RMSE: nan\n",
      "Epoch 26/40 - RMSE: nan\n",
      "Epoch 27/40 - RMSE: nan\n",
      "Epoch 28/40 - RMSE: nan\n",
      "Epoch 29/40 - RMSE: nan\n",
      "Epoch 30/40 - RMSE: nan\n",
      "Epoch 31/40 - RMSE: nan\n",
      "Epoch 32/40 - RMSE: nan\n",
      "Epoch 33/40 - RMSE: nan\n",
      "Epoch 34/40 - RMSE: nan\n",
      "Epoch 35/40 - RMSE: nan\n",
      "Epoch 36/40 - RMSE: nan\n",
      "Epoch 37/40 - RMSE: nan\n",
      "Epoch 38/40 - RMSE: nan\n",
      "Epoch 39/40 - RMSE: nan\n",
      "Epoch 40/40 - RMSE: nan\n",
      "(factors=20, lr=0.2, reg=0.1, ep=40) - RMSE=nan\n",
      "Epoch 1/50 - RMSE: nan\n",
      "Epoch 2/50 - RMSE: nan\n",
      "Epoch 3/50 - RMSE: nan\n",
      "Epoch 4/50 - RMSE: nan\n",
      "Epoch 5/50 - RMSE: nan\n",
      "Epoch 6/50 - RMSE: nan\n",
      "Epoch 7/50 - RMSE: nan\n",
      "Epoch 8/50 - RMSE: nan\n",
      "Epoch 9/50 - RMSE: nan\n",
      "Epoch 10/50 - RMSE: nan\n",
      "Epoch 11/50 - RMSE: nan\n",
      "Epoch 12/50 - RMSE: nan\n",
      "Epoch 13/50 - RMSE: nan\n",
      "Epoch 14/50 - RMSE: nan\n",
      "Epoch 15/50 - RMSE: nan\n",
      "Epoch 16/50 - RMSE: nan\n",
      "Epoch 17/50 - RMSE: nan\n",
      "Epoch 18/50 - RMSE: nan\n",
      "Epoch 19/50 - RMSE: nan\n",
      "Epoch 20/50 - RMSE: nan\n",
      "Epoch 21/50 - RMSE: nan\n",
      "Epoch 22/50 - RMSE: nan\n",
      "Epoch 23/50 - RMSE: nan\n",
      "Epoch 24/50 - RMSE: nan\n",
      "Epoch 25/50 - RMSE: nan\n",
      "Epoch 26/50 - RMSE: nan\n",
      "Epoch 27/50 - RMSE: nan\n",
      "Epoch 28/50 - RMSE: nan\n",
      "Epoch 29/50 - RMSE: nan\n",
      "Epoch 30/50 - RMSE: nan\n",
      "Epoch 31/50 - RMSE: nan\n",
      "Epoch 32/50 - RMSE: nan\n",
      "Epoch 33/50 - RMSE: nan\n",
      "Epoch 34/50 - RMSE: nan\n",
      "Epoch 35/50 - RMSE: nan\n",
      "Epoch 36/50 - RMSE: nan\n",
      "Epoch 37/50 - RMSE: nan\n",
      "Epoch 38/50 - RMSE: nan\n",
      "Epoch 39/50 - RMSE: nan\n",
      "Epoch 40/50 - RMSE: nan\n",
      "Epoch 41/50 - RMSE: nan\n",
      "Epoch 42/50 - RMSE: nan\n",
      "Epoch 43/50 - RMSE: nan\n",
      "Epoch 44/50 - RMSE: nan\n",
      "Epoch 45/50 - RMSE: nan\n",
      "Epoch 46/50 - RMSE: nan\n",
      "Epoch 47/50 - RMSE: nan\n",
      "Epoch 48/50 - RMSE: nan\n",
      "Epoch 49/50 - RMSE: nan\n",
      "Epoch 50/50 - RMSE: nan\n",
      "(factors=20, lr=0.2, reg=0.1, ep=50) - RMSE=nan\n",
      "Epoch 1/10 - RMSE: 1.0963\n",
      "Epoch 2/10 - RMSE: 1.0571\n",
      "Epoch 3/10 - RMSE: 1.0309\n",
      "Epoch 4/10 - RMSE: 1.0122\n",
      "Epoch 5/10 - RMSE: 0.9981\n",
      "Epoch 6/10 - RMSE: 0.9870\n",
      "Epoch 7/10 - RMSE: 0.9781\n",
      "Epoch 8/10 - RMSE: 0.9706\n",
      "Epoch 9/10 - RMSE: 0.9642\n",
      "Epoch 10/10 - RMSE: 0.9587\n",
      "(factors=50, lr=0.001, reg=0.001, ep=10) - RMSE=1.0005546094448021\n",
      "Epoch 1/20 - RMSE: 1.0957\n",
      "Epoch 2/20 - RMSE: 1.0564\n",
      "Epoch 3/20 - RMSE: 1.0303\n",
      "Epoch 4/20 - RMSE: 1.0116\n",
      "Epoch 5/20 - RMSE: 0.9975\n",
      "Epoch 6/20 - RMSE: 0.9864\n",
      "Epoch 7/20 - RMSE: 0.9774\n",
      "Epoch 8/20 - RMSE: 0.9699\n",
      "Epoch 9/20 - RMSE: 0.9635\n",
      "Epoch 10/20 - RMSE: 0.9580\n",
      "Epoch 11/20 - RMSE: 0.9531\n",
      "Epoch 12/20 - RMSE: 0.9488\n",
      "Epoch 13/20 - RMSE: 0.9448\n",
      "Epoch 14/20 - RMSE: 0.9412\n",
      "Epoch 15/20 - RMSE: 0.9379\n",
      "Epoch 16/20 - RMSE: 0.9348\n",
      "Epoch 17/20 - RMSE: 0.9319\n",
      "Epoch 18/20 - RMSE: 0.9292\n",
      "Epoch 19/20 - RMSE: 0.9266\n",
      "Epoch 20/20 - RMSE: 0.9241\n",
      "(factors=50, lr=0.001, reg=0.001, ep=20) - RMSE=0.9792590918022274\n",
      "Epoch 1/30 - RMSE: 1.0960\n",
      "Epoch 2/30 - RMSE: 1.0568\n",
      "Epoch 3/30 - RMSE: 1.0307\n",
      "Epoch 4/30 - RMSE: 1.0120\n",
      "Epoch 5/30 - RMSE: 0.9979\n",
      "Epoch 6/30 - RMSE: 0.9869\n",
      "Epoch 7/30 - RMSE: 0.9779\n",
      "Epoch 8/30 - RMSE: 0.9704\n",
      "Epoch 9/30 - RMSE: 0.9641\n",
      "Epoch 10/30 - RMSE: 0.9586\n",
      "Epoch 11/30 - RMSE: 0.9537\n",
      "Epoch 12/30 - RMSE: 0.9494\n",
      "Epoch 13/30 - RMSE: 0.9455\n",
      "Epoch 14/30 - RMSE: 0.9419\n",
      "Epoch 15/30 - RMSE: 0.9386\n",
      "Epoch 16/30 - RMSE: 0.9355\n",
      "Epoch 17/30 - RMSE: 0.9326\n",
      "Epoch 18/30 - RMSE: 0.9299\n",
      "Epoch 19/30 - RMSE: 0.9274\n",
      "Epoch 20/30 - RMSE: 0.9249\n",
      "Epoch 21/30 - RMSE: 0.9226\n",
      "Epoch 22/30 - RMSE: 0.9204\n",
      "Epoch 23/30 - RMSE: 0.9182\n",
      "Epoch 24/30 - RMSE: 0.9161\n",
      "Epoch 25/30 - RMSE: 0.9141\n",
      "Epoch 26/30 - RMSE: 0.9121\n",
      "Epoch 27/30 - RMSE: 0.9101\n",
      "Epoch 28/30 - RMSE: 0.9082\n",
      "Epoch 29/30 - RMSE: 0.9064\n",
      "Epoch 30/30 - RMSE: 0.9045\n",
      "(factors=50, lr=0.001, reg=0.001, ep=30) - RMSE=0.9706152839073476\n",
      "Epoch 1/40 - RMSE: 1.0959\n",
      "Epoch 2/40 - RMSE: 1.0567\n",
      "Epoch 3/40 - RMSE: 1.0305\n",
      "Epoch 4/40 - RMSE: 1.0118\n",
      "Epoch 5/40 - RMSE: 0.9976\n",
      "Epoch 6/40 - RMSE: 0.9865\n",
      "Epoch 7/40 - RMSE: 0.9775\n",
      "Epoch 8/40 - RMSE: 0.9700\n",
      "Epoch 9/40 - RMSE: 0.9636\n",
      "Epoch 10/40 - RMSE: 0.9581\n",
      "Epoch 11/40 - RMSE: 0.9532\n",
      "Epoch 12/40 - RMSE: 0.9488\n",
      "Epoch 13/40 - RMSE: 0.9449\n",
      "Epoch 14/40 - RMSE: 0.9413\n",
      "Epoch 15/40 - RMSE: 0.9379\n",
      "Epoch 16/40 - RMSE: 0.9348\n",
      "Epoch 17/40 - RMSE: 0.9319\n",
      "Epoch 18/40 - RMSE: 0.9292\n",
      "Epoch 19/40 - RMSE: 0.9266\n",
      "Epoch 20/40 - RMSE: 0.9241\n",
      "Epoch 21/40 - RMSE: 0.9217\n",
      "Epoch 22/40 - RMSE: 0.9194\n",
      "Epoch 23/40 - RMSE: 0.9172\n",
      "Epoch 24/40 - RMSE: 0.9150\n",
      "Epoch 25/40 - RMSE: 0.9130\n",
      "Epoch 26/40 - RMSE: 0.9109\n",
      "Epoch 27/40 - RMSE: 0.9089\n",
      "Epoch 28/40 - RMSE: 0.9070\n",
      "Epoch 29/40 - RMSE: 0.9050\n",
      "Epoch 30/40 - RMSE: 0.9031\n",
      "Epoch 31/40 - RMSE: 0.9012\n",
      "Epoch 32/40 - RMSE: 0.8994\n",
      "Epoch 33/40 - RMSE: 0.8975\n",
      "Epoch 34/40 - RMSE: 0.8956\n",
      "Epoch 35/40 - RMSE: 0.8938\n",
      "Epoch 36/40 - RMSE: 0.8919\n",
      "Epoch 37/40 - RMSE: 0.8901\n",
      "Epoch 38/40 - RMSE: 0.8882\n",
      "Epoch 39/40 - RMSE: 0.8863\n",
      "Epoch 40/40 - RMSE: 0.8844\n",
      "(factors=50, lr=0.001, reg=0.001, ep=40) - RMSE=0.9647502584677351\n",
      "Epoch 1/50 - RMSE: 1.0960\n",
      "Epoch 2/50 - RMSE: 1.0569\n",
      "Epoch 3/50 - RMSE: 1.0307\n",
      "Epoch 4/50 - RMSE: 1.0119\n",
      "Epoch 5/50 - RMSE: 0.9978\n",
      "Epoch 6/50 - RMSE: 0.9867\n",
      "Epoch 7/50 - RMSE: 0.9777\n",
      "Epoch 8/50 - RMSE: 0.9702\n",
      "Epoch 9/50 - RMSE: 0.9638\n",
      "Epoch 10/50 - RMSE: 0.9583\n",
      "Epoch 11/50 - RMSE: 0.9534\n",
      "Epoch 12/50 - RMSE: 0.9491\n",
      "Epoch 13/50 - RMSE: 0.9451\n",
      "Epoch 14/50 - RMSE: 0.9415\n",
      "Epoch 15/50 - RMSE: 0.9382\n",
      "Epoch 16/50 - RMSE: 0.9351\n",
      "Epoch 17/50 - RMSE: 0.9322\n",
      "Epoch 18/50 - RMSE: 0.9295\n",
      "Epoch 19/50 - RMSE: 0.9269\n",
      "Epoch 20/50 - RMSE: 0.9244\n",
      "Epoch 21/50 - RMSE: 0.9220\n",
      "Epoch 22/50 - RMSE: 0.9198\n",
      "Epoch 23/50 - RMSE: 0.9176\n",
      "Epoch 24/50 - RMSE: 0.9154\n",
      "Epoch 25/50 - RMSE: 0.9134\n",
      "Epoch 26/50 - RMSE: 0.9113\n",
      "Epoch 27/50 - RMSE: 0.9094\n",
      "Epoch 28/50 - RMSE: 0.9074\n",
      "Epoch 29/50 - RMSE: 0.9055\n",
      "Epoch 30/50 - RMSE: 0.9036\n",
      "Epoch 31/50 - RMSE: 0.9017\n",
      "Epoch 32/50 - RMSE: 0.8999\n",
      "Epoch 33/50 - RMSE: 0.8980\n",
      "Epoch 34/50 - RMSE: 0.8962\n",
      "Epoch 35/50 - RMSE: 0.8943\n",
      "Epoch 36/50 - RMSE: 0.8925\n",
      "Epoch 37/50 - RMSE: 0.8907\n",
      "Epoch 38/50 - RMSE: 0.8888\n",
      "Epoch 39/50 - RMSE: 0.8870\n",
      "Epoch 40/50 - RMSE: 0.8851\n",
      "Epoch 41/50 - RMSE: 0.8832\n",
      "Epoch 42/50 - RMSE: 0.8813\n",
      "Epoch 43/50 - RMSE: 0.8794\n",
      "Epoch 44/50 - RMSE: 0.8775\n",
      "Epoch 45/50 - RMSE: 0.8755\n",
      "Epoch 46/50 - RMSE: 0.8736\n",
      "Epoch 47/50 - RMSE: 0.8716\n",
      "Epoch 48/50 - RMSE: 0.8696\n",
      "Epoch 49/50 - RMSE: 0.8675\n",
      "Epoch 50/50 - RMSE: 0.8654\n",
      "(factors=50, lr=0.001, reg=0.001, ep=50) - RMSE=0.9627407770349956\n",
      "Epoch 1/10 - RMSE: 1.0957\n",
      "Epoch 2/10 - RMSE: 1.0566\n",
      "Epoch 3/10 - RMSE: 1.0305\n",
      "Epoch 4/10 - RMSE: 1.0118\n",
      "Epoch 5/10 - RMSE: 0.9978\n",
      "Epoch 6/10 - RMSE: 0.9867\n",
      "Epoch 7/10 - RMSE: 0.9777\n",
      "Epoch 8/10 - RMSE: 0.9703\n",
      "Epoch 9/10 - RMSE: 0.9639\n",
      "Epoch 10/10 - RMSE: 0.9584\n",
      "(factors=50, lr=0.001, reg=0.01, ep=10) - RMSE=1.0001525824902948\n",
      "Epoch 1/20 - RMSE: 1.0961\n",
      "Epoch 2/20 - RMSE: 1.0570\n",
      "Epoch 3/20 - RMSE: 1.0309\n",
      "Epoch 4/20 - RMSE: 1.0123\n",
      "Epoch 5/20 - RMSE: 0.9982\n",
      "Epoch 6/20 - RMSE: 0.9871\n",
      "Epoch 7/20 - RMSE: 0.9782\n",
      "Epoch 8/20 - RMSE: 0.9707\n",
      "Epoch 9/20 - RMSE: 0.9644\n",
      "Epoch 10/20 - RMSE: 0.9589\n",
      "Epoch 11/20 - RMSE: 0.9541\n",
      "Epoch 12/20 - RMSE: 0.9498\n",
      "Epoch 13/20 - RMSE: 0.9459\n",
      "Epoch 14/20 - RMSE: 0.9423\n",
      "Epoch 15/20 - RMSE: 0.9391\n",
      "Epoch 16/20 - RMSE: 0.9361\n",
      "Epoch 17/20 - RMSE: 0.9332\n",
      "Epoch 18/20 - RMSE: 0.9306\n",
      "Epoch 19/20 - RMSE: 0.9281\n",
      "Epoch 20/20 - RMSE: 0.9257\n",
      "(factors=50, lr=0.001, reg=0.01, ep=20) - RMSE=0.9793982844112613\n",
      "Epoch 1/30 - RMSE: 1.0957\n",
      "Epoch 2/30 - RMSE: 1.0566\n",
      "Epoch 3/30 - RMSE: 1.0305\n",
      "Epoch 4/30 - RMSE: 1.0118\n",
      "Epoch 5/30 - RMSE: 0.9977\n",
      "Epoch 6/30 - RMSE: 0.9867\n",
      "Epoch 7/30 - RMSE: 0.9777\n",
      "Epoch 8/30 - RMSE: 0.9703\n",
      "Epoch 9/30 - RMSE: 0.9639\n",
      "Epoch 10/30 - RMSE: 0.9584\n",
      "Epoch 11/30 - RMSE: 0.9536\n",
      "Epoch 12/30 - RMSE: 0.9493\n",
      "Epoch 13/30 - RMSE: 0.9454\n",
      "Epoch 14/30 - RMSE: 0.9418\n",
      "Epoch 15/30 - RMSE: 0.9386\n",
      "Epoch 16/30 - RMSE: 0.9356\n",
      "Epoch 17/30 - RMSE: 0.9327\n",
      "Epoch 18/30 - RMSE: 0.9301\n",
      "Epoch 19/30 - RMSE: 0.9276\n",
      "Epoch 20/30 - RMSE: 0.9252\n",
      "Epoch 21/30 - RMSE: 0.9229\n",
      "Epoch 22/30 - RMSE: 0.9207\n",
      "Epoch 23/30 - RMSE: 0.9186\n",
      "Epoch 24/30 - RMSE: 0.9166\n",
      "Epoch 25/30 - RMSE: 0.9146\n",
      "Epoch 26/30 - RMSE: 0.9127\n",
      "Epoch 27/30 - RMSE: 0.9109\n",
      "Epoch 28/30 - RMSE: 0.9091\n",
      "Epoch 29/30 - RMSE: 0.9073\n",
      "Epoch 30/30 - RMSE: 0.9055\n",
      "(factors=50, lr=0.001, reg=0.01, ep=30) - RMSE=0.9708709196099342\n",
      "Epoch 1/40 - RMSE: 1.0960\n",
      "Epoch 2/40 - RMSE: 1.0568\n",
      "Epoch 3/40 - RMSE: 1.0307\n",
      "Epoch 4/40 - RMSE: 1.0120\n",
      "Epoch 5/40 - RMSE: 0.9979\n",
      "Epoch 6/40 - RMSE: 0.9869\n",
      "Epoch 7/40 - RMSE: 0.9779\n",
      "Epoch 8/40 - RMSE: 0.9704\n",
      "Epoch 9/40 - RMSE: 0.9641\n",
      "Epoch 10/40 - RMSE: 0.9586\n",
      "Epoch 11/40 - RMSE: 0.9537\n",
      "Epoch 12/40 - RMSE: 0.9494\n",
      "Epoch 13/40 - RMSE: 0.9455\n",
      "Epoch 14/40 - RMSE: 0.9420\n",
      "Epoch 15/40 - RMSE: 0.9387\n",
      "Epoch 16/40 - RMSE: 0.9357\n",
      "Epoch 17/40 - RMSE: 0.9328\n",
      "Epoch 18/40 - RMSE: 0.9302\n",
      "Epoch 19/40 - RMSE: 0.9277\n",
      "Epoch 20/40 - RMSE: 0.9253\n",
      "Epoch 21/40 - RMSE: 0.9230\n",
      "Epoch 22/40 - RMSE: 0.9208\n",
      "Epoch 23/40 - RMSE: 0.9187\n",
      "Epoch 24/40 - RMSE: 0.9167\n",
      "Epoch 25/40 - RMSE: 0.9147\n",
      "Epoch 26/40 - RMSE: 0.9128\n",
      "Epoch 27/40 - RMSE: 0.9110\n",
      "Epoch 28/40 - RMSE: 0.9091\n",
      "Epoch 29/40 - RMSE: 0.9074\n",
      "Epoch 30/40 - RMSE: 0.9056\n",
      "Epoch 31/40 - RMSE: 0.9039\n",
      "Epoch 32/40 - RMSE: 0.9022\n",
      "Epoch 33/40 - RMSE: 0.9005\n",
      "Epoch 34/40 - RMSE: 0.8988\n",
      "Epoch 35/40 - RMSE: 0.8972\n",
      "Epoch 36/40 - RMSE: 0.8955\n",
      "Epoch 37/40 - RMSE: 0.8939\n",
      "Epoch 38/40 - RMSE: 0.8923\n",
      "Epoch 39/40 - RMSE: 0.8906\n",
      "Epoch 40/40 - RMSE: 0.8890\n",
      "(factors=50, lr=0.001, reg=0.01, ep=40) - RMSE=0.9666732306107441\n",
      "Epoch 1/50 - RMSE: 1.0963\n",
      "Epoch 2/50 - RMSE: 1.0571\n",
      "Epoch 3/50 - RMSE: 1.0310\n",
      "Epoch 4/50 - RMSE: 1.0123\n",
      "Epoch 5/50 - RMSE: 0.9982\n",
      "Epoch 6/50 - RMSE: 0.9871\n",
      "Epoch 7/50 - RMSE: 0.9782\n",
      "Epoch 8/50 - RMSE: 0.9707\n",
      "Epoch 9/50 - RMSE: 0.9644\n",
      "Epoch 10/50 - RMSE: 0.9589\n",
      "Epoch 11/50 - RMSE: 0.9541\n",
      "Epoch 12/50 - RMSE: 0.9498\n",
      "Epoch 13/50 - RMSE: 0.9459\n",
      "Epoch 14/50 - RMSE: 0.9423\n",
      "Epoch 15/50 - RMSE: 0.9391\n",
      "Epoch 16/50 - RMSE: 0.9361\n",
      "Epoch 17/50 - RMSE: 0.9332\n",
      "Epoch 18/50 - RMSE: 0.9306\n",
      "Epoch 19/50 - RMSE: 0.9281\n",
      "Epoch 20/50 - RMSE: 0.9257\n",
      "Epoch 21/50 - RMSE: 0.9234\n",
      "Epoch 22/50 - RMSE: 0.9213\n",
      "Epoch 23/50 - RMSE: 0.9192\n",
      "Epoch 24/50 - RMSE: 0.9172\n",
      "Epoch 25/50 - RMSE: 0.9152\n",
      "Epoch 26/50 - RMSE: 0.9133\n",
      "Epoch 27/50 - RMSE: 0.9115\n",
      "Epoch 28/50 - RMSE: 0.9097\n",
      "Epoch 29/50 - RMSE: 0.9079\n",
      "Epoch 30/50 - RMSE: 0.9062\n",
      "Epoch 31/50 - RMSE: 0.9044\n",
      "Epoch 32/50 - RMSE: 0.9028\n",
      "Epoch 33/50 - RMSE: 0.9011\n",
      "Epoch 34/50 - RMSE: 0.8994\n",
      "Epoch 35/50 - RMSE: 0.8978\n",
      "Epoch 36/50 - RMSE: 0.8961\n",
      "Epoch 37/50 - RMSE: 0.8945\n",
      "Epoch 38/50 - RMSE: 0.8929\n",
      "Epoch 39/50 - RMSE: 0.8913\n",
      "Epoch 40/50 - RMSE: 0.8896\n",
      "Epoch 41/50 - RMSE: 0.8880\n",
      "Epoch 42/50 - RMSE: 0.8864\n",
      "Epoch 43/50 - RMSE: 0.8847\n",
      "Epoch 44/50 - RMSE: 0.8830\n",
      "Epoch 45/50 - RMSE: 0.8814\n",
      "Epoch 46/50 - RMSE: 0.8797\n",
      "Epoch 47/50 - RMSE: 0.8780\n",
      "Epoch 48/50 - RMSE: 0.8763\n",
      "Epoch 49/50 - RMSE: 0.8745\n",
      "Epoch 50/50 - RMSE: 0.8728\n",
      "(factors=50, lr=0.001, reg=0.01, ep=50) - RMSE=0.9620901114840691\n",
      "Epoch 1/10 - RMSE: 1.0964\n",
      "Epoch 2/10 - RMSE: 1.0578\n",
      "Epoch 3/10 - RMSE: 1.0322\n",
      "Epoch 4/10 - RMSE: 1.0140\n",
      "Epoch 5/10 - RMSE: 1.0002\n",
      "Epoch 6/10 - RMSE: 0.9895\n",
      "Epoch 7/10 - RMSE: 0.9808\n",
      "Epoch 8/10 - RMSE: 0.9736\n",
      "Epoch 9/10 - RMSE: 0.9676\n",
      "Epoch 10/10 - RMSE: 0.9624\n",
      "(factors=50, lr=0.001, reg=0.1, ep=10) - RMSE=1.0019863147921217\n",
      "Epoch 1/20 - RMSE: 1.0961\n",
      "Epoch 2/20 - RMSE: 1.0575\n",
      "Epoch 3/20 - RMSE: 1.0319\n",
      "Epoch 4/20 - RMSE: 1.0136\n",
      "Epoch 5/20 - RMSE: 1.0000\n",
      "Epoch 6/20 - RMSE: 0.9892\n",
      "Epoch 7/20 - RMSE: 0.9806\n",
      "Epoch 8/20 - RMSE: 0.9734\n",
      "Epoch 9/20 - RMSE: 0.9674\n",
      "Epoch 10/20 - RMSE: 0.9622\n",
      "Epoch 11/20 - RMSE: 0.9577\n",
      "Epoch 12/20 - RMSE: 0.9538\n",
      "Epoch 13/20 - RMSE: 0.9502\n",
      "Epoch 14/20 - RMSE: 0.9471\n",
      "Epoch 15/20 - RMSE: 0.9442\n",
      "Epoch 16/20 - RMSE: 0.9417\n",
      "Epoch 17/20 - RMSE: 0.9393\n",
      "Epoch 18/20 - RMSE: 0.9371\n",
      "Epoch 19/20 - RMSE: 0.9351\n",
      "Epoch 20/20 - RMSE: 0.9332\n",
      "(factors=50, lr=0.001, reg=0.1, ep=20) - RMSE=0.9804251249794066\n",
      "Epoch 1/30 - RMSE: 1.0961\n",
      "Epoch 2/30 - RMSE: 1.0575\n",
      "Epoch 3/30 - RMSE: 1.0319\n",
      "Epoch 4/30 - RMSE: 1.0137\n",
      "Epoch 5/30 - RMSE: 1.0000\n",
      "Epoch 6/30 - RMSE: 0.9892\n",
      "Epoch 7/30 - RMSE: 0.9806\n",
      "Epoch 8/30 - RMSE: 0.9735\n",
      "Epoch 9/30 - RMSE: 0.9674\n",
      "Epoch 10/30 - RMSE: 0.9623\n",
      "Epoch 11/30 - RMSE: 0.9578\n",
      "Epoch 12/30 - RMSE: 0.9538\n",
      "Epoch 13/30 - RMSE: 0.9503\n",
      "Epoch 14/30 - RMSE: 0.9471\n",
      "Epoch 15/30 - RMSE: 0.9443\n",
      "Epoch 16/30 - RMSE: 0.9417\n",
      "Epoch 17/30 - RMSE: 0.9393\n",
      "Epoch 18/30 - RMSE: 0.9372\n",
      "Epoch 19/30 - RMSE: 0.9352\n",
      "Epoch 20/30 - RMSE: 0.9333\n",
      "Epoch 21/30 - RMSE: 0.9316\n",
      "Epoch 22/30 - RMSE: 0.9300\n",
      "Epoch 23/30 - RMSE: 0.9285\n",
      "Epoch 24/30 - RMSE: 0.9271\n",
      "Epoch 25/30 - RMSE: 0.9257\n",
      "Epoch 26/30 - RMSE: 0.9245\n",
      "Epoch 27/30 - RMSE: 0.9233\n",
      "Epoch 28/30 - RMSE: 0.9222\n",
      "Epoch 29/30 - RMSE: 0.9211\n",
      "Epoch 30/30 - RMSE: 0.9201\n",
      "(factors=50, lr=0.001, reg=0.1, ep=30) - RMSE=0.9718946181193889\n",
      "Epoch 1/40 - RMSE: 1.0961\n",
      "Epoch 2/40 - RMSE: 1.0575\n",
      "Epoch 3/40 - RMSE: 1.0319\n",
      "Epoch 4/40 - RMSE: 1.0137\n",
      "Epoch 5/40 - RMSE: 1.0000\n",
      "Epoch 6/40 - RMSE: 0.9892\n",
      "Epoch 7/40 - RMSE: 0.9806\n",
      "Epoch 8/40 - RMSE: 0.9734\n",
      "Epoch 9/40 - RMSE: 0.9674\n",
      "Epoch 10/40 - RMSE: 0.9622\n",
      "Epoch 11/40 - RMSE: 0.9577\n",
      "Epoch 12/40 - RMSE: 0.9538\n",
      "Epoch 13/40 - RMSE: 0.9503\n",
      "Epoch 14/40 - RMSE: 0.9471\n",
      "Epoch 15/40 - RMSE: 0.9443\n",
      "Epoch 16/40 - RMSE: 0.9417\n",
      "Epoch 17/40 - RMSE: 0.9393\n",
      "Epoch 18/40 - RMSE: 0.9372\n",
      "Epoch 19/40 - RMSE: 0.9352\n",
      "Epoch 20/40 - RMSE: 0.9333\n",
      "Epoch 21/40 - RMSE: 0.9316\n",
      "Epoch 22/40 - RMSE: 0.9300\n",
      "Epoch 23/40 - RMSE: 0.9285\n",
      "Epoch 24/40 - RMSE: 0.9271\n",
      "Epoch 25/40 - RMSE: 0.9258\n",
      "Epoch 26/40 - RMSE: 0.9245\n",
      "Epoch 27/40 - RMSE: 0.9233\n",
      "Epoch 28/40 - RMSE: 0.9222\n",
      "Epoch 29/40 - RMSE: 0.9212\n",
      "Epoch 30/40 - RMSE: 0.9201\n",
      "Epoch 31/40 - RMSE: 0.9192\n",
      "Epoch 32/40 - RMSE: 0.9183\n",
      "Epoch 33/40 - RMSE: 0.9174\n",
      "Epoch 34/40 - RMSE: 0.9166\n",
      "Epoch 35/40 - RMSE: 0.9158\n",
      "Epoch 36/40 - RMSE: 0.9150\n",
      "Epoch 37/40 - RMSE: 0.9143\n",
      "Epoch 38/40 - RMSE: 0.9136\n",
      "Epoch 39/40 - RMSE: 0.9129\n",
      "Epoch 40/40 - RMSE: 0.9122\n",
      "(factors=50, lr=0.001, reg=0.1, ep=40) - RMSE=0.9667274354982474\n",
      "Epoch 1/50 - RMSE: 1.0961\n",
      "Epoch 2/50 - RMSE: 1.0575\n",
      "Epoch 3/50 - RMSE: 1.0319\n",
      "Epoch 4/50 - RMSE: 1.0137\n",
      "Epoch 5/50 - RMSE: 1.0000\n",
      "Epoch 6/50 - RMSE: 0.9893\n",
      "Epoch 7/50 - RMSE: 0.9806\n",
      "Epoch 8/50 - RMSE: 0.9735\n",
      "Epoch 9/50 - RMSE: 0.9675\n",
      "Epoch 10/50 - RMSE: 0.9623\n",
      "Epoch 11/50 - RMSE: 0.9578\n",
      "Epoch 12/50 - RMSE: 0.9539\n",
      "Epoch 13/50 - RMSE: 0.9504\n",
      "Epoch 14/50 - RMSE: 0.9472\n",
      "Epoch 15/50 - RMSE: 0.9444\n",
      "Epoch 16/50 - RMSE: 0.9418\n",
      "Epoch 17/50 - RMSE: 0.9394\n",
      "Epoch 18/50 - RMSE: 0.9373\n",
      "Epoch 19/50 - RMSE: 0.9353\n",
      "Epoch 20/50 - RMSE: 0.9334\n",
      "Epoch 21/50 - RMSE: 0.9317\n",
      "Epoch 22/50 - RMSE: 0.9300\n",
      "Epoch 23/50 - RMSE: 0.9286\n",
      "Epoch 24/50 - RMSE: 0.9272\n",
      "Epoch 25/50 - RMSE: 0.9258\n",
      "Epoch 26/50 - RMSE: 0.9246\n",
      "Epoch 27/50 - RMSE: 0.9234\n",
      "Epoch 28/50 - RMSE: 0.9223\n",
      "Epoch 29/50 - RMSE: 0.9212\n",
      "Epoch 30/50 - RMSE: 0.9202\n",
      "Epoch 31/50 - RMSE: 0.9193\n",
      "Epoch 32/50 - RMSE: 0.9184\n",
      "Epoch 33/50 - RMSE: 0.9175\n",
      "Epoch 34/50 - RMSE: 0.9166\n",
      "Epoch 35/50 - RMSE: 0.9159\n",
      "Epoch 36/50 - RMSE: 0.9151\n",
      "Epoch 37/50 - RMSE: 0.9144\n",
      "Epoch 38/50 - RMSE: 0.9137\n",
      "Epoch 39/50 - RMSE: 0.9130\n",
      "Epoch 40/50 - RMSE: 0.9123\n",
      "Epoch 41/50 - RMSE: 0.9117\n",
      "Epoch 42/50 - RMSE: 0.9111\n",
      "Epoch 43/50 - RMSE: 0.9105\n",
      "Epoch 44/50 - RMSE: 0.9099\n",
      "Epoch 45/50 - RMSE: 0.9093\n",
      "Epoch 46/50 - RMSE: 0.9088\n",
      "Epoch 47/50 - RMSE: 0.9083\n",
      "Epoch 48/50 - RMSE: 0.9078\n",
      "Epoch 49/50 - RMSE: 0.9073\n",
      "Epoch 50/50 - RMSE: 0.9067\n",
      "(factors=50, lr=0.001, reg=0.1, ep=50) - RMSE=0.9633531513959823\n",
      "Epoch 1/10 - RMSE: 1.0445\n",
      "Epoch 2/10 - RMSE: 0.9765\n",
      "Epoch 3/10 - RMSE: 0.9505\n",
      "Epoch 4/10 - RMSE: 0.9347\n",
      "Epoch 5/10 - RMSE: 0.9230\n",
      "Epoch 6/10 - RMSE: 0.9130\n",
      "Epoch 7/10 - RMSE: 0.9038\n",
      "Epoch 8/10 - RMSE: 0.8950\n",
      "Epoch 9/10 - RMSE: 0.8861\n",
      "Epoch 10/10 - RMSE: 0.8766\n",
      "(factors=50, lr=0.005, reg=0.001, ep=10) - RMSE=0.9644034916167519\n",
      "Epoch 1/20 - RMSE: 1.0449\n",
      "Epoch 2/20 - RMSE: 0.9766\n",
      "Epoch 3/20 - RMSE: 0.9506\n",
      "Epoch 4/20 - RMSE: 0.9347\n",
      "Epoch 5/20 - RMSE: 0.9228\n",
      "Epoch 6/20 - RMSE: 0.9127\n",
      "Epoch 7/20 - RMSE: 0.9034\n",
      "Epoch 8/20 - RMSE: 0.8944\n",
      "Epoch 9/20 - RMSE: 0.8852\n",
      "Epoch 10/20 - RMSE: 0.8756\n",
      "Epoch 11/20 - RMSE: 0.8653\n",
      "Epoch 12/20 - RMSE: 0.8542\n",
      "Epoch 13/20 - RMSE: 0.8422\n",
      "Epoch 14/20 - RMSE: 0.8292\n",
      "Epoch 15/20 - RMSE: 0.8154\n",
      "Epoch 16/20 - RMSE: 0.8008\n",
      "Epoch 17/20 - RMSE: 0.7856\n",
      "Epoch 18/20 - RMSE: 0.7701\n",
      "Epoch 19/20 - RMSE: 0.7544\n",
      "Epoch 20/20 - RMSE: 0.7388\n",
      "(factors=50, lr=0.005, reg=0.001, ep=20) - RMSE=0.9591417563925912\n",
      "Epoch 1/30 - RMSE: 1.0449\n",
      "Epoch 2/30 - RMSE: 0.9765\n",
      "Epoch 3/30 - RMSE: 0.9506\n",
      "Epoch 4/30 - RMSE: 0.9347\n",
      "Epoch 5/30 - RMSE: 0.9228\n",
      "Epoch 6/30 - RMSE: 0.9127\n",
      "Epoch 7/30 - RMSE: 0.9034\n",
      "Epoch 8/30 - RMSE: 0.8944\n",
      "Epoch 9/30 - RMSE: 0.8852\n",
      "Epoch 10/30 - RMSE: 0.8756\n",
      "Epoch 11/30 - RMSE: 0.8654\n",
      "Epoch 12/30 - RMSE: 0.8543\n",
      "Epoch 13/30 - RMSE: 0.8424\n",
      "Epoch 14/30 - RMSE: 0.8295\n",
      "Epoch 15/30 - RMSE: 0.8158\n",
      "Epoch 16/30 - RMSE: 0.8015\n",
      "Epoch 17/30 - RMSE: 0.7865\n",
      "Epoch 18/30 - RMSE: 0.7713\n",
      "Epoch 19/30 - RMSE: 0.7559\n",
      "Epoch 20/30 - RMSE: 0.7404\n",
      "Epoch 21/30 - RMSE: 0.7250\n",
      "Epoch 22/30 - RMSE: 0.7098\n",
      "Epoch 23/30 - RMSE: 0.6948\n",
      "Epoch 24/30 - RMSE: 0.6802\n",
      "Epoch 25/30 - RMSE: 0.6659\n",
      "Epoch 26/30 - RMSE: 0.6520\n",
      "Epoch 27/30 - RMSE: 0.6386\n",
      "Epoch 28/30 - RMSE: 0.6256\n",
      "Epoch 29/30 - RMSE: 0.6130\n",
      "Epoch 30/30 - RMSE: 0.6010\n",
      "(factors=50, lr=0.005, reg=0.001, ep=30) - RMSE=0.9816005830121302\n",
      "Epoch 1/40 - RMSE: 1.0451\n",
      "Epoch 2/40 - RMSE: 0.9769\n",
      "Epoch 3/40 - RMSE: 0.9510\n",
      "Epoch 4/40 - RMSE: 0.9351\n",
      "Epoch 5/40 - RMSE: 0.9232\n",
      "Epoch 6/40 - RMSE: 0.9132\n",
      "Epoch 7/40 - RMSE: 0.9040\n",
      "Epoch 8/40 - RMSE: 0.8950\n",
      "Epoch 9/40 - RMSE: 0.8860\n",
      "Epoch 10/40 - RMSE: 0.8765\n",
      "Epoch 11/40 - RMSE: 0.8663\n",
      "Epoch 12/40 - RMSE: 0.8553\n",
      "Epoch 13/40 - RMSE: 0.8435\n",
      "Epoch 14/40 - RMSE: 0.8308\n",
      "Epoch 15/40 - RMSE: 0.8173\n",
      "Epoch 16/40 - RMSE: 0.8030\n",
      "Epoch 17/40 - RMSE: 0.7882\n",
      "Epoch 18/40 - RMSE: 0.7730\n",
      "Epoch 19/40 - RMSE: 0.7576\n",
      "Epoch 20/40 - RMSE: 0.7421\n",
      "Epoch 21/40 - RMSE: 0.7266\n",
      "Epoch 22/40 - RMSE: 0.7113\n",
      "Epoch 23/40 - RMSE: 0.6962\n",
      "Epoch 24/40 - RMSE: 0.6814\n",
      "Epoch 25/40 - RMSE: 0.6669\n",
      "Epoch 26/40 - RMSE: 0.6529\n",
      "Epoch 27/40 - RMSE: 0.6393\n",
      "Epoch 28/40 - RMSE: 0.6262\n",
      "Epoch 29/40 - RMSE: 0.6135\n",
      "Epoch 30/40 - RMSE: 0.6013\n",
      "Epoch 31/40 - RMSE: 0.5896\n",
      "Epoch 32/40 - RMSE: 0.5783\n",
      "Epoch 33/40 - RMSE: 0.5676\n",
      "Epoch 34/40 - RMSE: 0.5572\n",
      "Epoch 35/40 - RMSE: 0.5473\n",
      "Epoch 36/40 - RMSE: 0.5378\n",
      "Epoch 37/40 - RMSE: 0.5286\n",
      "Epoch 38/40 - RMSE: 0.5199\n",
      "Epoch 39/40 - RMSE: 0.5115\n",
      "Epoch 40/40 - RMSE: 0.5035\n",
      "(factors=50, lr=0.005, reg=0.001, ep=40) - RMSE=1.0245896486772095\n",
      "Epoch 1/50 - RMSE: 1.0451\n",
      "Epoch 2/50 - RMSE: 0.9768\n",
      "Epoch 3/50 - RMSE: 0.9508\n",
      "Epoch 4/50 - RMSE: 0.9350\n",
      "Epoch 5/50 - RMSE: 0.9230\n",
      "Epoch 6/50 - RMSE: 0.9130\n",
      "Epoch 7/50 - RMSE: 0.9037\n",
      "Epoch 8/50 - RMSE: 0.8946\n",
      "Epoch 9/50 - RMSE: 0.8854\n",
      "Epoch 10/50 - RMSE: 0.8758\n",
      "Epoch 11/50 - RMSE: 0.8654\n",
      "Epoch 12/50 - RMSE: 0.8543\n",
      "Epoch 13/50 - RMSE: 0.8422\n",
      "Epoch 14/50 - RMSE: 0.8293\n",
      "Epoch 15/50 - RMSE: 0.8157\n",
      "Epoch 16/50 - RMSE: 0.8013\n",
      "Epoch 17/50 - RMSE: 0.7865\n",
      "Epoch 18/50 - RMSE: 0.7714\n",
      "Epoch 19/50 - RMSE: 0.7561\n",
      "Epoch 20/50 - RMSE: 0.7407\n",
      "Epoch 21/50 - RMSE: 0.7253\n",
      "Epoch 22/50 - RMSE: 0.7100\n",
      "Epoch 23/50 - RMSE: 0.6950\n",
      "Epoch 24/50 - RMSE: 0.6802\n",
      "Epoch 25/50 - RMSE: 0.6659\n",
      "Epoch 26/50 - RMSE: 0.6519\n",
      "Epoch 27/50 - RMSE: 0.6383\n",
      "Epoch 28/50 - RMSE: 0.6252\n",
      "Epoch 29/50 - RMSE: 0.6126\n",
      "Epoch 30/50 - RMSE: 0.6005\n",
      "Epoch 31/50 - RMSE: 0.5889\n",
      "Epoch 32/50 - RMSE: 0.5776\n",
      "Epoch 33/50 - RMSE: 0.5669\n",
      "Epoch 34/50 - RMSE: 0.5566\n",
      "Epoch 35/50 - RMSE: 0.5467\n",
      "Epoch 36/50 - RMSE: 0.5372\n",
      "Epoch 37/50 - RMSE: 0.5281\n",
      "Epoch 38/50 - RMSE: 0.5193\n",
      "Epoch 39/50 - RMSE: 0.5109\n",
      "Epoch 40/50 - RMSE: 0.5029\n",
      "Epoch 41/50 - RMSE: 0.4951\n",
      "Epoch 42/50 - RMSE: 0.4877\n",
      "Epoch 43/50 - RMSE: 0.4806\n",
      "Epoch 44/50 - RMSE: 0.4737\n",
      "Epoch 45/50 - RMSE: 0.4671\n",
      "Epoch 46/50 - RMSE: 0.4608\n",
      "Epoch 47/50 - RMSE: 0.4546\n",
      "Epoch 48/50 - RMSE: 0.4488\n",
      "Epoch 49/50 - RMSE: 0.4431\n",
      "Epoch 50/50 - RMSE: 0.4376\n",
      "(factors=50, lr=0.005, reg=0.001, ep=50) - RMSE=1.055579380874597\n",
      "Epoch 1/10 - RMSE: 1.0445\n",
      "Epoch 2/10 - RMSE: 0.9768\n",
      "Epoch 3/10 - RMSE: 0.9509\n",
      "Epoch 4/10 - RMSE: 0.9353\n",
      "Epoch 5/10 - RMSE: 0.9238\n",
      "Epoch 6/10 - RMSE: 0.9142\n",
      "Epoch 7/10 - RMSE: 0.9056\n",
      "Epoch 8/10 - RMSE: 0.8974\n",
      "Epoch 9/10 - RMSE: 0.8893\n",
      "Epoch 10/10 - RMSE: 0.8809\n",
      "(factors=50, lr=0.005, reg=0.01, ep=10) - RMSE=0.9613123862688855\n",
      "Epoch 1/20 - RMSE: 1.0453\n",
      "Epoch 2/20 - RMSE: 0.9771\n",
      "Epoch 3/20 - RMSE: 0.9512\n",
      "Epoch 4/20 - RMSE: 0.9356\n",
      "Epoch 5/20 - RMSE: 0.9240\n",
      "Epoch 6/20 - RMSE: 0.9144\n",
      "Epoch 7/20 - RMSE: 0.9057\n",
      "Epoch 8/20 - RMSE: 0.8975\n",
      "Epoch 9/20 - RMSE: 0.8893\n",
      "Epoch 10/20 - RMSE: 0.8809\n",
      "Epoch 11/20 - RMSE: 0.8720\n",
      "Epoch 12/20 - RMSE: 0.8624\n",
      "Epoch 13/20 - RMSE: 0.8521\n",
      "Epoch 14/20 - RMSE: 0.8411\n",
      "Epoch 15/20 - RMSE: 0.8292\n",
      "Epoch 16/20 - RMSE: 0.8167\n",
      "Epoch 17/20 - RMSE: 0.8035\n",
      "Epoch 18/20 - RMSE: 0.7899\n",
      "Epoch 19/20 - RMSE: 0.7759\n",
      "Epoch 20/20 - RMSE: 0.7618\n",
      "(factors=50, lr=0.005, reg=0.01, ep=20) - RMSE=0.953915479778839\n",
      "Epoch 1/30 - RMSE: 1.0449\n",
      "Epoch 2/30 - RMSE: 0.9768\n",
      "Epoch 3/30 - RMSE: 0.9509\n",
      "Epoch 4/30 - RMSE: 0.9353\n",
      "Epoch 5/30 - RMSE: 0.9237\n",
      "Epoch 6/30 - RMSE: 0.9141\n",
      "Epoch 7/30 - RMSE: 0.9055\n",
      "Epoch 8/30 - RMSE: 0.8972\n",
      "Epoch 9/30 - RMSE: 0.8889\n",
      "Epoch 10/30 - RMSE: 0.8805\n",
      "Epoch 11/30 - RMSE: 0.8715\n",
      "Epoch 12/30 - RMSE: 0.8619\n",
      "Epoch 13/30 - RMSE: 0.8516\n",
      "Epoch 14/30 - RMSE: 0.8404\n",
      "Epoch 15/30 - RMSE: 0.8286\n",
      "Epoch 16/30 - RMSE: 0.8159\n",
      "Epoch 17/30 - RMSE: 0.8027\n",
      "Epoch 18/30 - RMSE: 0.7891\n",
      "Epoch 19/30 - RMSE: 0.7750\n",
      "Epoch 20/30 - RMSE: 0.7609\n",
      "Epoch 21/30 - RMSE: 0.7465\n",
      "Epoch 22/30 - RMSE: 0.7323\n",
      "Epoch 23/30 - RMSE: 0.7181\n",
      "Epoch 24/30 - RMSE: 0.7041\n",
      "Epoch 25/30 - RMSE: 0.6903\n",
      "Epoch 26/30 - RMSE: 0.6768\n",
      "Epoch 27/30 - RMSE: 0.6636\n",
      "Epoch 28/30 - RMSE: 0.6509\n",
      "Epoch 29/30 - RMSE: 0.6384\n",
      "Epoch 30/30 - RMSE: 0.6264\n",
      "(factors=50, lr=0.005, reg=0.01, ep=30) - RMSE=0.962866431042252\n",
      "Epoch 1/40 - RMSE: 1.0450\n",
      "Epoch 2/40 - RMSE: 0.9769\n",
      "Epoch 3/40 - RMSE: 0.9511\n",
      "Epoch 4/40 - RMSE: 0.9356\n",
      "Epoch 5/40 - RMSE: 0.9241\n",
      "Epoch 6/40 - RMSE: 0.9146\n",
      "Epoch 7/40 - RMSE: 0.9060\n",
      "Epoch 8/40 - RMSE: 0.8978\n",
      "Epoch 9/40 - RMSE: 0.8898\n",
      "Epoch 10/40 - RMSE: 0.8814\n",
      "Epoch 11/40 - RMSE: 0.8727\n",
      "Epoch 12/40 - RMSE: 0.8632\n",
      "Epoch 13/40 - RMSE: 0.8531\n",
      "Epoch 14/40 - RMSE: 0.8422\n",
      "Epoch 15/40 - RMSE: 0.8305\n",
      "Epoch 16/40 - RMSE: 0.8181\n",
      "Epoch 17/40 - RMSE: 0.8050\n",
      "Epoch 18/40 - RMSE: 0.7914\n",
      "Epoch 19/40 - RMSE: 0.7775\n",
      "Epoch 20/40 - RMSE: 0.7633\n",
      "Epoch 21/40 - RMSE: 0.7491\n",
      "Epoch 22/40 - RMSE: 0.7349\n",
      "Epoch 23/40 - RMSE: 0.7208\n",
      "Epoch 24/40 - RMSE: 0.7069\n",
      "Epoch 25/40 - RMSE: 0.6932\n",
      "Epoch 26/40 - RMSE: 0.6799\n",
      "Epoch 27/40 - RMSE: 0.6668\n",
      "Epoch 28/40 - RMSE: 0.6541\n",
      "Epoch 29/40 - RMSE: 0.6417\n",
      "Epoch 30/40 - RMSE: 0.6298\n",
      "Epoch 31/40 - RMSE: 0.6182\n",
      "Epoch 32/40 - RMSE: 0.6071\n",
      "Epoch 33/40 - RMSE: 0.5963\n",
      "Epoch 34/40 - RMSE: 0.5859\n",
      "Epoch 35/40 - RMSE: 0.5759\n",
      "Epoch 36/40 - RMSE: 0.5664\n",
      "Epoch 37/40 - RMSE: 0.5570\n",
      "Epoch 38/40 - RMSE: 0.5481\n",
      "Epoch 39/40 - RMSE: 0.5396\n",
      "Epoch 40/40 - RMSE: 0.5313\n",
      "(factors=50, lr=0.005, reg=0.01, ep=40) - RMSE=0.9879870992274127\n",
      "Epoch 1/50 - RMSE: 1.0450\n",
      "Epoch 2/50 - RMSE: 0.9770\n",
      "Epoch 3/50 - RMSE: 0.9512\n",
      "Epoch 4/50 - RMSE: 0.9356\n",
      "Epoch 5/50 - RMSE: 0.9241\n",
      "Epoch 6/50 - RMSE: 0.9145\n",
      "Epoch 7/50 - RMSE: 0.9059\n",
      "Epoch 8/50 - RMSE: 0.8976\n",
      "Epoch 9/50 - RMSE: 0.8894\n",
      "Epoch 10/50 - RMSE: 0.8809\n",
      "Epoch 11/50 - RMSE: 0.8719\n",
      "Epoch 12/50 - RMSE: 0.8623\n",
      "Epoch 13/50 - RMSE: 0.8520\n",
      "Epoch 14/50 - RMSE: 0.8409\n",
      "Epoch 15/50 - RMSE: 0.8290\n",
      "Epoch 16/50 - RMSE: 0.8164\n",
      "Epoch 17/50 - RMSE: 0.8033\n",
      "Epoch 18/50 - RMSE: 0.7897\n",
      "Epoch 19/50 - RMSE: 0.7758\n",
      "Epoch 20/50 - RMSE: 0.7617\n",
      "Epoch 21/50 - RMSE: 0.7476\n",
      "Epoch 22/50 - RMSE: 0.7335\n",
      "Epoch 23/50 - RMSE: 0.7195\n",
      "Epoch 24/50 - RMSE: 0.7057\n",
      "Epoch 25/50 - RMSE: 0.6921\n",
      "Epoch 26/50 - RMSE: 0.6788\n",
      "Epoch 27/50 - RMSE: 0.6658\n",
      "Epoch 28/50 - RMSE: 0.6532\n",
      "Epoch 29/50 - RMSE: 0.6409\n",
      "Epoch 30/50 - RMSE: 0.6290\n",
      "Epoch 31/50 - RMSE: 0.6175\n",
      "Epoch 32/50 - RMSE: 0.6064\n",
      "Epoch 33/50 - RMSE: 0.5957\n",
      "Epoch 34/50 - RMSE: 0.5854\n",
      "Epoch 35/50 - RMSE: 0.5755\n",
      "Epoch 36/50 - RMSE: 0.5659\n",
      "Epoch 37/50 - RMSE: 0.5567\n",
      "Epoch 38/50 - RMSE: 0.5479\n",
      "Epoch 39/50 - RMSE: 0.5393\n",
      "Epoch 40/50 - RMSE: 0.5312\n",
      "Epoch 41/50 - RMSE: 0.5234\n",
      "Epoch 42/50 - RMSE: 0.5158\n",
      "Epoch 43/50 - RMSE: 0.5085\n",
      "Epoch 44/50 - RMSE: 0.5016\n",
      "Epoch 45/50 - RMSE: 0.4948\n",
      "Epoch 46/50 - RMSE: 0.4884\n",
      "Epoch 47/50 - RMSE: 0.4822\n",
      "Epoch 48/50 - RMSE: 0.4761\n",
      "Epoch 49/50 - RMSE: 0.4704\n",
      "Epoch 50/50 - RMSE: 0.4648\n",
      "(factors=50, lr=0.005, reg=0.01, ep=50) - RMSE=1.0089077553668728\n",
      "Epoch 1/10 - RMSE: 1.0461\n",
      "Epoch 2/10 - RMSE: 0.9797\n",
      "Epoch 3/10 - RMSE: 0.9555\n",
      "Epoch 4/10 - RMSE: 0.9420\n",
      "Epoch 5/10 - RMSE: 0.9331\n",
      "Epoch 6/10 - RMSE: 0.9267\n",
      "Epoch 7/10 - RMSE: 0.9217\n",
      "Epoch 8/10 - RMSE: 0.9178\n",
      "Epoch 9/10 - RMSE: 0.9146\n",
      "Epoch 10/10 - RMSE: 0.9119\n",
      "(factors=50, lr=0.005, reg=0.1, ep=10) - RMSE=0.9631515575337751\n",
      "Epoch 1/20 - RMSE: 1.0460\n",
      "Epoch 2/20 - RMSE: 0.9796\n",
      "Epoch 3/20 - RMSE: 0.9553\n",
      "Epoch 4/20 - RMSE: 0.9418\n",
      "Epoch 5/20 - RMSE: 0.9329\n",
      "Epoch 6/20 - RMSE: 0.9265\n",
      "Epoch 7/20 - RMSE: 0.9216\n",
      "Epoch 8/20 - RMSE: 0.9177\n",
      "Epoch 9/20 - RMSE: 0.9143\n",
      "Epoch 10/20 - RMSE: 0.9117\n",
      "Epoch 11/20 - RMSE: 0.9091\n",
      "Epoch 12/20 - RMSE: 0.9068\n",
      "Epoch 13/20 - RMSE: 0.9049\n",
      "Epoch 14/20 - RMSE: 0.9027\n",
      "Epoch 15/20 - RMSE: 0.9008\n",
      "Epoch 16/20 - RMSE: 0.8989\n",
      "Epoch 17/20 - RMSE: 0.8969\n",
      "Epoch 18/20 - RMSE: 0.8950\n",
      "Epoch 19/20 - RMSE: 0.8928\n",
      "Epoch 20/20 - RMSE: 0.8908\n",
      "(factors=50, lr=0.005, reg=0.1, ep=20) - RMSE=0.9539343483191378\n",
      "Epoch 1/30 - RMSE: 1.0455\n",
      "Epoch 2/30 - RMSE: 0.9790\n",
      "Epoch 3/30 - RMSE: 0.9548\n",
      "Epoch 4/30 - RMSE: 0.9413\n",
      "Epoch 5/30 - RMSE: 0.9324\n",
      "Epoch 6/30 - RMSE: 0.9260\n",
      "Epoch 7/30 - RMSE: 0.9211\n",
      "Epoch 8/30 - RMSE: 0.9172\n",
      "Epoch 9/30 - RMSE: 0.9139\n",
      "Epoch 10/30 - RMSE: 0.9111\n",
      "Epoch 11/30 - RMSE: 0.9087\n",
      "Epoch 12/30 - RMSE: 0.9065\n",
      "Epoch 13/30 - RMSE: 0.9044\n",
      "Epoch 14/30 - RMSE: 0.9023\n",
      "Epoch 15/30 - RMSE: 0.9005\n",
      "Epoch 16/30 - RMSE: 0.8985\n",
      "Epoch 17/30 - RMSE: 0.8966\n",
      "Epoch 18/30 - RMSE: 0.8947\n",
      "Epoch 19/30 - RMSE: 0.8927\n",
      "Epoch 20/30 - RMSE: 0.8906\n",
      "Epoch 21/30 - RMSE: 0.8884\n",
      "Epoch 22/30 - RMSE: 0.8860\n",
      "Epoch 23/30 - RMSE: 0.8838\n",
      "Epoch 24/30 - RMSE: 0.8813\n",
      "Epoch 25/30 - RMSE: 0.8786\n",
      "Epoch 26/30 - RMSE: 0.8759\n",
      "Epoch 27/30 - RMSE: 0.8731\n",
      "Epoch 28/30 - RMSE: 0.8702\n",
      "Epoch 29/30 - RMSE: 0.8671\n",
      "Epoch 30/30 - RMSE: 0.8642\n",
      "(factors=50, lr=0.005, reg=0.1, ep=30) - RMSE=0.9450041534113457\n",
      "Epoch 1/40 - RMSE: 1.0460\n",
      "Epoch 2/40 - RMSE: 0.9795\n",
      "Epoch 3/40 - RMSE: 0.9553\n",
      "Epoch 4/40 - RMSE: 0.9417\n",
      "Epoch 5/40 - RMSE: 0.9328\n",
      "Epoch 6/40 - RMSE: 0.9264\n",
      "Epoch 7/40 - RMSE: 0.9214\n",
      "Epoch 8/40 - RMSE: 0.9176\n",
      "Epoch 9/40 - RMSE: 0.9143\n",
      "Epoch 10/40 - RMSE: 0.9116\n",
      "Epoch 11/40 - RMSE: 0.9091\n",
      "Epoch 12/40 - RMSE: 0.9068\n",
      "Epoch 13/40 - RMSE: 0.9047\n",
      "Epoch 14/40 - RMSE: 0.9027\n",
      "Epoch 15/40 - RMSE: 0.9009\n",
      "Epoch 16/40 - RMSE: 0.8989\n",
      "Epoch 17/40 - RMSE: 0.8970\n",
      "Epoch 18/40 - RMSE: 0.8950\n",
      "Epoch 19/40 - RMSE: 0.8930\n",
      "Epoch 20/40 - RMSE: 0.8910\n",
      "Epoch 21/40 - RMSE: 0.8888\n",
      "Epoch 22/40 - RMSE: 0.8866\n",
      "Epoch 23/40 - RMSE: 0.8841\n",
      "Epoch 24/40 - RMSE: 0.8816\n",
      "Epoch 25/40 - RMSE: 0.8791\n",
      "Epoch 26/40 - RMSE: 0.8764\n",
      "Epoch 27/40 - RMSE: 0.8736\n",
      "Epoch 28/40 - RMSE: 0.8707\n",
      "Epoch 29/40 - RMSE: 0.8676\n",
      "Epoch 30/40 - RMSE: 0.8645\n",
      "Epoch 31/40 - RMSE: 0.8614\n",
      "Epoch 32/40 - RMSE: 0.8582\n",
      "Epoch 33/40 - RMSE: 0.8549\n",
      "Epoch 34/40 - RMSE: 0.8518\n",
      "Epoch 35/40 - RMSE: 0.8484\n",
      "Epoch 36/40 - RMSE: 0.8452\n",
      "Epoch 37/40 - RMSE: 0.8418\n",
      "Epoch 38/40 - RMSE: 0.8386\n",
      "Epoch 39/40 - RMSE: 0.8352\n",
      "Epoch 40/40 - RMSE: 0.8319\n",
      "(factors=50, lr=0.005, reg=0.1, ep=40) - RMSE=0.9359905315442402\n",
      "Epoch 1/50 - RMSE: 1.0461\n",
      "Epoch 2/50 - RMSE: 0.9796\n",
      "Epoch 3/50 - RMSE: 0.9553\n",
      "Epoch 4/50 - RMSE: 0.9419\n",
      "Epoch 5/50 - RMSE: 0.9328\n",
      "Epoch 6/50 - RMSE: 0.9265\n",
      "Epoch 7/50 - RMSE: 0.9215\n",
      "Epoch 8/50 - RMSE: 0.9177\n",
      "Epoch 9/50 - RMSE: 0.9143\n",
      "Epoch 10/50 - RMSE: 0.9114\n",
      "Epoch 11/50 - RMSE: 0.9092\n",
      "Epoch 12/50 - RMSE: 0.9068\n",
      "Epoch 13/50 - RMSE: 0.9047\n",
      "Epoch 14/50 - RMSE: 0.9027\n",
      "Epoch 15/50 - RMSE: 0.9009\n",
      "Epoch 16/50 - RMSE: 0.8988\n",
      "Epoch 17/50 - RMSE: 0.8970\n",
      "Epoch 18/50 - RMSE: 0.8951\n",
      "Epoch 19/50 - RMSE: 0.8930\n",
      "Epoch 20/50 - RMSE: 0.8910\n",
      "Epoch 21/50 - RMSE: 0.8888\n",
      "Epoch 22/50 - RMSE: 0.8866\n",
      "Epoch 23/50 - RMSE: 0.8843\n",
      "Epoch 24/50 - RMSE: 0.8818\n",
      "Epoch 25/50 - RMSE: 0.8793\n",
      "Epoch 26/50 - RMSE: 0.8765\n",
      "Epoch 27/50 - RMSE: 0.8737\n",
      "Epoch 28/50 - RMSE: 0.8709\n",
      "Epoch 29/50 - RMSE: 0.8680\n",
      "Epoch 30/50 - RMSE: 0.8649\n",
      "Epoch 31/50 - RMSE: 0.8619\n",
      "Epoch 32/50 - RMSE: 0.8587\n",
      "Epoch 33/50 - RMSE: 0.8556\n",
      "Epoch 34/50 - RMSE: 0.8524\n",
      "Epoch 35/50 - RMSE: 0.8492\n",
      "Epoch 36/50 - RMSE: 0.8459\n",
      "Epoch 37/50 - RMSE: 0.8426\n",
      "Epoch 38/50 - RMSE: 0.8394\n",
      "Epoch 39/50 - RMSE: 0.8363\n",
      "Epoch 40/50 - RMSE: 0.8330\n",
      "Epoch 41/50 - RMSE: 0.8296\n",
      "Epoch 42/50 - RMSE: 0.8262\n",
      "Epoch 43/50 - RMSE: 0.8232\n",
      "Epoch 44/50 - RMSE: 0.8200\n",
      "Epoch 45/50 - RMSE: 0.8167\n",
      "Epoch 46/50 - RMSE: 0.8135\n",
      "Epoch 47/50 - RMSE: 0.8103\n",
      "Epoch 48/50 - RMSE: 0.8072\n",
      "Epoch 49/50 - RMSE: 0.8040\n",
      "Epoch 50/50 - RMSE: 0.8010\n",
      "(factors=50, lr=0.005, reg=0.1, ep=50) - RMSE=0.9298950872468783\n",
      "Epoch 1/10 - RMSE: 1.0180\n",
      "Epoch 2/10 - RMSE: 0.9491\n",
      "Epoch 3/10 - RMSE: 0.9244\n",
      "Epoch 4/10 - RMSE: 0.9062\n",
      "Epoch 5/10 - RMSE: 0.8886\n",
      "Epoch 6/10 - RMSE: 0.8693\n",
      "Epoch 7/10 - RMSE: 0.8467\n",
      "Epoch 8/10 - RMSE: 0.8204\n",
      "Epoch 9/10 - RMSE: 0.7908\n",
      "Epoch 10/10 - RMSE: 0.7597\n",
      "(factors=50, lr=0.01, reg=0.001, ep=10) - RMSE=0.9588771007064618\n",
      "Epoch 1/20 - RMSE: 1.0169\n",
      "Epoch 2/20 - RMSE: 0.9481\n",
      "Epoch 3/20 - RMSE: 0.9231\n",
      "Epoch 4/20 - RMSE: 0.9041\n",
      "Epoch 5/20 - RMSE: 0.8856\n",
      "Epoch 6/20 - RMSE: 0.8647\n",
      "Epoch 7/20 - RMSE: 0.8407\n",
      "Epoch 8/20 - RMSE: 0.8132\n",
      "Epoch 9/20 - RMSE: 0.7835\n",
      "Epoch 10/20 - RMSE: 0.7529\n",
      "Epoch 11/20 - RMSE: 0.7222\n",
      "Epoch 12/20 - RMSE: 0.6926\n",
      "Epoch 13/20 - RMSE: 0.6643\n",
      "Epoch 14/20 - RMSE: 0.6377\n",
      "Epoch 15/20 - RMSE: 0.6128\n",
      "Epoch 16/20 - RMSE: 0.5899\n",
      "Epoch 17/20 - RMSE: 0.5687\n",
      "Epoch 18/20 - RMSE: 0.5491\n",
      "Epoch 19/20 - RMSE: 0.5311\n",
      "Epoch 20/20 - RMSE: 0.5144\n",
      "(factors=50, lr=0.01, reg=0.001, ep=20) - RMSE=1.0198222660840854\n",
      "Epoch 1/30 - RMSE: 1.0174\n",
      "Epoch 2/30 - RMSE: 0.9491\n",
      "Epoch 3/30 - RMSE: 0.9245\n",
      "Epoch 4/30 - RMSE: 0.9062\n",
      "Epoch 5/30 - RMSE: 0.8885\n",
      "Epoch 6/30 - RMSE: 0.8689\n",
      "Epoch 7/30 - RMSE: 0.8462\n",
      "Epoch 8/30 - RMSE: 0.8200\n",
      "Epoch 9/30 - RMSE: 0.7909\n",
      "Epoch 10/30 - RMSE: 0.7602\n",
      "Epoch 11/30 - RMSE: 0.7295\n",
      "Epoch 12/30 - RMSE: 0.6994\n",
      "Epoch 13/30 - RMSE: 0.6704\n",
      "Epoch 14/30 - RMSE: 0.6433\n",
      "Epoch 15/30 - RMSE: 0.6180\n",
      "Epoch 16/30 - RMSE: 0.5944\n",
      "Epoch 17/30 - RMSE: 0.5728\n",
      "Epoch 18/30 - RMSE: 0.5529\n",
      "Epoch 19/30 - RMSE: 0.5345\n",
      "Epoch 20/30 - RMSE: 0.5176\n",
      "Epoch 21/30 - RMSE: 0.5020\n",
      "Epoch 22/30 - RMSE: 0.4876\n",
      "Epoch 23/30 - RMSE: 0.4744\n",
      "Epoch 24/30 - RMSE: 0.4621\n",
      "Epoch 25/30 - RMSE: 0.4507\n",
      "Epoch 26/30 - RMSE: 0.4401\n",
      "Epoch 27/30 - RMSE: 0.4303\n",
      "Epoch 28/30 - RMSE: 0.4210\n",
      "Epoch 29/30 - RMSE: 0.4124\n",
      "Epoch 30/30 - RMSE: 0.4043\n",
      "(factors=50, lr=0.01, reg=0.001, ep=30) - RMSE=1.0774042824995218\n",
      "Epoch 1/40 - RMSE: 1.0178\n",
      "Epoch 2/40 - RMSE: 0.9491\n",
      "Epoch 3/40 - RMSE: 0.9245\n",
      "Epoch 4/40 - RMSE: 0.9062\n",
      "Epoch 5/40 - RMSE: 0.8885\n",
      "Epoch 6/40 - RMSE: 0.8690\n",
      "Epoch 7/40 - RMSE: 0.8461\n",
      "Epoch 8/40 - RMSE: 0.8197\n",
      "Epoch 9/40 - RMSE: 0.7902\n",
      "Epoch 10/40 - RMSE: 0.7594\n",
      "Epoch 11/40 - RMSE: 0.7284\n",
      "Epoch 12/40 - RMSE: 0.6982\n",
      "Epoch 13/40 - RMSE: 0.6694\n",
      "Epoch 14/40 - RMSE: 0.6422\n",
      "Epoch 15/40 - RMSE: 0.6170\n",
      "Epoch 16/40 - RMSE: 0.5936\n",
      "Epoch 17/40 - RMSE: 0.5719\n",
      "Epoch 18/40 - RMSE: 0.5521\n",
      "Epoch 19/40 - RMSE: 0.5338\n",
      "Epoch 20/40 - RMSE: 0.5170\n",
      "Epoch 21/40 - RMSE: 0.5015\n",
      "Epoch 22/40 - RMSE: 0.4872\n",
      "Epoch 23/40 - RMSE: 0.4740\n",
      "Epoch 24/40 - RMSE: 0.4617\n",
      "Epoch 25/40 - RMSE: 0.4505\n",
      "Epoch 26/40 - RMSE: 0.4399\n",
      "Epoch 27/40 - RMSE: 0.4301\n",
      "Epoch 28/40 - RMSE: 0.4210\n",
      "Epoch 29/40 - RMSE: 0.4124\n",
      "Epoch 30/40 - RMSE: 0.4044\n",
      "Epoch 31/40 - RMSE: 0.3968\n",
      "Epoch 32/40 - RMSE: 0.3897\n",
      "Epoch 33/40 - RMSE: 0.3831\n",
      "Epoch 34/40 - RMSE: 0.3768\n",
      "Epoch 35/40 - RMSE: 0.3708\n",
      "Epoch 36/40 - RMSE: 0.3652\n",
      "Epoch 37/40 - RMSE: 0.3599\n",
      "Epoch 38/40 - RMSE: 0.3548\n",
      "Epoch 39/40 - RMSE: 0.3500\n",
      "Epoch 40/40 - RMSE: 0.3454\n",
      "(factors=50, lr=0.01, reg=0.001, ep=40) - RMSE=1.1195555048510413\n",
      "Epoch 1/50 - RMSE: 1.0175\n",
      "Epoch 2/50 - RMSE: 0.9485\n",
      "Epoch 3/50 - RMSE: 0.9237\n",
      "Epoch 4/50 - RMSE: 0.9050\n",
      "Epoch 5/50 - RMSE: 0.8868\n",
      "Epoch 6/50 - RMSE: 0.8665\n",
      "Epoch 7/50 - RMSE: 0.8429\n",
      "Epoch 8/50 - RMSE: 0.8157\n",
      "Epoch 9/50 - RMSE: 0.7858\n",
      "Epoch 10/50 - RMSE: 0.7548\n",
      "Epoch 11/50 - RMSE: 0.7238\n",
      "Epoch 12/50 - RMSE: 0.6937\n",
      "Epoch 13/50 - RMSE: 0.6651\n",
      "Epoch 14/50 - RMSE: 0.6383\n",
      "Epoch 15/50 - RMSE: 0.6134\n",
      "Epoch 16/50 - RMSE: 0.5905\n",
      "Epoch 17/50 - RMSE: 0.5693\n",
      "Epoch 18/50 - RMSE: 0.5499\n",
      "Epoch 19/50 - RMSE: 0.5320\n",
      "Epoch 20/50 - RMSE: 0.5156\n",
      "Epoch 21/50 - RMSE: 0.5003\n",
      "Epoch 22/50 - RMSE: 0.4863\n",
      "Epoch 23/50 - RMSE: 0.4731\n",
      "Epoch 24/50 - RMSE: 0.4611\n",
      "Epoch 25/50 - RMSE: 0.4499\n",
      "Epoch 26/50 - RMSE: 0.4394\n",
      "Epoch 27/50 - RMSE: 0.4296\n",
      "Epoch 28/50 - RMSE: 0.4205\n",
      "Epoch 29/50 - RMSE: 0.4118\n",
      "Epoch 30/50 - RMSE: 0.4037\n",
      "Epoch 31/50 - RMSE: 0.3962\n",
      "Epoch 32/50 - RMSE: 0.3892\n",
      "Epoch 33/50 - RMSE: 0.3824\n",
      "Epoch 34/50 - RMSE: 0.3760\n",
      "Epoch 35/50 - RMSE: 0.3700\n",
      "Epoch 36/50 - RMSE: 0.3643\n",
      "Epoch 37/50 - RMSE: 0.3590\n",
      "Epoch 38/50 - RMSE: 0.3538\n",
      "Epoch 39/50 - RMSE: 0.3489\n",
      "Epoch 40/50 - RMSE: 0.3444\n",
      "Epoch 41/50 - RMSE: 0.3399\n",
      "Epoch 42/50 - RMSE: 0.3357\n",
      "Epoch 43/50 - RMSE: 0.3316\n",
      "Epoch 44/50 - RMSE: 0.3278\n",
      "Epoch 45/50 - RMSE: 0.3241\n",
      "Epoch 46/50 - RMSE: 0.3206\n",
      "Epoch 47/50 - RMSE: 0.3172\n",
      "Epoch 48/50 - RMSE: 0.3139\n",
      "Epoch 49/50 - RMSE: 0.3108\n",
      "Epoch 50/50 - RMSE: 0.3078\n",
      "(factors=50, lr=0.01, reg=0.001, ep=50) - RMSE=1.1588676479307212\n",
      "Epoch 1/10 - RMSE: 1.0167\n",
      "Epoch 2/10 - RMSE: 0.9484\n",
      "Epoch 3/10 - RMSE: 0.9242\n",
      "Epoch 4/10 - RMSE: 0.9066\n",
      "Epoch 5/10 - RMSE: 0.8901\n",
      "Epoch 6/10 - RMSE: 0.8722\n",
      "Epoch 7/10 - RMSE: 0.8516\n",
      "Epoch 8/10 - RMSE: 0.8282\n",
      "Epoch 9/10 - RMSE: 0.8022\n",
      "Epoch 10/10 - RMSE: 0.7746\n",
      "(factors=50, lr=0.01, reg=0.01, ep=10) - RMSE=0.9533202290534946\n",
      "Epoch 1/20 - RMSE: 1.0173\n",
      "Epoch 2/20 - RMSE: 0.9489\n",
      "Epoch 3/20 - RMSE: 0.9247\n",
      "Epoch 4/20 - RMSE: 0.9072\n",
      "Epoch 5/20 - RMSE: 0.8909\n",
      "Epoch 6/20 - RMSE: 0.8730\n",
      "Epoch 7/20 - RMSE: 0.8524\n",
      "Epoch 8/20 - RMSE: 0.8289\n",
      "Epoch 9/20 - RMSE: 0.8026\n",
      "Epoch 10/20 - RMSE: 0.7748\n",
      "Epoch 11/20 - RMSE: 0.7465\n",
      "Epoch 12/20 - RMSE: 0.7183\n",
      "Epoch 13/20 - RMSE: 0.6909\n",
      "Epoch 14/20 - RMSE: 0.6647\n",
      "Epoch 15/20 - RMSE: 0.6401\n",
      "Epoch 16/20 - RMSE: 0.6170\n",
      "Epoch 17/20 - RMSE: 0.5955\n",
      "Epoch 18/20 - RMSE: 0.5756\n",
      "Epoch 19/20 - RMSE: 0.5572\n",
      "Epoch 20/20 - RMSE: 0.5402\n",
      "(factors=50, lr=0.01, reg=0.01, ep=20) - RMSE=0.9851846503740704\n",
      "Epoch 1/30 - RMSE: 1.0182\n",
      "Epoch 2/30 - RMSE: 0.9492\n",
      "Epoch 3/30 - RMSE: 0.9248\n",
      "Epoch 4/30 - RMSE: 0.9075\n",
      "Epoch 5/30 - RMSE: 0.8911\n",
      "Epoch 6/30 - RMSE: 0.8736\n",
      "Epoch 7/30 - RMSE: 0.8531\n",
      "Epoch 8/30 - RMSE: 0.8297\n",
      "Epoch 9/30 - RMSE: 0.8035\n",
      "Epoch 10/30 - RMSE: 0.7755\n",
      "Epoch 11/30 - RMSE: 0.7470\n",
      "Epoch 12/30 - RMSE: 0.7189\n",
      "Epoch 13/30 - RMSE: 0.6917\n",
      "Epoch 14/30 - RMSE: 0.6657\n",
      "Epoch 15/30 - RMSE: 0.6412\n",
      "Epoch 16/30 - RMSE: 0.6184\n",
      "Epoch 17/30 - RMSE: 0.5970\n",
      "Epoch 18/30 - RMSE: 0.5772\n",
      "Epoch 19/30 - RMSE: 0.5589\n",
      "Epoch 20/30 - RMSE: 0.5418\n",
      "Epoch 21/30 - RMSE: 0.5261\n",
      "Epoch 22/30 - RMSE: 0.5115\n",
      "Epoch 23/30 - RMSE: 0.4980\n",
      "Epoch 24/30 - RMSE: 0.4855\n",
      "Epoch 25/30 - RMSE: 0.4738\n",
      "Epoch 26/30 - RMSE: 0.4630\n",
      "Epoch 27/30 - RMSE: 0.4529\n",
      "Epoch 28/30 - RMSE: 0.4435\n",
      "Epoch 29/30 - RMSE: 0.4347\n",
      "Epoch 30/30 - RMSE: 0.4265\n",
      "(factors=50, lr=0.01, reg=0.01, ep=30) - RMSE=1.0262470719672863\n",
      "Epoch 1/40 - RMSE: 1.0170\n",
      "Epoch 2/40 - RMSE: 0.9486\n",
      "Epoch 3/40 - RMSE: 0.9243\n",
      "Epoch 4/40 - RMSE: 0.9069\n",
      "Epoch 5/40 - RMSE: 0.8903\n",
      "Epoch 6/40 - RMSE: 0.8725\n",
      "Epoch 7/40 - RMSE: 0.8520\n",
      "Epoch 8/40 - RMSE: 0.8282\n",
      "Epoch 9/40 - RMSE: 0.8022\n",
      "Epoch 10/40 - RMSE: 0.7744\n",
      "Epoch 11/40 - RMSE: 0.7461\n",
      "Epoch 12/40 - RMSE: 0.7180\n",
      "Epoch 13/40 - RMSE: 0.6907\n",
      "Epoch 14/40 - RMSE: 0.6645\n",
      "Epoch 15/40 - RMSE: 0.6396\n",
      "Epoch 16/40 - RMSE: 0.6165\n",
      "Epoch 17/40 - RMSE: 0.5949\n",
      "Epoch 18/40 - RMSE: 0.5749\n",
      "Epoch 19/40 - RMSE: 0.5564\n",
      "Epoch 20/40 - RMSE: 0.5395\n",
      "Epoch 21/40 - RMSE: 0.5238\n",
      "Epoch 22/40 - RMSE: 0.5093\n",
      "Epoch 23/40 - RMSE: 0.4958\n",
      "Epoch 24/40 - RMSE: 0.4834\n",
      "Epoch 25/40 - RMSE: 0.4719\n",
      "Epoch 26/40 - RMSE: 0.4612\n",
      "Epoch 27/40 - RMSE: 0.4513\n",
      "Epoch 28/40 - RMSE: 0.4420\n",
      "Epoch 29/40 - RMSE: 0.4332\n",
      "Epoch 30/40 - RMSE: 0.4251\n",
      "Epoch 31/40 - RMSE: 0.4176\n",
      "Epoch 32/40 - RMSE: 0.4105\n",
      "Epoch 33/40 - RMSE: 0.4038\n",
      "Epoch 34/40 - RMSE: 0.3974\n",
      "Epoch 35/40 - RMSE: 0.3915\n",
      "Epoch 36/40 - RMSE: 0.3859\n",
      "Epoch 37/40 - RMSE: 0.3807\n",
      "Epoch 38/40 - RMSE: 0.3756\n",
      "Epoch 39/40 - RMSE: 0.3709\n",
      "Epoch 40/40 - RMSE: 0.3664\n",
      "(factors=50, lr=0.01, reg=0.01, ep=40) - RMSE=1.0588756970524917\n",
      "Epoch 1/50 - RMSE: 1.0169\n",
      "Epoch 2/50 - RMSE: 0.9487\n",
      "Epoch 3/50 - RMSE: 0.9245\n",
      "Epoch 4/50 - RMSE: 0.9071\n",
      "Epoch 5/50 - RMSE: 0.8908\n",
      "Epoch 6/50 - RMSE: 0.8734\n",
      "Epoch 7/50 - RMSE: 0.8533\n",
      "Epoch 8/50 - RMSE: 0.8304\n",
      "Epoch 9/50 - RMSE: 0.8048\n",
      "Epoch 10/50 - RMSE: 0.7775\n",
      "Epoch 11/50 - RMSE: 0.7495\n",
      "Epoch 12/50 - RMSE: 0.7217\n",
      "Epoch 13/50 - RMSE: 0.6946\n",
      "Epoch 14/50 - RMSE: 0.6685\n",
      "Epoch 15/50 - RMSE: 0.6438\n",
      "Epoch 16/50 - RMSE: 0.6207\n",
      "Epoch 17/50 - RMSE: 0.5991\n",
      "Epoch 18/50 - RMSE: 0.5790\n",
      "Epoch 19/50 - RMSE: 0.5605\n",
      "Epoch 20/50 - RMSE: 0.5433\n",
      "Epoch 21/50 - RMSE: 0.5274\n",
      "Epoch 22/50 - RMSE: 0.5128\n",
      "Epoch 23/50 - RMSE: 0.4992\n",
      "Epoch 24/50 - RMSE: 0.4867\n",
      "Epoch 25/50 - RMSE: 0.4751\n",
      "Epoch 26/50 - RMSE: 0.4642\n",
      "Epoch 27/50 - RMSE: 0.4541\n",
      "Epoch 28/50 - RMSE: 0.4447\n",
      "Epoch 29/50 - RMSE: 0.4358\n",
      "Epoch 30/50 - RMSE: 0.4276\n",
      "Epoch 31/50 - RMSE: 0.4199\n",
      "Epoch 32/50 - RMSE: 0.4126\n",
      "Epoch 33/50 - RMSE: 0.4059\n",
      "Epoch 34/50 - RMSE: 0.3995\n",
      "Epoch 35/50 - RMSE: 0.3934\n",
      "Epoch 36/50 - RMSE: 0.3876\n",
      "Epoch 37/50 - RMSE: 0.3822\n",
      "Epoch 38/50 - RMSE: 0.3771\n",
      "Epoch 39/50 - RMSE: 0.3722\n",
      "Epoch 40/50 - RMSE: 0.3676\n",
      "Epoch 41/50 - RMSE: 0.3632\n",
      "Epoch 42/50 - RMSE: 0.3590\n",
      "Epoch 43/50 - RMSE: 0.3550\n",
      "Epoch 44/50 - RMSE: 0.3512\n",
      "Epoch 45/50 - RMSE: 0.3476\n",
      "Epoch 46/50 - RMSE: 0.3442\n",
      "Epoch 47/50 - RMSE: 0.3409\n",
      "Epoch 48/50 - RMSE: 0.3376\n",
      "Epoch 49/50 - RMSE: 0.3346\n",
      "Epoch 50/50 - RMSE: 0.3317\n",
      "(factors=50, lr=0.01, reg=0.01, ep=50) - RMSE=1.0875730261230554\n",
      "Epoch 1/10 - RMSE: 1.0186\n",
      "Epoch 2/10 - RMSE: 0.9532\n",
      "Epoch 3/10 - RMSE: 0.9341\n",
      "Epoch 4/10 - RMSE: 0.9239\n",
      "Epoch 5/10 - RMSE: 0.9171\n",
      "Epoch 6/10 - RMSE: 0.9119\n",
      "Epoch 7/10 - RMSE: 0.9077\n",
      "Epoch 8/10 - RMSE: 0.9035\n",
      "Epoch 9/10 - RMSE: 0.8997\n",
      "Epoch 10/10 - RMSE: 0.8955\n",
      "(factors=50, lr=0.01, reg=0.1, ep=10) - RMSE=0.9544230327122674\n",
      "Epoch 1/20 - RMSE: 1.0189\n",
      "Epoch 2/20 - RMSE: 0.9533\n",
      "Epoch 3/20 - RMSE: 0.9342\n",
      "Epoch 4/20 - RMSE: 0.9238\n",
      "Epoch 5/20 - RMSE: 0.9172\n",
      "Epoch 6/20 - RMSE: 0.9120\n",
      "Epoch 7/20 - RMSE: 0.9077\n",
      "Epoch 8/20 - RMSE: 0.9039\n",
      "Epoch 9/20 - RMSE: 0.9001\n",
      "Epoch 10/20 - RMSE: 0.8960\n",
      "Epoch 11/20 - RMSE: 0.8915\n",
      "Epoch 12/20 - RMSE: 0.8867\n",
      "Epoch 13/20 - RMSE: 0.8817\n",
      "Epoch 14/20 - RMSE: 0.8762\n",
      "Epoch 15/20 - RMSE: 0.8703\n",
      "Epoch 16/20 - RMSE: 0.8640\n",
      "Epoch 17/20 - RMSE: 0.8578\n",
      "Epoch 18/20 - RMSE: 0.8515\n",
      "Epoch 19/20 - RMSE: 0.8449\n",
      "Epoch 20/20 - RMSE: 0.8385\n",
      "(factors=50, lr=0.01, reg=0.1, ep=20) - RMSE=0.9349191029768051\n",
      "Epoch 1/30 - RMSE: 1.0187\n",
      "Epoch 2/30 - RMSE: 0.9534\n",
      "Epoch 3/30 - RMSE: 0.9341\n",
      "Epoch 4/30 - RMSE: 0.9238\n",
      "Epoch 5/30 - RMSE: 0.9172\n",
      "Epoch 6/30 - RMSE: 0.9119\n",
      "Epoch 7/30 - RMSE: 0.9078\n",
      "Epoch 8/30 - RMSE: 0.9037\n",
      "Epoch 9/30 - RMSE: 0.8996\n",
      "Epoch 10/30 - RMSE: 0.8956\n",
      "Epoch 11/30 - RMSE: 0.8911\n",
      "Epoch 12/30 - RMSE: 0.8864\n",
      "Epoch 13/30 - RMSE: 0.8809\n",
      "Epoch 14/30 - RMSE: 0.8753\n",
      "Epoch 15/30 - RMSE: 0.8690\n",
      "Epoch 16/30 - RMSE: 0.8630\n",
      "Epoch 17/30 - RMSE: 0.8565\n",
      "Epoch 18/30 - RMSE: 0.8499\n",
      "Epoch 19/30 - RMSE: 0.8432\n",
      "Epoch 20/30 - RMSE: 0.8369\n",
      "Epoch 21/30 - RMSE: 0.8304\n",
      "Epoch 22/30 - RMSE: 0.8237\n",
      "Epoch 23/30 - RMSE: 0.8172\n",
      "Epoch 24/30 - RMSE: 0.8107\n",
      "Epoch 25/30 - RMSE: 0.8046\n",
      "Epoch 26/30 - RMSE: 0.7983\n",
      "Epoch 27/30 - RMSE: 0.7920\n",
      "Epoch 28/30 - RMSE: 0.7861\n",
      "Epoch 29/30 - RMSE: 0.7804\n",
      "Epoch 30/30 - RMSE: 0.7745\n",
      "(factors=50, lr=0.01, reg=0.1, ep=30) - RMSE=0.9255448192415433\n",
      "Epoch 1/40 - RMSE: 1.0184\n",
      "Epoch 2/40 - RMSE: 0.9531\n",
      "Epoch 3/40 - RMSE: 0.9342\n",
      "Epoch 4/40 - RMSE: 0.9238\n",
      "Epoch 5/40 - RMSE: 0.9171\n",
      "Epoch 6/40 - RMSE: 0.9122\n",
      "Epoch 7/40 - RMSE: 0.9077\n",
      "Epoch 8/40 - RMSE: 0.9040\n",
      "Epoch 9/40 - RMSE: 0.9002\n",
      "Epoch 10/40 - RMSE: 0.8959\n",
      "Epoch 11/40 - RMSE: 0.8916\n",
      "Epoch 12/40 - RMSE: 0.8867\n",
      "Epoch 13/40 - RMSE: 0.8813\n",
      "Epoch 14/40 - RMSE: 0.8760\n",
      "Epoch 15/40 - RMSE: 0.8699\n",
      "Epoch 16/40 - RMSE: 0.8636\n",
      "Epoch 17/40 - RMSE: 0.8573\n",
      "Epoch 18/40 - RMSE: 0.8510\n",
      "Epoch 19/40 - RMSE: 0.8446\n",
      "Epoch 20/40 - RMSE: 0.8383\n",
      "Epoch 21/40 - RMSE: 0.8315\n",
      "Epoch 22/40 - RMSE: 0.8254\n",
      "Epoch 23/40 - RMSE: 0.8188\n",
      "Epoch 24/40 - RMSE: 0.8124\n",
      "Epoch 25/40 - RMSE: 0.8063\n",
      "Epoch 26/40 - RMSE: 0.8000\n",
      "Epoch 27/40 - RMSE: 0.7939\n",
      "Epoch 28/40 - RMSE: 0.7879\n",
      "Epoch 29/40 - RMSE: 0.7822\n",
      "Epoch 30/40 - RMSE: 0.7762\n",
      "Epoch 31/40 - RMSE: 0.7710\n",
      "Epoch 32/40 - RMSE: 0.7654\n",
      "Epoch 33/40 - RMSE: 0.7603\n",
      "Epoch 34/40 - RMSE: 0.7555\n",
      "Epoch 35/40 - RMSE: 0.7506\n",
      "Epoch 36/40 - RMSE: 0.7462\n",
      "Epoch 37/40 - RMSE: 0.7417\n",
      "Epoch 38/40 - RMSE: 0.7375\n",
      "Epoch 39/40 - RMSE: 0.7333\n",
      "Epoch 40/40 - RMSE: 0.7295\n",
      "(factors=50, lr=0.01, reg=0.1, ep=40) - RMSE=0.9245627593731306\n",
      "Epoch 1/50 - RMSE: 1.0188\n",
      "Epoch 2/50 - RMSE: 0.9532\n",
      "Epoch 3/50 - RMSE: 0.9340\n",
      "Epoch 4/50 - RMSE: 0.9237\n",
      "Epoch 5/50 - RMSE: 0.9170\n",
      "Epoch 6/50 - RMSE: 0.9116\n",
      "Epoch 7/50 - RMSE: 0.9074\n",
      "Epoch 8/50 - RMSE: 0.9033\n",
      "Epoch 9/50 - RMSE: 0.8993\n",
      "Epoch 10/50 - RMSE: 0.8950\n",
      "Epoch 11/50 - RMSE: 0.8906\n",
      "Epoch 12/50 - RMSE: 0.8854\n",
      "Epoch 13/50 - RMSE: 0.8800\n",
      "Epoch 14/50 - RMSE: 0.8743\n",
      "Epoch 15/50 - RMSE: 0.8683\n",
      "Epoch 16/50 - RMSE: 0.8619\n",
      "Epoch 17/50 - RMSE: 0.8557\n",
      "Epoch 18/50 - RMSE: 0.8489\n",
      "Epoch 19/50 - RMSE: 0.8427\n",
      "Epoch 20/50 - RMSE: 0.8362\n",
      "Epoch 21/50 - RMSE: 0.8298\n",
      "Epoch 22/50 - RMSE: 0.8234\n",
      "Epoch 23/50 - RMSE: 0.8168\n",
      "Epoch 24/50 - RMSE: 0.8109\n",
      "Epoch 25/50 - RMSE: 0.8045\n",
      "Epoch 26/50 - RMSE: 0.7986\n",
      "Epoch 27/50 - RMSE: 0.7925\n",
      "Epoch 28/50 - RMSE: 0.7865\n",
      "Epoch 29/50 - RMSE: 0.7810\n",
      "Epoch 30/50 - RMSE: 0.7753\n",
      "Epoch 31/50 - RMSE: 0.7698\n",
      "Epoch 32/50 - RMSE: 0.7645\n",
      "Epoch 33/50 - RMSE: 0.7595\n",
      "Epoch 34/50 - RMSE: 0.7544\n",
      "Epoch 35/50 - RMSE: 0.7499\n",
      "Epoch 36/50 - RMSE: 0.7454\n",
      "Epoch 37/50 - RMSE: 0.7410\n",
      "Epoch 38/50 - RMSE: 0.7367\n",
      "Epoch 39/50 - RMSE: 0.7328\n",
      "Epoch 40/50 - RMSE: 0.7290\n",
      "Epoch 41/50 - RMSE: 0.7253\n",
      "Epoch 42/50 - RMSE: 0.7218\n",
      "Epoch 43/50 - RMSE: 0.7185\n",
      "Epoch 44/50 - RMSE: 0.7152\n",
      "Epoch 45/50 - RMSE: 0.7120\n",
      "Epoch 46/50 - RMSE: 0.7091\n",
      "Epoch 47/50 - RMSE: 0.7059\n",
      "Epoch 48/50 - RMSE: 0.7034\n",
      "Epoch 49/50 - RMSE: 0.7009\n",
      "Epoch 50/50 - RMSE: 0.6984\n",
      "(factors=50, lr=0.01, reg=0.1, ep=50) - RMSE=0.9242151829475271\n",
      "Epoch 1/10 - RMSE: nan\n",
      "Epoch 2/10 - RMSE: nan\n",
      "Epoch 3/10 - RMSE: nan\n",
      "Epoch 4/10 - RMSE: nan\n",
      "Epoch 5/10 - RMSE: nan\n",
      "Epoch 6/10 - RMSE: nan\n",
      "Epoch 7/10 - RMSE: nan\n",
      "Epoch 8/10 - RMSE: nan\n",
      "Epoch 9/10 - RMSE: nan\n",
      "Epoch 10/10 - RMSE: nan\n",
      "(factors=50, lr=0.2, reg=0.001, ep=10) - RMSE=nan\n",
      "Epoch 1/20 - RMSE: nan\n",
      "Epoch 2/20 - RMSE: nan\n",
      "Epoch 3/20 - RMSE: nan\n",
      "Epoch 4/20 - RMSE: nan\n",
      "Epoch 5/20 - RMSE: nan\n",
      "Epoch 6/20 - RMSE: nan\n",
      "Epoch 7/20 - RMSE: nan\n",
      "Epoch 8/20 - RMSE: nan\n",
      "Epoch 9/20 - RMSE: nan\n",
      "Epoch 10/20 - RMSE: nan\n",
      "Epoch 11/20 - RMSE: nan\n",
      "Epoch 12/20 - RMSE: nan\n",
      "Epoch 13/20 - RMSE: nan\n",
      "Epoch 14/20 - RMSE: nan\n",
      "Epoch 15/20 - RMSE: nan\n",
      "Epoch 16/20 - RMSE: nan\n",
      "Epoch 17/20 - RMSE: nan\n",
      "Epoch 18/20 - RMSE: nan\n",
      "Epoch 19/20 - RMSE: nan\n",
      "Epoch 20/20 - RMSE: nan\n",
      "(factors=50, lr=0.2, reg=0.001, ep=20) - RMSE=nan\n",
      "Epoch 1/30 - RMSE: nan\n",
      "Epoch 2/30 - RMSE: nan\n",
      "Epoch 3/30 - RMSE: nan\n",
      "Epoch 4/30 - RMSE: nan\n",
      "Epoch 5/30 - RMSE: nan\n",
      "Epoch 6/30 - RMSE: nan\n",
      "Epoch 7/30 - RMSE: nan\n",
      "Epoch 8/30 - RMSE: nan\n",
      "Epoch 9/30 - RMSE: nan\n",
      "Epoch 10/30 - RMSE: nan\n",
      "Epoch 11/30 - RMSE: nan\n",
      "Epoch 12/30 - RMSE: nan\n",
      "Epoch 13/30 - RMSE: nan\n",
      "Epoch 14/30 - RMSE: nan\n",
      "Epoch 15/30 - RMSE: nan\n",
      "Epoch 16/30 - RMSE: nan\n",
      "Epoch 17/30 - RMSE: nan\n",
      "Epoch 18/30 - RMSE: nan\n",
      "Epoch 19/30 - RMSE: nan\n",
      "Epoch 20/30 - RMSE: nan\n",
      "Epoch 21/30 - RMSE: nan\n",
      "Epoch 22/30 - RMSE: nan\n",
      "Epoch 23/30 - RMSE: nan\n",
      "Epoch 24/30 - RMSE: nan\n",
      "Epoch 25/30 - RMSE: nan\n",
      "Epoch 26/30 - RMSE: nan\n",
      "Epoch 27/30 - RMSE: nan\n",
      "Epoch 28/30 - RMSE: nan\n",
      "Epoch 29/30 - RMSE: nan\n",
      "Epoch 30/30 - RMSE: nan\n",
      "(factors=50, lr=0.2, reg=0.001, ep=30) - RMSE=nan\n",
      "Epoch 1/40 - RMSE: nan\n",
      "Epoch 2/40 - RMSE: nan\n",
      "Epoch 3/40 - RMSE: nan\n",
      "Epoch 4/40 - RMSE: nan\n",
      "Epoch 5/40 - RMSE: nan\n",
      "Epoch 6/40 - RMSE: nan\n",
      "Epoch 7/40 - RMSE: nan\n",
      "Epoch 8/40 - RMSE: nan\n",
      "Epoch 9/40 - RMSE: nan\n",
      "Epoch 10/40 - RMSE: nan\n",
      "Epoch 11/40 - RMSE: nan\n",
      "Epoch 12/40 - RMSE: nan\n",
      "Epoch 13/40 - RMSE: nan\n",
      "Epoch 14/40 - RMSE: nan\n",
      "Epoch 15/40 - RMSE: nan\n",
      "Epoch 16/40 - RMSE: nan\n",
      "Epoch 17/40 - RMSE: nan\n",
      "Epoch 18/40 - RMSE: nan\n",
      "Epoch 19/40 - RMSE: nan\n",
      "Epoch 20/40 - RMSE: nan\n",
      "Epoch 21/40 - RMSE: nan\n",
      "Epoch 22/40 - RMSE: nan\n",
      "Epoch 23/40 - RMSE: nan\n",
      "Epoch 24/40 - RMSE: nan\n",
      "Epoch 25/40 - RMSE: nan\n",
      "Epoch 26/40 - RMSE: nan\n",
      "Epoch 27/40 - RMSE: nan\n",
      "Epoch 28/40 - RMSE: nan\n",
      "Epoch 29/40 - RMSE: nan\n",
      "Epoch 30/40 - RMSE: nan\n",
      "Epoch 31/40 - RMSE: nan\n",
      "Epoch 32/40 - RMSE: nan\n",
      "Epoch 33/40 - RMSE: nan\n",
      "Epoch 34/40 - RMSE: nan\n",
      "Epoch 35/40 - RMSE: nan\n",
      "Epoch 36/40 - RMSE: nan\n",
      "Epoch 37/40 - RMSE: nan\n",
      "Epoch 38/40 - RMSE: nan\n",
      "Epoch 39/40 - RMSE: nan\n",
      "Epoch 40/40 - RMSE: nan\n",
      "(factors=50, lr=0.2, reg=0.001, ep=40) - RMSE=nan\n",
      "Epoch 1/50 - RMSE: nan\n",
      "Epoch 2/50 - RMSE: nan\n",
      "Epoch 3/50 - RMSE: nan\n",
      "Epoch 4/50 - RMSE: nan\n",
      "Epoch 5/50 - RMSE: nan\n",
      "Epoch 6/50 - RMSE: nan\n",
      "Epoch 7/50 - RMSE: nan\n",
      "Epoch 8/50 - RMSE: nan\n",
      "Epoch 9/50 - RMSE: nan\n",
      "Epoch 10/50 - RMSE: nan\n",
      "Epoch 11/50 - RMSE: nan\n",
      "Epoch 12/50 - RMSE: nan\n",
      "Epoch 13/50 - RMSE: nan\n",
      "Epoch 14/50 - RMSE: nan\n",
      "Epoch 15/50 - RMSE: nan\n",
      "Epoch 16/50 - RMSE: nan\n",
      "Epoch 17/50 - RMSE: nan\n",
      "Epoch 18/50 - RMSE: nan\n",
      "Epoch 19/50 - RMSE: nan\n",
      "Epoch 20/50 - RMSE: nan\n",
      "Epoch 21/50 - RMSE: nan\n",
      "Epoch 22/50 - RMSE: nan\n",
      "Epoch 23/50 - RMSE: nan\n",
      "Epoch 24/50 - RMSE: nan\n",
      "Epoch 25/50 - RMSE: nan\n",
      "Epoch 26/50 - RMSE: nan\n",
      "Epoch 27/50 - RMSE: nan\n",
      "Epoch 28/50 - RMSE: nan\n",
      "Epoch 29/50 - RMSE: nan\n",
      "Epoch 30/50 - RMSE: nan\n",
      "Epoch 31/50 - RMSE: nan\n",
      "Epoch 32/50 - RMSE: nan\n",
      "Epoch 33/50 - RMSE: nan\n",
      "Epoch 34/50 - RMSE: nan\n",
      "Epoch 35/50 - RMSE: nan\n",
      "Epoch 36/50 - RMSE: nan\n",
      "Epoch 37/50 - RMSE: nan\n",
      "Epoch 38/50 - RMSE: nan\n",
      "Epoch 39/50 - RMSE: nan\n",
      "Epoch 40/50 - RMSE: nan\n",
      "Epoch 41/50 - RMSE: nan\n",
      "Epoch 42/50 - RMSE: nan\n",
      "Epoch 43/50 - RMSE: nan\n",
      "Epoch 44/50 - RMSE: nan\n",
      "Epoch 45/50 - RMSE: nan\n",
      "Epoch 46/50 - RMSE: nan\n",
      "Epoch 47/50 - RMSE: nan\n",
      "Epoch 48/50 - RMSE: nan\n",
      "Epoch 49/50 - RMSE: nan\n",
      "Epoch 50/50 - RMSE: nan\n",
      "(factors=50, lr=0.2, reg=0.001, ep=50) - RMSE=nan\n",
      "Epoch 1/10 - RMSE: nan\n",
      "Epoch 2/10 - RMSE: nan\n",
      "Epoch 3/10 - RMSE: nan\n",
      "Epoch 4/10 - RMSE: nan\n",
      "Epoch 5/10 - RMSE: nan\n",
      "Epoch 6/10 - RMSE: nan\n",
      "Epoch 7/10 - RMSE: nan\n",
      "Epoch 8/10 - RMSE: nan\n",
      "Epoch 9/10 - RMSE: nan\n",
      "Epoch 10/10 - RMSE: nan\n",
      "(factors=50, lr=0.2, reg=0.01, ep=10) - RMSE=nan\n",
      "Epoch 1/20 - RMSE: nan\n",
      "Epoch 2/20 - RMSE: nan\n",
      "Epoch 3/20 - RMSE: nan\n",
      "Epoch 4/20 - RMSE: nan\n",
      "Epoch 5/20 - RMSE: nan\n",
      "Epoch 6/20 - RMSE: nan\n",
      "Epoch 7/20 - RMSE: nan\n",
      "Epoch 8/20 - RMSE: nan\n",
      "Epoch 9/20 - RMSE: nan\n",
      "Epoch 10/20 - RMSE: nan\n",
      "Epoch 11/20 - RMSE: nan\n",
      "Epoch 12/20 - RMSE: nan\n",
      "Epoch 13/20 - RMSE: nan\n",
      "Epoch 14/20 - RMSE: nan\n",
      "Epoch 15/20 - RMSE: nan\n",
      "Epoch 16/20 - RMSE: nan\n",
      "Epoch 17/20 - RMSE: nan\n",
      "Epoch 18/20 - RMSE: nan\n",
      "Epoch 19/20 - RMSE: nan\n",
      "Epoch 20/20 - RMSE: nan\n",
      "(factors=50, lr=0.2, reg=0.01, ep=20) - RMSE=nan\n",
      "Epoch 1/30 - RMSE: nan\n",
      "Epoch 2/30 - RMSE: nan\n",
      "Epoch 3/30 - RMSE: nan\n",
      "Epoch 4/30 - RMSE: nan\n",
      "Epoch 5/30 - RMSE: nan\n",
      "Epoch 6/30 - RMSE: nan\n",
      "Epoch 7/30 - RMSE: nan\n",
      "Epoch 8/30 - RMSE: nan\n",
      "Epoch 9/30 - RMSE: nan\n",
      "Epoch 10/30 - RMSE: nan\n",
      "Epoch 11/30 - RMSE: nan\n",
      "Epoch 12/30 - RMSE: nan\n",
      "Epoch 13/30 - RMSE: nan\n",
      "Epoch 14/30 - RMSE: nan\n",
      "Epoch 15/30 - RMSE: nan\n",
      "Epoch 16/30 - RMSE: nan\n",
      "Epoch 17/30 - RMSE: nan\n",
      "Epoch 18/30 - RMSE: nan\n",
      "Epoch 19/30 - RMSE: nan\n",
      "Epoch 20/30 - RMSE: nan\n",
      "Epoch 21/30 - RMSE: nan\n",
      "Epoch 22/30 - RMSE: nan\n",
      "Epoch 23/30 - RMSE: nan\n",
      "Epoch 24/30 - RMSE: nan\n",
      "Epoch 25/30 - RMSE: nan\n",
      "Epoch 26/30 - RMSE: nan\n",
      "Epoch 27/30 - RMSE: nan\n",
      "Epoch 28/30 - RMSE: nan\n",
      "Epoch 29/30 - RMSE: nan\n",
      "Epoch 30/30 - RMSE: nan\n",
      "(factors=50, lr=0.2, reg=0.01, ep=30) - RMSE=nan\n",
      "Epoch 1/40 - RMSE: nan\n",
      "Epoch 2/40 - RMSE: nan\n",
      "Epoch 3/40 - RMSE: nan\n",
      "Epoch 4/40 - RMSE: nan\n",
      "Epoch 5/40 - RMSE: nan\n",
      "Epoch 6/40 - RMSE: nan\n",
      "Epoch 7/40 - RMSE: nan\n",
      "Epoch 8/40 - RMSE: nan\n",
      "Epoch 9/40 - RMSE: nan\n",
      "Epoch 10/40 - RMSE: nan\n",
      "Epoch 11/40 - RMSE: nan\n",
      "Epoch 12/40 - RMSE: nan\n",
      "Epoch 13/40 - RMSE: nan\n",
      "Epoch 14/40 - RMSE: nan\n",
      "Epoch 15/40 - RMSE: nan\n",
      "Epoch 16/40 - RMSE: nan\n",
      "Epoch 17/40 - RMSE: nan\n",
      "Epoch 18/40 - RMSE: nan\n",
      "Epoch 19/40 - RMSE: nan\n",
      "Epoch 20/40 - RMSE: nan\n",
      "Epoch 21/40 - RMSE: nan\n",
      "Epoch 22/40 - RMSE: nan\n",
      "Epoch 23/40 - RMSE: nan\n",
      "Epoch 24/40 - RMSE: nan\n",
      "Epoch 25/40 - RMSE: nan\n",
      "Epoch 26/40 - RMSE: nan\n",
      "Epoch 27/40 - RMSE: nan\n",
      "Epoch 28/40 - RMSE: nan\n",
      "Epoch 29/40 - RMSE: nan\n",
      "Epoch 30/40 - RMSE: nan\n",
      "Epoch 31/40 - RMSE: nan\n",
      "Epoch 32/40 - RMSE: nan\n",
      "Epoch 33/40 - RMSE: nan\n",
      "Epoch 34/40 - RMSE: nan\n",
      "Epoch 35/40 - RMSE: nan\n",
      "Epoch 36/40 - RMSE: nan\n",
      "Epoch 37/40 - RMSE: nan\n",
      "Epoch 38/40 - RMSE: nan\n",
      "Epoch 39/40 - RMSE: nan\n",
      "Epoch 40/40 - RMSE: nan\n",
      "(factors=50, lr=0.2, reg=0.01, ep=40) - RMSE=nan\n",
      "Epoch 1/50 - RMSE: nan\n",
      "Epoch 2/50 - RMSE: nan\n",
      "Epoch 3/50 - RMSE: nan\n",
      "Epoch 4/50 - RMSE: nan\n",
      "Epoch 5/50 - RMSE: nan\n",
      "Epoch 6/50 - RMSE: nan\n",
      "Epoch 7/50 - RMSE: nan\n",
      "Epoch 8/50 - RMSE: nan\n",
      "Epoch 9/50 - RMSE: nan\n",
      "Epoch 10/50 - RMSE: nan\n",
      "Epoch 11/50 - RMSE: nan\n",
      "Epoch 12/50 - RMSE: nan\n",
      "Epoch 13/50 - RMSE: nan\n",
      "Epoch 14/50 - RMSE: nan\n",
      "Epoch 15/50 - RMSE: nan\n",
      "Epoch 16/50 - RMSE: nan\n",
      "Epoch 17/50 - RMSE: nan\n",
      "Epoch 18/50 - RMSE: nan\n",
      "Epoch 19/50 - RMSE: nan\n",
      "Epoch 20/50 - RMSE: nan\n",
      "Epoch 21/50 - RMSE: nan\n",
      "Epoch 22/50 - RMSE: nan\n",
      "Epoch 23/50 - RMSE: nan\n",
      "Epoch 24/50 - RMSE: nan\n",
      "Epoch 25/50 - RMSE: nan\n",
      "Epoch 26/50 - RMSE: nan\n",
      "Epoch 27/50 - RMSE: nan\n",
      "Epoch 28/50 - RMSE: nan\n",
      "Epoch 29/50 - RMSE: nan\n",
      "Epoch 30/50 - RMSE: nan\n",
      "Epoch 31/50 - RMSE: nan\n",
      "Epoch 32/50 - RMSE: nan\n",
      "Epoch 33/50 - RMSE: nan\n",
      "Epoch 34/50 - RMSE: nan\n",
      "Epoch 35/50 - RMSE: nan\n",
      "Epoch 36/50 - RMSE: nan\n",
      "Epoch 37/50 - RMSE: nan\n",
      "Epoch 38/50 - RMSE: nan\n",
      "Epoch 39/50 - RMSE: nan\n",
      "Epoch 40/50 - RMSE: nan\n",
      "Epoch 41/50 - RMSE: nan\n",
      "Epoch 42/50 - RMSE: nan\n",
      "Epoch 43/50 - RMSE: nan\n",
      "Epoch 44/50 - RMSE: nan\n",
      "Epoch 45/50 - RMSE: nan\n",
      "Epoch 46/50 - RMSE: nan\n",
      "Epoch 47/50 - RMSE: nan\n",
      "Epoch 48/50 - RMSE: nan\n",
      "Epoch 49/50 - RMSE: nan\n",
      "Epoch 50/50 - RMSE: nan\n",
      "(factors=50, lr=0.2, reg=0.01, ep=50) - RMSE=nan\n",
      "Epoch 1/10 - RMSE: nan\n",
      "Epoch 2/10 - RMSE: nan\n",
      "Epoch 3/10 - RMSE: nan\n",
      "Epoch 4/10 - RMSE: nan\n",
      "Epoch 5/10 - RMSE: nan\n",
      "Epoch 6/10 - RMSE: nan\n",
      "Epoch 7/10 - RMSE: nan\n",
      "Epoch 8/10 - RMSE: nan\n",
      "Epoch 9/10 - RMSE: nan\n",
      "Epoch 10/10 - RMSE: nan\n",
      "(factors=50, lr=0.2, reg=0.1, ep=10) - RMSE=nan\n",
      "Epoch 1/20 - RMSE: nan\n",
      "Epoch 2/20 - RMSE: nan\n",
      "Epoch 3/20 - RMSE: nan\n",
      "Epoch 4/20 - RMSE: nan\n",
      "Epoch 5/20 - RMSE: nan\n",
      "Epoch 6/20 - RMSE: nan\n",
      "Epoch 7/20 - RMSE: nan\n",
      "Epoch 8/20 - RMSE: nan\n",
      "Epoch 9/20 - RMSE: nan\n",
      "Epoch 10/20 - RMSE: nan\n",
      "Epoch 11/20 - RMSE: nan\n",
      "Epoch 12/20 - RMSE: nan\n",
      "Epoch 13/20 - RMSE: nan\n",
      "Epoch 14/20 - RMSE: nan\n",
      "Epoch 15/20 - RMSE: nan\n",
      "Epoch 16/20 - RMSE: nan\n",
      "Epoch 17/20 - RMSE: nan\n",
      "Epoch 18/20 - RMSE: nan\n",
      "Epoch 19/20 - RMSE: nan\n",
      "Epoch 20/20 - RMSE: nan\n",
      "(factors=50, lr=0.2, reg=0.1, ep=20) - RMSE=nan\n",
      "Epoch 1/30 - RMSE: nan\n",
      "Epoch 2/30 - RMSE: nan\n",
      "Epoch 3/30 - RMSE: nan\n",
      "Epoch 4/30 - RMSE: nan\n",
      "Epoch 5/30 - RMSE: nan\n",
      "Epoch 6/30 - RMSE: nan\n",
      "Epoch 7/30 - RMSE: nan\n",
      "Epoch 8/30 - RMSE: nan\n",
      "Epoch 9/30 - RMSE: nan\n",
      "Epoch 10/30 - RMSE: nan\n",
      "Epoch 11/30 - RMSE: nan\n",
      "Epoch 12/30 - RMSE: nan\n",
      "Epoch 13/30 - RMSE: nan\n",
      "Epoch 14/30 - RMSE: nan\n",
      "Epoch 15/30 - RMSE: nan\n",
      "Epoch 16/30 - RMSE: nan\n",
      "Epoch 17/30 - RMSE: nan\n",
      "Epoch 18/30 - RMSE: nan\n",
      "Epoch 19/30 - RMSE: nan\n",
      "Epoch 20/30 - RMSE: nan\n",
      "Epoch 21/30 - RMSE: nan\n",
      "Epoch 22/30 - RMSE: nan\n",
      "Epoch 23/30 - RMSE: nan\n",
      "Epoch 24/30 - RMSE: nan\n",
      "Epoch 25/30 - RMSE: nan\n",
      "Epoch 26/30 - RMSE: nan\n",
      "Epoch 27/30 - RMSE: nan\n",
      "Epoch 28/30 - RMSE: nan\n",
      "Epoch 29/30 - RMSE: nan\n",
      "Epoch 30/30 - RMSE: nan\n",
      "(factors=50, lr=0.2, reg=0.1, ep=30) - RMSE=nan\n",
      "Epoch 1/40 - RMSE: nan\n",
      "Epoch 2/40 - RMSE: nan\n",
      "Epoch 3/40 - RMSE: nan\n",
      "Epoch 4/40 - RMSE: nan\n",
      "Epoch 5/40 - RMSE: nan\n",
      "Epoch 6/40 - RMSE: nan\n",
      "Epoch 7/40 - RMSE: nan\n",
      "Epoch 8/40 - RMSE: nan\n",
      "Epoch 9/40 - RMSE: nan\n",
      "Epoch 10/40 - RMSE: nan\n",
      "Epoch 11/40 - RMSE: nan\n",
      "Epoch 12/40 - RMSE: nan\n",
      "Epoch 13/40 - RMSE: nan\n",
      "Epoch 14/40 - RMSE: nan\n",
      "Epoch 15/40 - RMSE: nan\n",
      "Epoch 16/40 - RMSE: nan\n",
      "Epoch 17/40 - RMSE: nan\n",
      "Epoch 18/40 - RMSE: nan\n",
      "Epoch 19/40 - RMSE: nan\n",
      "Epoch 20/40 - RMSE: nan\n",
      "Epoch 21/40 - RMSE: nan\n",
      "Epoch 22/40 - RMSE: nan\n",
      "Epoch 23/40 - RMSE: nan\n",
      "Epoch 24/40 - RMSE: nan\n",
      "Epoch 25/40 - RMSE: nan\n",
      "Epoch 26/40 - RMSE: nan\n",
      "Epoch 27/40 - RMSE: nan\n",
      "Epoch 28/40 - RMSE: nan\n",
      "Epoch 29/40 - RMSE: nan\n",
      "Epoch 30/40 - RMSE: nan\n",
      "Epoch 31/40 - RMSE: nan\n",
      "Epoch 32/40 - RMSE: nan\n",
      "Epoch 33/40 - RMSE: nan\n",
      "Epoch 34/40 - RMSE: nan\n",
      "Epoch 35/40 - RMSE: nan\n",
      "Epoch 36/40 - RMSE: nan\n",
      "Epoch 37/40 - RMSE: nan\n",
      "Epoch 38/40 - RMSE: nan\n",
      "Epoch 39/40 - RMSE: nan\n",
      "Epoch 40/40 - RMSE: nan\n",
      "(factors=50, lr=0.2, reg=0.1, ep=40) - RMSE=nan\n",
      "Epoch 1/50 - RMSE: nan\n",
      "Epoch 2/50 - RMSE: nan\n",
      "Epoch 3/50 - RMSE: nan\n",
      "Epoch 4/50 - RMSE: nan\n",
      "Epoch 5/50 - RMSE: nan\n",
      "Epoch 6/50 - RMSE: nan\n",
      "Epoch 7/50 - RMSE: nan\n",
      "Epoch 8/50 - RMSE: nan\n",
      "Epoch 9/50 - RMSE: nan\n",
      "Epoch 10/50 - RMSE: nan\n",
      "Epoch 11/50 - RMSE: nan\n",
      "Epoch 12/50 - RMSE: nan\n",
      "Epoch 13/50 - RMSE: nan\n",
      "Epoch 14/50 - RMSE: nan\n",
      "Epoch 15/50 - RMSE: nan\n",
      "Epoch 16/50 - RMSE: nan\n",
      "Epoch 17/50 - RMSE: nan\n",
      "Epoch 18/50 - RMSE: nan\n",
      "Epoch 19/50 - RMSE: nan\n",
      "Epoch 20/50 - RMSE: nan\n",
      "Epoch 21/50 - RMSE: nan\n",
      "Epoch 22/50 - RMSE: nan\n",
      "Epoch 23/50 - RMSE: nan\n",
      "Epoch 24/50 - RMSE: nan\n",
      "Epoch 25/50 - RMSE: nan\n",
      "Epoch 26/50 - RMSE: nan\n",
      "Epoch 27/50 - RMSE: nan\n",
      "Epoch 28/50 - RMSE: nan\n",
      "Epoch 29/50 - RMSE: nan\n",
      "Epoch 30/50 - RMSE: nan\n",
      "Epoch 31/50 - RMSE: nan\n",
      "Epoch 32/50 - RMSE: nan\n",
      "Epoch 33/50 - RMSE: nan\n",
      "Epoch 34/50 - RMSE: nan\n",
      "Epoch 35/50 - RMSE: nan\n",
      "Epoch 36/50 - RMSE: nan\n",
      "Epoch 37/50 - RMSE: nan\n",
      "Epoch 38/50 - RMSE: nan\n",
      "Epoch 39/50 - RMSE: nan\n",
      "Epoch 40/50 - RMSE: nan\n",
      "Epoch 41/50 - RMSE: nan\n",
      "Epoch 42/50 - RMSE: nan\n",
      "Epoch 43/50 - RMSE: nan\n",
      "Epoch 44/50 - RMSE: nan\n",
      "Epoch 45/50 - RMSE: nan\n",
      "Epoch 46/50 - RMSE: nan\n",
      "Epoch 47/50 - RMSE: nan\n",
      "Epoch 48/50 - RMSE: nan\n",
      "Epoch 49/50 - RMSE: nan\n",
      "Epoch 50/50 - RMSE: nan\n",
      "(factors=50, lr=0.2, reg=0.1, ep=50) - RMSE=nan\n",
      "Epoch 1/10 - RMSE: 1.0985\n",
      "Epoch 2/10 - RMSE: 1.0583\n",
      "Epoch 3/10 - RMSE: 1.0312\n",
      "Epoch 4/10 - RMSE: 1.0115\n",
      "Epoch 5/10 - RMSE: 0.9965\n",
      "Epoch 6/10 - RMSE: 0.9845\n",
      "Epoch 7/10 - RMSE: 0.9746\n",
      "Epoch 8/10 - RMSE: 0.9662\n",
      "Epoch 9/10 - RMSE: 0.9590\n",
      "Epoch 10/10 - RMSE: 0.9526\n",
      "(factors=100, lr=0.001, reg=0.001, ep=10) - RMSE=1.003485757353091\n",
      "Epoch 1/20 - RMSE: 1.0986\n",
      "Epoch 2/20 - RMSE: 1.0583\n",
      "Epoch 3/20 - RMSE: 1.0312\n",
      "Epoch 4/20 - RMSE: 1.0115\n",
      "Epoch 5/20 - RMSE: 0.9964\n",
      "Epoch 6/20 - RMSE: 0.9844\n",
      "Epoch 7/20 - RMSE: 0.9744\n",
      "Epoch 8/20 - RMSE: 0.9660\n",
      "Epoch 9/20 - RMSE: 0.9587\n",
      "Epoch 10/20 - RMSE: 0.9523\n",
      "Epoch 11/20 - RMSE: 0.9465\n",
      "Epoch 12/20 - RMSE: 0.9413\n",
      "Epoch 13/20 - RMSE: 0.9364\n",
      "Epoch 14/20 - RMSE: 0.9319\n",
      "Epoch 15/20 - RMSE: 0.9277\n",
      "Epoch 16/20 - RMSE: 0.9237\n",
      "Epoch 17/20 - RMSE: 0.9199\n",
      "Epoch 18/20 - RMSE: 0.9162\n",
      "Epoch 19/20 - RMSE: 0.9127\n",
      "Epoch 20/20 - RMSE: 0.9093\n",
      "(factors=100, lr=0.001, reg=0.001, ep=20) - RMSE=0.9822245486382096\n",
      "Epoch 1/30 - RMSE: 1.0983\n",
      "Epoch 2/30 - RMSE: 1.0580\n",
      "Epoch 3/30 - RMSE: 1.0309\n",
      "Epoch 4/30 - RMSE: 1.0113\n",
      "Epoch 5/30 - RMSE: 0.9962\n",
      "Epoch 6/30 - RMSE: 0.9842\n",
      "Epoch 7/30 - RMSE: 0.9744\n",
      "Epoch 8/30 - RMSE: 0.9660\n",
      "Epoch 9/30 - RMSE: 0.9587\n",
      "Epoch 10/30 - RMSE: 0.9523\n",
      "Epoch 11/30 - RMSE: 0.9466\n",
      "Epoch 12/30 - RMSE: 0.9413\n",
      "Epoch 13/30 - RMSE: 0.9365\n",
      "Epoch 14/30 - RMSE: 0.9320\n",
      "Epoch 15/30 - RMSE: 0.9278\n",
      "Epoch 16/30 - RMSE: 0.9238\n",
      "Epoch 17/30 - RMSE: 0.9200\n",
      "Epoch 18/30 - RMSE: 0.9164\n",
      "Epoch 19/30 - RMSE: 0.9129\n",
      "Epoch 20/30 - RMSE: 0.9096\n",
      "Epoch 21/30 - RMSE: 0.9063\n",
      "Epoch 22/30 - RMSE: 0.9031\n",
      "Epoch 23/30 - RMSE: 0.9000\n",
      "Epoch 24/30 - RMSE: 0.8969\n",
      "Epoch 25/30 - RMSE: 0.8939\n",
      "Epoch 26/30 - RMSE: 0.8909\n",
      "Epoch 27/30 - RMSE: 0.8879\n",
      "Epoch 28/30 - RMSE: 0.8850\n",
      "Epoch 29/30 - RMSE: 0.8821\n",
      "Epoch 30/30 - RMSE: 0.8792\n",
      "(factors=100, lr=0.001, reg=0.001, ep=30) - RMSE=0.9732696621994579\n",
      "Epoch 1/40 - RMSE: 1.0982\n",
      "Epoch 2/40 - RMSE: 1.0580\n",
      "Epoch 3/40 - RMSE: 1.0308\n",
      "Epoch 4/40 - RMSE: 1.0112\n",
      "Epoch 5/40 - RMSE: 0.9961\n",
      "Epoch 6/40 - RMSE: 0.9841\n",
      "Epoch 7/40 - RMSE: 0.9742\n",
      "Epoch 8/40 - RMSE: 0.9658\n",
      "Epoch 9/40 - RMSE: 0.9586\n",
      "Epoch 10/40 - RMSE: 0.9522\n",
      "Epoch 11/40 - RMSE: 0.9464\n",
      "Epoch 12/40 - RMSE: 0.9412\n",
      "Epoch 13/40 - RMSE: 0.9363\n",
      "Epoch 14/40 - RMSE: 0.9319\n",
      "Epoch 15/40 - RMSE: 0.9276\n",
      "Epoch 16/40 - RMSE: 0.9237\n",
      "Epoch 17/40 - RMSE: 0.9199\n",
      "Epoch 18/40 - RMSE: 0.9162\n",
      "Epoch 19/40 - RMSE: 0.9127\n",
      "Epoch 20/40 - RMSE: 0.9093\n",
      "Epoch 21/40 - RMSE: 0.9061\n",
      "Epoch 22/40 - RMSE: 0.9028\n",
      "Epoch 23/40 - RMSE: 0.8997\n",
      "Epoch 24/40 - RMSE: 0.8966\n",
      "Epoch 25/40 - RMSE: 0.8936\n",
      "Epoch 26/40 - RMSE: 0.8906\n",
      "Epoch 27/40 - RMSE: 0.8876\n",
      "Epoch 28/40 - RMSE: 0.8847\n",
      "Epoch 29/40 - RMSE: 0.8818\n",
      "Epoch 30/40 - RMSE: 0.8789\n",
      "Epoch 31/40 - RMSE: 0.8759\n",
      "Epoch 32/40 - RMSE: 0.8730\n",
      "Epoch 33/40 - RMSE: 0.8701\n",
      "Epoch 34/40 - RMSE: 0.8672\n",
      "Epoch 35/40 - RMSE: 0.8643\n",
      "Epoch 36/40 - RMSE: 0.8613\n",
      "Epoch 37/40 - RMSE: 0.8584\n",
      "Epoch 38/40 - RMSE: 0.8554\n",
      "Epoch 39/40 - RMSE: 0.8524\n",
      "Epoch 40/40 - RMSE: 0.8493\n",
      "(factors=100, lr=0.001, reg=0.001, ep=40) - RMSE=0.9711691142256782\n",
      "Epoch 1/50 - RMSE: 1.0982\n",
      "Epoch 2/50 - RMSE: 1.0579\n",
      "Epoch 3/50 - RMSE: 1.0307\n",
      "Epoch 4/50 - RMSE: 1.0110\n",
      "Epoch 5/50 - RMSE: 0.9959\n",
      "Epoch 6/50 - RMSE: 0.9839\n",
      "Epoch 7/50 - RMSE: 0.9739\n",
      "Epoch 8/50 - RMSE: 0.9655\n",
      "Epoch 9/50 - RMSE: 0.9582\n",
      "Epoch 10/50 - RMSE: 0.9518\n",
      "Epoch 11/50 - RMSE: 0.9460\n",
      "Epoch 12/50 - RMSE: 0.9407\n",
      "Epoch 13/50 - RMSE: 0.9358\n",
      "Epoch 14/50 - RMSE: 0.9313\n",
      "Epoch 15/50 - RMSE: 0.9270\n",
      "Epoch 16/50 - RMSE: 0.9230\n",
      "Epoch 17/50 - RMSE: 0.9192\n",
      "Epoch 18/50 - RMSE: 0.9155\n",
      "Epoch 19/50 - RMSE: 0.9119\n",
      "Epoch 20/50 - RMSE: 0.9085\n",
      "Epoch 21/50 - RMSE: 0.9052\n",
      "Epoch 22/50 - RMSE: 0.9019\n",
      "Epoch 23/50 - RMSE: 0.8988\n",
      "Epoch 24/50 - RMSE: 0.8956\n",
      "Epoch 25/50 - RMSE: 0.8926\n",
      "Epoch 26/50 - RMSE: 0.8895\n",
      "Epoch 27/50 - RMSE: 0.8865\n",
      "Epoch 28/50 - RMSE: 0.8835\n",
      "Epoch 29/50 - RMSE: 0.8805\n",
      "Epoch 30/50 - RMSE: 0.8776\n",
      "Epoch 31/50 - RMSE: 0.8746\n",
      "Epoch 32/50 - RMSE: 0.8717\n",
      "Epoch 33/50 - RMSE: 0.8687\n",
      "Epoch 34/50 - RMSE: 0.8657\n",
      "Epoch 35/50 - RMSE: 0.8627\n",
      "Epoch 36/50 - RMSE: 0.8597\n",
      "Epoch 37/50 - RMSE: 0.8567\n",
      "Epoch 38/50 - RMSE: 0.8537\n",
      "Epoch 39/50 - RMSE: 0.8506\n",
      "Epoch 40/50 - RMSE: 0.8475\n",
      "Epoch 41/50 - RMSE: 0.8444\n",
      "Epoch 42/50 - RMSE: 0.8412\n",
      "Epoch 43/50 - RMSE: 0.8381\n",
      "Epoch 44/50 - RMSE: 0.8348\n",
      "Epoch 45/50 - RMSE: 0.8316\n",
      "Epoch 46/50 - RMSE: 0.8283\n",
      "Epoch 47/50 - RMSE: 0.8249\n",
      "Epoch 48/50 - RMSE: 0.8216\n",
      "Epoch 49/50 - RMSE: 0.8181\n",
      "Epoch 50/50 - RMSE: 0.8147\n",
      "(factors=100, lr=0.001, reg=0.001, ep=50) - RMSE=0.9665043985065043\n",
      "Epoch 1/10 - RMSE: 1.0986\n",
      "Epoch 2/10 - RMSE: 1.0584\n",
      "Epoch 3/10 - RMSE: 1.0313\n",
      "Epoch 4/10 - RMSE: 1.0117\n",
      "Epoch 5/10 - RMSE: 0.9967\n",
      "Epoch 6/10 - RMSE: 0.9847\n",
      "Epoch 7/10 - RMSE: 0.9748\n",
      "Epoch 8/10 - RMSE: 0.9665\n",
      "Epoch 9/10 - RMSE: 0.9593\n",
      "Epoch 10/10 - RMSE: 0.9529\n",
      "(factors=100, lr=0.001, reg=0.01, ep=10) - RMSE=1.0043040992090364\n",
      "Epoch 1/20 - RMSE: 1.0984\n",
      "Epoch 2/20 - RMSE: 1.0582\n",
      "Epoch 3/20 - RMSE: 1.0311\n",
      "Epoch 4/20 - RMSE: 1.0115\n",
      "Epoch 5/20 - RMSE: 0.9965\n",
      "Epoch 6/20 - RMSE: 0.9845\n",
      "Epoch 7/20 - RMSE: 0.9747\n",
      "Epoch 8/20 - RMSE: 0.9664\n",
      "Epoch 9/20 - RMSE: 0.9591\n",
      "Epoch 10/20 - RMSE: 0.9528\n",
      "Epoch 11/20 - RMSE: 0.9471\n",
      "Epoch 12/20 - RMSE: 0.9419\n",
      "Epoch 13/20 - RMSE: 0.9372\n",
      "Epoch 14/20 - RMSE: 0.9328\n",
      "Epoch 15/20 - RMSE: 0.9287\n",
      "Epoch 16/20 - RMSE: 0.9248\n",
      "Epoch 17/20 - RMSE: 0.9212\n",
      "Epoch 18/20 - RMSE: 0.9177\n",
      "Epoch 19/20 - RMSE: 0.9143\n",
      "Epoch 20/20 - RMSE: 0.9111\n",
      "(factors=100, lr=0.001, reg=0.01, ep=20) - RMSE=0.9806115458383868\n",
      "Epoch 1/30 - RMSE: 1.0987\n",
      "Epoch 2/30 - RMSE: 1.0585\n",
      "Epoch 3/30 - RMSE: 1.0315\n",
      "Epoch 4/30 - RMSE: 1.0119\n",
      "Epoch 5/30 - RMSE: 0.9969\n",
      "Epoch 6/30 - RMSE: 0.9849\n",
      "Epoch 7/30 - RMSE: 0.9751\n",
      "Epoch 8/30 - RMSE: 0.9668\n",
      "Epoch 9/30 - RMSE: 0.9596\n",
      "Epoch 10/30 - RMSE: 0.9532\n",
      "Epoch 11/30 - RMSE: 0.9475\n",
      "Epoch 12/30 - RMSE: 0.9424\n",
      "Epoch 13/30 - RMSE: 0.9377\n",
      "Epoch 14/30 - RMSE: 0.9333\n",
      "Epoch 15/30 - RMSE: 0.9292\n",
      "Epoch 16/30 - RMSE: 0.9253\n",
      "Epoch 17/30 - RMSE: 0.9217\n",
      "Epoch 18/30 - RMSE: 0.9182\n",
      "Epoch 19/30 - RMSE: 0.9148\n",
      "Epoch 20/30 - RMSE: 0.9116\n",
      "Epoch 21/30 - RMSE: 0.9085\n",
      "Epoch 22/30 - RMSE: 0.9055\n",
      "Epoch 23/30 - RMSE: 0.9025\n",
      "Epoch 24/30 - RMSE: 0.8997\n",
      "Epoch 25/30 - RMSE: 0.8968\n",
      "Epoch 26/30 - RMSE: 0.8941\n",
      "Epoch 27/30 - RMSE: 0.8913\n",
      "Epoch 28/30 - RMSE: 0.8887\n",
      "Epoch 29/30 - RMSE: 0.8860\n",
      "Epoch 30/30 - RMSE: 0.8833\n",
      "(factors=100, lr=0.001, reg=0.01, ep=30) - RMSE=0.972870380527807\n",
      "Epoch 1/40 - RMSE: 1.0977\n",
      "Epoch 2/40 - RMSE: 1.0576\n",
      "Epoch 3/40 - RMSE: 1.0305\n",
      "Epoch 4/40 - RMSE: 1.0109\n",
      "Epoch 5/40 - RMSE: 0.9959\n",
      "Epoch 6/40 - RMSE: 0.9840\n",
      "Epoch 7/40 - RMSE: 0.9742\n",
      "Epoch 8/40 - RMSE: 0.9658\n",
      "Epoch 9/40 - RMSE: 0.9587\n",
      "Epoch 10/40 - RMSE: 0.9523\n",
      "Epoch 11/40 - RMSE: 0.9466\n",
      "Epoch 12/40 - RMSE: 0.9415\n",
      "Epoch 13/40 - RMSE: 0.9368\n",
      "Epoch 14/40 - RMSE: 0.9324\n",
      "Epoch 15/40 - RMSE: 0.9283\n",
      "Epoch 16/40 - RMSE: 0.9244\n",
      "Epoch 17/40 - RMSE: 0.9207\n",
      "Epoch 18/40 - RMSE: 0.9172\n",
      "Epoch 19/40 - RMSE: 0.9139\n",
      "Epoch 20/40 - RMSE: 0.9107\n",
      "Epoch 21/40 - RMSE: 0.9076\n",
      "Epoch 22/40 - RMSE: 0.9045\n",
      "Epoch 23/40 - RMSE: 0.9016\n",
      "Epoch 24/40 - RMSE: 0.8987\n",
      "Epoch 25/40 - RMSE: 0.8959\n",
      "Epoch 26/40 - RMSE: 0.8931\n",
      "Epoch 27/40 - RMSE: 0.8904\n",
      "Epoch 28/40 - RMSE: 0.8877\n",
      "Epoch 29/40 - RMSE: 0.8850\n",
      "Epoch 30/40 - RMSE: 0.8824\n",
      "Epoch 31/40 - RMSE: 0.8797\n",
      "Epoch 32/40 - RMSE: 0.8771\n",
      "Epoch 33/40 - RMSE: 0.8745\n",
      "Epoch 34/40 - RMSE: 0.8719\n",
      "Epoch 35/40 - RMSE: 0.8693\n",
      "Epoch 36/40 - RMSE: 0.8667\n",
      "Epoch 37/40 - RMSE: 0.8641\n",
      "Epoch 38/40 - RMSE: 0.8614\n",
      "Epoch 39/40 - RMSE: 0.8588\n",
      "Epoch 40/40 - RMSE: 0.8562\n",
      "(factors=100, lr=0.001, reg=0.01, ep=40) - RMSE=0.9689264955046614\n",
      "Epoch 1/50 - RMSE: 1.0983\n",
      "Epoch 2/50 - RMSE: 1.0581\n",
      "Epoch 3/50 - RMSE: 1.0311\n",
      "Epoch 4/50 - RMSE: 1.0115\n",
      "Epoch 5/50 - RMSE: 0.9965\n",
      "Epoch 6/50 - RMSE: 0.9845\n",
      "Epoch 7/50 - RMSE: 0.9747\n",
      "Epoch 8/50 - RMSE: 0.9663\n",
      "Epoch 9/50 - RMSE: 0.9591\n",
      "Epoch 10/50 - RMSE: 0.9528\n",
      "Epoch 11/50 - RMSE: 0.9471\n",
      "Epoch 12/50 - RMSE: 0.9419\n",
      "Epoch 13/50 - RMSE: 0.9372\n",
      "Epoch 14/50 - RMSE: 0.9328\n",
      "Epoch 15/50 - RMSE: 0.9287\n",
      "Epoch 16/50 - RMSE: 0.9248\n",
      "Epoch 17/50 - RMSE: 0.9212\n",
      "Epoch 18/50 - RMSE: 0.9177\n",
      "Epoch 19/50 - RMSE: 0.9143\n",
      "Epoch 20/50 - RMSE: 0.9111\n",
      "Epoch 21/50 - RMSE: 0.9080\n",
      "Epoch 22/50 - RMSE: 0.9049\n",
      "Epoch 23/50 - RMSE: 0.9020\n",
      "Epoch 24/50 - RMSE: 0.8991\n",
      "Epoch 25/50 - RMSE: 0.8963\n",
      "Epoch 26/50 - RMSE: 0.8935\n",
      "Epoch 27/50 - RMSE: 0.8907\n",
      "Epoch 28/50 - RMSE: 0.8880\n",
      "Epoch 29/50 - RMSE: 0.8853\n",
      "Epoch 30/50 - RMSE: 0.8827\n",
      "Epoch 31/50 - RMSE: 0.8800\n",
      "Epoch 32/50 - RMSE: 0.8774\n",
      "Epoch 33/50 - RMSE: 0.8748\n",
      "Epoch 34/50 - RMSE: 0.8722\n",
      "Epoch 35/50 - RMSE: 0.8696\n",
      "Epoch 36/50 - RMSE: 0.8669\n",
      "Epoch 37/50 - RMSE: 0.8643\n",
      "Epoch 38/50 - RMSE: 0.8617\n",
      "Epoch 39/50 - RMSE: 0.8590\n",
      "Epoch 40/50 - RMSE: 0.8564\n",
      "Epoch 41/50 - RMSE: 0.8537\n",
      "Epoch 42/50 - RMSE: 0.8510\n",
      "Epoch 43/50 - RMSE: 0.8483\n",
      "Epoch 44/50 - RMSE: 0.8455\n",
      "Epoch 45/50 - RMSE: 0.8428\n",
      "Epoch 46/50 - RMSE: 0.8400\n",
      "Epoch 47/50 - RMSE: 0.8372\n",
      "Epoch 48/50 - RMSE: 0.8343\n",
      "Epoch 49/50 - RMSE: 0.8314\n",
      "Epoch 50/50 - RMSE: 0.8285\n",
      "(factors=100, lr=0.001, reg=0.01, ep=50) - RMSE=0.966635728920881\n",
      "Epoch 1/10 - RMSE: 1.0982\n",
      "Epoch 2/10 - RMSE: 1.0585\n",
      "Epoch 3/10 - RMSE: 1.0319\n",
      "Epoch 4/10 - RMSE: 1.0128\n",
      "Epoch 5/10 - RMSE: 0.9982\n",
      "Epoch 6/10 - RMSE: 0.9867\n",
      "Epoch 7/10 - RMSE: 0.9774\n",
      "Epoch 8/10 - RMSE: 0.9695\n",
      "Epoch 9/10 - RMSE: 0.9628\n",
      "Epoch 10/10 - RMSE: 0.9571\n",
      "(factors=100, lr=0.001, reg=0.1, ep=10) - RMSE=1.0036775551531343\n",
      "Epoch 1/20 - RMSE: 1.0984\n",
      "Epoch 2/20 - RMSE: 1.0586\n",
      "Epoch 3/20 - RMSE: 1.0321\n",
      "Epoch 4/20 - RMSE: 1.0129\n",
      "Epoch 5/20 - RMSE: 0.9984\n",
      "Epoch 6/20 - RMSE: 0.9868\n",
      "Epoch 7/20 - RMSE: 0.9774\n",
      "Epoch 8/20 - RMSE: 0.9696\n",
      "Epoch 9/20 - RMSE: 0.9629\n",
      "Epoch 10/20 - RMSE: 0.9570\n",
      "Epoch 11/20 - RMSE: 0.9519\n",
      "Epoch 12/20 - RMSE: 0.9475\n",
      "Epoch 13/20 - RMSE: 0.9434\n",
      "Epoch 14/20 - RMSE: 0.9397\n",
      "Epoch 15/20 - RMSE: 0.9364\n",
      "Epoch 16/20 - RMSE: 0.9334\n",
      "Epoch 17/20 - RMSE: 0.9306\n",
      "Epoch 18/20 - RMSE: 0.9280\n",
      "Epoch 19/20 - RMSE: 0.9256\n",
      "Epoch 20/20 - RMSE: 0.9234\n",
      "(factors=100, lr=0.001, reg=0.1, ep=20) - RMSE=0.9825676791607564\n",
      "Epoch 1/30 - RMSE: 1.0983\n",
      "Epoch 2/30 - RMSE: 1.0586\n",
      "Epoch 3/30 - RMSE: 1.0320\n",
      "Epoch 4/30 - RMSE: 1.0129\n",
      "Epoch 5/30 - RMSE: 0.9983\n",
      "Epoch 6/30 - RMSE: 0.9868\n",
      "Epoch 7/30 - RMSE: 0.9774\n",
      "Epoch 8/30 - RMSE: 0.9695\n",
      "Epoch 9/30 - RMSE: 0.9628\n",
      "Epoch 10/30 - RMSE: 0.9571\n",
      "Epoch 11/30 - RMSE: 0.9520\n",
      "Epoch 12/30 - RMSE: 0.9475\n",
      "Epoch 13/30 - RMSE: 0.9435\n",
      "Epoch 14/30 - RMSE: 0.9398\n",
      "Epoch 15/30 - RMSE: 0.9365\n",
      "Epoch 16/30 - RMSE: 0.9335\n",
      "Epoch 17/30 - RMSE: 0.9307\n",
      "Epoch 18/30 - RMSE: 0.9282\n",
      "Epoch 19/30 - RMSE: 0.9258\n",
      "Epoch 20/30 - RMSE: 0.9236\n",
      "Epoch 21/30 - RMSE: 0.9215\n",
      "Epoch 22/30 - RMSE: 0.9196\n",
      "Epoch 23/30 - RMSE: 0.9178\n",
      "Epoch 24/30 - RMSE: 0.9160\n",
      "Epoch 25/30 - RMSE: 0.9144\n",
      "Epoch 26/30 - RMSE: 0.9129\n",
      "Epoch 27/30 - RMSE: 0.9114\n",
      "Epoch 28/30 - RMSE: 0.9101\n",
      "Epoch 29/30 - RMSE: 0.9087\n",
      "Epoch 30/30 - RMSE: 0.9075\n",
      "(factors=100, lr=0.001, reg=0.1, ep=30) - RMSE=0.9727741928183977\n",
      "Epoch 1/40 - RMSE: 1.0982\n",
      "Epoch 2/40 - RMSE: 1.0584\n",
      "Epoch 3/40 - RMSE: 1.0319\n",
      "Epoch 4/40 - RMSE: 1.0127\n",
      "Epoch 5/40 - RMSE: 0.9982\n",
      "Epoch 6/40 - RMSE: 0.9867\n",
      "Epoch 7/40 - RMSE: 0.9772\n",
      "Epoch 8/40 - RMSE: 0.9694\n",
      "Epoch 9/40 - RMSE: 0.9627\n",
      "Epoch 10/40 - RMSE: 0.9569\n",
      "Epoch 11/40 - RMSE: 0.9518\n",
      "Epoch 12/40 - RMSE: 0.9473\n",
      "Epoch 13/40 - RMSE: 0.9433\n",
      "Epoch 14/40 - RMSE: 0.9397\n",
      "Epoch 15/40 - RMSE: 0.9364\n",
      "Epoch 16/40 - RMSE: 0.9333\n",
      "Epoch 17/40 - RMSE: 0.9305\n",
      "Epoch 18/40 - RMSE: 0.9280\n",
      "Epoch 19/40 - RMSE: 0.9256\n",
      "Epoch 20/40 - RMSE: 0.9233\n",
      "Epoch 21/40 - RMSE: 0.9213\n",
      "Epoch 22/40 - RMSE: 0.9193\n",
      "Epoch 23/40 - RMSE: 0.9175\n",
      "Epoch 24/40 - RMSE: 0.9158\n",
      "Epoch 25/40 - RMSE: 0.9142\n",
      "Epoch 26/40 - RMSE: 0.9126\n",
      "Epoch 27/40 - RMSE: 0.9112\n",
      "Epoch 28/40 - RMSE: 0.9097\n",
      "Epoch 29/40 - RMSE: 0.9084\n",
      "Epoch 30/40 - RMSE: 0.9072\n",
      "Epoch 31/40 - RMSE: 0.9060\n",
      "Epoch 32/40 - RMSE: 0.9048\n",
      "Epoch 33/40 - RMSE: 0.9037\n",
      "Epoch 34/40 - RMSE: 0.9026\n",
      "Epoch 35/40 - RMSE: 0.9016\n",
      "Epoch 36/40 - RMSE: 0.9006\n",
      "Epoch 37/40 - RMSE: 0.8997\n",
      "Epoch 38/40 - RMSE: 0.8987\n",
      "Epoch 39/40 - RMSE: 0.8978\n",
      "Epoch 40/40 - RMSE: 0.8969\n",
      "(factors=100, lr=0.001, reg=0.1, ep=40) - RMSE=0.967397660865062\n",
      "Epoch 1/50 - RMSE: 1.0983\n",
      "Epoch 2/50 - RMSE: 1.0585\n",
      "Epoch 3/50 - RMSE: 1.0320\n",
      "Epoch 4/50 - RMSE: 1.0128\n",
      "Epoch 5/50 - RMSE: 0.9983\n",
      "Epoch 6/50 - RMSE: 0.9867\n",
      "Epoch 7/50 - RMSE: 0.9774\n",
      "Epoch 8/50 - RMSE: 0.9695\n",
      "Epoch 9/50 - RMSE: 0.9629\n",
      "Epoch 10/50 - RMSE: 0.9571\n",
      "Epoch 11/50 - RMSE: 0.9520\n",
      "Epoch 12/50 - RMSE: 0.9475\n",
      "Epoch 13/50 - RMSE: 0.9435\n",
      "Epoch 14/50 - RMSE: 0.9399\n",
      "Epoch 15/50 - RMSE: 0.9366\n",
      "Epoch 16/50 - RMSE: 0.9336\n",
      "Epoch 17/50 - RMSE: 0.9308\n",
      "Epoch 18/50 - RMSE: 0.9283\n",
      "Epoch 19/50 - RMSE: 0.9259\n",
      "Epoch 20/50 - RMSE: 0.9237\n",
      "Epoch 21/50 - RMSE: 0.9216\n",
      "Epoch 22/50 - RMSE: 0.9197\n",
      "Epoch 23/50 - RMSE: 0.9179\n",
      "Epoch 24/50 - RMSE: 0.9161\n",
      "Epoch 25/50 - RMSE: 0.9145\n",
      "Epoch 26/50 - RMSE: 0.9130\n",
      "Epoch 27/50 - RMSE: 0.9116\n",
      "Epoch 28/50 - RMSE: 0.9102\n",
      "Epoch 29/50 - RMSE: 0.9089\n",
      "Epoch 30/50 - RMSE: 0.9076\n",
      "Epoch 31/50 - RMSE: 0.9064\n",
      "Epoch 32/50 - RMSE: 0.9053\n",
      "Epoch 33/50 - RMSE: 0.9041\n",
      "Epoch 34/50 - RMSE: 0.9031\n",
      "Epoch 35/50 - RMSE: 0.9021\n",
      "Epoch 36/50 - RMSE: 0.9011\n",
      "Epoch 37/50 - RMSE: 0.9002\n",
      "Epoch 38/50 - RMSE: 0.8993\n",
      "Epoch 39/50 - RMSE: 0.8984\n",
      "Epoch 40/50 - RMSE: 0.8975\n",
      "Epoch 41/50 - RMSE: 0.8967\n",
      "Epoch 42/50 - RMSE: 0.8959\n",
      "Epoch 43/50 - RMSE: 0.8951\n",
      "Epoch 44/50 - RMSE: 0.8944\n",
      "Epoch 45/50 - RMSE: 0.8936\n",
      "Epoch 46/50 - RMSE: 0.8929\n",
      "Epoch 47/50 - RMSE: 0.8922\n",
      "Epoch 48/50 - RMSE: 0.8914\n",
      "Epoch 49/50 - RMSE: 0.8908\n",
      "Epoch 50/50 - RMSE: 0.8901\n",
      "(factors=100, lr=0.001, reg=0.1, ep=50) - RMSE=0.9643043931007407\n",
      "Epoch 1/10 - RMSE: 1.0465\n",
      "Epoch 2/10 - RMSE: 0.9736\n",
      "Epoch 3/10 - RMSE: 0.9430\n",
      "Epoch 4/10 - RMSE: 0.9226\n",
      "Epoch 5/10 - RMSE: 0.9059\n",
      "Epoch 6/10 - RMSE: 0.8909\n",
      "Epoch 7/10 - RMSE: 0.8762\n",
      "Epoch 8/10 - RMSE: 0.8615\n",
      "Epoch 9/10 - RMSE: 0.8461\n",
      "Epoch 10/10 - RMSE: 0.8299\n",
      "(factors=100, lr=0.005, reg=0.001, ep=10) - RMSE=0.9661050681500113\n",
      "Epoch 1/20 - RMSE: 1.0466\n",
      "Epoch 2/20 - RMSE: 0.9736\n",
      "Epoch 3/20 - RMSE: 0.9429\n",
      "Epoch 4/20 - RMSE: 0.9225\n",
      "Epoch 5/20 - RMSE: 0.9059\n",
      "Epoch 6/20 - RMSE: 0.8909\n",
      "Epoch 7/20 - RMSE: 0.8763\n",
      "Epoch 8/20 - RMSE: 0.8616\n",
      "Epoch 9/20 - RMSE: 0.8463\n",
      "Epoch 10/20 - RMSE: 0.8301\n",
      "Epoch 11/20 - RMSE: 0.8130\n",
      "Epoch 12/20 - RMSE: 0.7948\n",
      "Epoch 13/20 - RMSE: 0.7756\n",
      "Epoch 14/20 - RMSE: 0.7556\n",
      "Epoch 15/20 - RMSE: 0.7350\n",
      "Epoch 16/20 - RMSE: 0.7140\n",
      "Epoch 17/20 - RMSE: 0.6928\n",
      "Epoch 18/20 - RMSE: 0.6715\n",
      "Epoch 19/20 - RMSE: 0.6505\n",
      "Epoch 20/20 - RMSE: 0.6297\n",
      "(factors=100, lr=0.005, reg=0.001, ep=20) - RMSE=0.964117422650231\n",
      "Epoch 1/30 - RMSE: 1.0472\n",
      "Epoch 2/30 - RMSE: 0.9743\n",
      "Epoch 3/30 - RMSE: 0.9438\n",
      "Epoch 4/30 - RMSE: 0.9235\n",
      "Epoch 5/30 - RMSE: 0.9070\n",
      "Epoch 6/30 - RMSE: 0.8922\n",
      "Epoch 7/30 - RMSE: 0.8778\n",
      "Epoch 8/30 - RMSE: 0.8634\n",
      "Epoch 9/30 - RMSE: 0.8484\n",
      "Epoch 10/30 - RMSE: 0.8325\n",
      "Epoch 11/30 - RMSE: 0.8157\n",
      "Epoch 12/30 - RMSE: 0.7978\n",
      "Epoch 13/30 - RMSE: 0.7789\n",
      "Epoch 14/30 - RMSE: 0.7590\n",
      "Epoch 15/30 - RMSE: 0.7384\n",
      "Epoch 16/30 - RMSE: 0.7175\n",
      "Epoch 17/30 - RMSE: 0.6962\n",
      "Epoch 18/30 - RMSE: 0.6749\n",
      "Epoch 19/30 - RMSE: 0.6537\n",
      "Epoch 20/30 - RMSE: 0.6329\n",
      "Epoch 21/30 - RMSE: 0.6124\n",
      "Epoch 22/30 - RMSE: 0.5924\n",
      "Epoch 23/30 - RMSE: 0.5730\n",
      "Epoch 24/30 - RMSE: 0.5543\n",
      "Epoch 25/30 - RMSE: 0.5362\n",
      "Epoch 26/30 - RMSE: 0.5188\n",
      "Epoch 27/30 - RMSE: 0.5021\n",
      "Epoch 28/30 - RMSE: 0.4861\n",
      "Epoch 29/30 - RMSE: 0.4708\n",
      "Epoch 30/30 - RMSE: 0.4561\n",
      "(factors=100, lr=0.005, reg=0.001, ep=30) - RMSE=0.9942761207510663\n",
      "Epoch 1/40 - RMSE: 1.0471\n",
      "Epoch 2/40 - RMSE: 0.9739\n",
      "Epoch 3/40 - RMSE: 0.9435\n",
      "Epoch 4/40 - RMSE: 0.9230\n",
      "Epoch 5/40 - RMSE: 0.9066\n",
      "Epoch 6/40 - RMSE: 0.8916\n",
      "Epoch 7/40 - RMSE: 0.8772\n",
      "Epoch 8/40 - RMSE: 0.8627\n",
      "Epoch 9/40 - RMSE: 0.8475\n",
      "Epoch 10/40 - RMSE: 0.8316\n",
      "Epoch 11/40 - RMSE: 0.8146\n",
      "Epoch 12/40 - RMSE: 0.7965\n",
      "Epoch 13/40 - RMSE: 0.7775\n",
      "Epoch 14/40 - RMSE: 0.7575\n",
      "Epoch 15/40 - RMSE: 0.7369\n",
      "Epoch 16/40 - RMSE: 0.7159\n",
      "Epoch 17/40 - RMSE: 0.6946\n",
      "Epoch 18/40 - RMSE: 0.6734\n",
      "Epoch 19/40 - RMSE: 0.6524\n",
      "Epoch 20/40 - RMSE: 0.6317\n",
      "Epoch 21/40 - RMSE: 0.6114\n",
      "Epoch 22/40 - RMSE: 0.5916\n",
      "Epoch 23/40 - RMSE: 0.5724\n",
      "Epoch 24/40 - RMSE: 0.5539\n",
      "Epoch 25/40 - RMSE: 0.5360\n",
      "Epoch 26/40 - RMSE: 0.5188\n",
      "Epoch 27/40 - RMSE: 0.5022\n",
      "Epoch 28/40 - RMSE: 0.4864\n",
      "Epoch 29/40 - RMSE: 0.4712\n",
      "Epoch 30/40 - RMSE: 0.4567\n",
      "Epoch 31/40 - RMSE: 0.4428\n",
      "Epoch 32/40 - RMSE: 0.4295\n",
      "Epoch 33/40 - RMSE: 0.4167\n",
      "Epoch 34/40 - RMSE: 0.4046\n",
      "Epoch 35/40 - RMSE: 0.3930\n",
      "Epoch 36/40 - RMSE: 0.3819\n",
      "Epoch 37/40 - RMSE: 0.3713\n",
      "Epoch 38/40 - RMSE: 0.3611\n",
      "Epoch 39/40 - RMSE: 0.3514\n",
      "Epoch 40/40 - RMSE: 0.3421\n",
      "(factors=100, lr=0.005, reg=0.001, ep=40) - RMSE=1.0193721969198446\n",
      "Epoch 1/50 - RMSE: 1.0469\n",
      "Epoch 2/50 - RMSE: 0.9739\n",
      "Epoch 3/50 - RMSE: 0.9435\n",
      "Epoch 4/50 - RMSE: 0.9232\n",
      "Epoch 5/50 - RMSE: 0.9067\n",
      "Epoch 6/50 - RMSE: 0.8919\n",
      "Epoch 7/50 - RMSE: 0.8776\n",
      "Epoch 8/50 - RMSE: 0.8632\n",
      "Epoch 9/50 - RMSE: 0.8483\n",
      "Epoch 10/50 - RMSE: 0.8325\n",
      "Epoch 11/50 - RMSE: 0.8158\n",
      "Epoch 12/50 - RMSE: 0.7978\n",
      "Epoch 13/50 - RMSE: 0.7789\n",
      "Epoch 14/50 - RMSE: 0.7589\n",
      "Epoch 15/50 - RMSE: 0.7382\n",
      "Epoch 16/50 - RMSE: 0.7169\n",
      "Epoch 17/50 - RMSE: 0.6953\n",
      "Epoch 18/50 - RMSE: 0.6738\n",
      "Epoch 19/50 - RMSE: 0.6524\n",
      "Epoch 20/50 - RMSE: 0.6313\n",
      "Epoch 21/50 - RMSE: 0.6107\n",
      "Epoch 22/50 - RMSE: 0.5906\n",
      "Epoch 23/50 - RMSE: 0.5712\n",
      "Epoch 24/50 - RMSE: 0.5524\n",
      "Epoch 25/50 - RMSE: 0.5343\n",
      "Epoch 26/50 - RMSE: 0.5170\n",
      "Epoch 27/50 - RMSE: 0.5003\n",
      "Epoch 28/50 - RMSE: 0.4843\n",
      "Epoch 29/50 - RMSE: 0.4691\n",
      "Epoch 30/50 - RMSE: 0.4545\n",
      "Epoch 31/50 - RMSE: 0.4406\n",
      "Epoch 32/50 - RMSE: 0.4273\n",
      "Epoch 33/50 - RMSE: 0.4146\n",
      "Epoch 34/50 - RMSE: 0.4024\n",
      "Epoch 35/50 - RMSE: 0.3908\n",
      "Epoch 36/50 - RMSE: 0.3798\n",
      "Epoch 37/50 - RMSE: 0.3692\n",
      "Epoch 38/50 - RMSE: 0.3591\n",
      "Epoch 39/50 - RMSE: 0.3495\n",
      "Epoch 40/50 - RMSE: 0.3402\n",
      "Epoch 41/50 - RMSE: 0.3314\n",
      "Epoch 42/50 - RMSE: 0.3229\n",
      "Epoch 43/50 - RMSE: 0.3148\n",
      "Epoch 44/50 - RMSE: 0.3070\n",
      "Epoch 45/50 - RMSE: 0.2996\n",
      "Epoch 46/50 - RMSE: 0.2925\n",
      "Epoch 47/50 - RMSE: 0.2856\n",
      "Epoch 48/50 - RMSE: 0.2790\n",
      "Epoch 49/50 - RMSE: 0.2727\n",
      "Epoch 50/50 - RMSE: 0.2666\n",
      "(factors=100, lr=0.005, reg=0.001, ep=50) - RMSE=1.0488675628035122\n",
      "Epoch 1/10 - RMSE: 1.0476\n",
      "Epoch 2/10 - RMSE: 0.9744\n",
      "Epoch 3/10 - RMSE: 0.9441\n",
      "Epoch 4/10 - RMSE: 0.9242\n",
      "Epoch 5/10 - RMSE: 0.9082\n",
      "Epoch 6/10 - RMSE: 0.8941\n",
      "Epoch 7/10 - RMSE: 0.8808\n",
      "Epoch 8/10 - RMSE: 0.8676\n",
      "Epoch 9/10 - RMSE: 0.8542\n",
      "Epoch 10/10 - RMSE: 0.8401\n",
      "(factors=100, lr=0.005, reg=0.01, ep=10) - RMSE=0.9662496765722447\n",
      "Epoch 1/20 - RMSE: 1.0469\n",
      "Epoch 2/20 - RMSE: 0.9740\n",
      "Epoch 3/20 - RMSE: 0.9437\n",
      "Epoch 4/20 - RMSE: 0.9239\n",
      "Epoch 5/20 - RMSE: 0.9080\n",
      "Epoch 6/20 - RMSE: 0.8939\n",
      "Epoch 7/20 - RMSE: 0.8806\n",
      "Epoch 8/20 - RMSE: 0.8675\n",
      "Epoch 9/20 - RMSE: 0.8540\n",
      "Epoch 10/20 - RMSE: 0.8400\n",
      "Epoch 11/20 - RMSE: 0.8253\n",
      "Epoch 12/20 - RMSE: 0.8096\n",
      "Epoch 13/20 - RMSE: 0.7931\n",
      "Epoch 14/20 - RMSE: 0.7758\n",
      "Epoch 15/20 - RMSE: 0.7578\n",
      "Epoch 16/20 - RMSE: 0.7392\n",
      "Epoch 17/20 - RMSE: 0.7203\n",
      "Epoch 18/20 - RMSE: 0.7013\n",
      "Epoch 19/20 - RMSE: 0.6822\n",
      "Epoch 20/20 - RMSE: 0.6632\n",
      "(factors=100, lr=0.005, reg=0.01, ep=20) - RMSE=0.9552471582885278\n",
      "Epoch 1/30 - RMSE: 1.0476\n",
      "Epoch 2/30 - RMSE: 0.9744\n",
      "Epoch 3/30 - RMSE: 0.9442\n",
      "Epoch 4/30 - RMSE: 0.9243\n",
      "Epoch 5/30 - RMSE: 0.9084\n",
      "Epoch 6/30 - RMSE: 0.8944\n",
      "Epoch 7/30 - RMSE: 0.8812\n",
      "Epoch 8/30 - RMSE: 0.8680\n",
      "Epoch 9/30 - RMSE: 0.8546\n",
      "Epoch 10/30 - RMSE: 0.8407\n",
      "Epoch 11/30 - RMSE: 0.8259\n",
      "Epoch 12/30 - RMSE: 0.8103\n",
      "Epoch 13/30 - RMSE: 0.7939\n",
      "Epoch 14/30 - RMSE: 0.7767\n",
      "Epoch 15/30 - RMSE: 0.7587\n",
      "Epoch 16/30 - RMSE: 0.7404\n",
      "Epoch 17/30 - RMSE: 0.7216\n",
      "Epoch 18/30 - RMSE: 0.7027\n",
      "Epoch 19/30 - RMSE: 0.6838\n",
      "Epoch 20/30 - RMSE: 0.6649\n",
      "Epoch 21/30 - RMSE: 0.6463\n",
      "Epoch 22/30 - RMSE: 0.6279\n",
      "Epoch 23/30 - RMSE: 0.6100\n",
      "Epoch 24/30 - RMSE: 0.5925\n",
      "Epoch 25/30 - RMSE: 0.5754\n",
      "Epoch 26/30 - RMSE: 0.5589\n",
      "Epoch 27/30 - RMSE: 0.5430\n",
      "Epoch 28/30 - RMSE: 0.5276\n",
      "Epoch 29/30 - RMSE: 0.5127\n",
      "Epoch 30/30 - RMSE: 0.4985\n",
      "(factors=100, lr=0.005, reg=0.01, ep=30) - RMSE=0.9727755783132501\n",
      "Epoch 1/40 - RMSE: 1.0472\n",
      "Epoch 2/40 - RMSE: 0.9741\n",
      "Epoch 3/40 - RMSE: 0.9438\n",
      "Epoch 4/40 - RMSE: 0.9238\n",
      "Epoch 5/40 - RMSE: 0.9079\n",
      "Epoch 6/40 - RMSE: 0.8938\n",
      "Epoch 7/40 - RMSE: 0.8804\n",
      "Epoch 8/40 - RMSE: 0.8673\n",
      "Epoch 9/40 - RMSE: 0.8537\n",
      "Epoch 10/40 - RMSE: 0.8396\n",
      "Epoch 11/40 - RMSE: 0.8248\n",
      "Epoch 12/40 - RMSE: 0.8090\n",
      "Epoch 13/40 - RMSE: 0.7923\n",
      "Epoch 14/40 - RMSE: 0.7748\n",
      "Epoch 15/40 - RMSE: 0.7566\n",
      "Epoch 16/40 - RMSE: 0.7379\n",
      "Epoch 17/40 - RMSE: 0.7188\n",
      "Epoch 18/40 - RMSE: 0.6996\n",
      "Epoch 19/40 - RMSE: 0.6804\n",
      "Epoch 20/40 - RMSE: 0.6614\n",
      "Epoch 21/40 - RMSE: 0.6426\n",
      "Epoch 22/40 - RMSE: 0.6242\n",
      "Epoch 23/40 - RMSE: 0.6062\n",
      "Epoch 24/40 - RMSE: 0.5886\n",
      "Epoch 25/40 - RMSE: 0.5716\n",
      "Epoch 26/40 - RMSE: 0.5552\n",
      "Epoch 27/40 - RMSE: 0.5393\n",
      "Epoch 28/40 - RMSE: 0.5240\n",
      "Epoch 29/40 - RMSE: 0.5093\n",
      "Epoch 30/40 - RMSE: 0.4951\n",
      "Epoch 31/40 - RMSE: 0.4815\n",
      "Epoch 32/40 - RMSE: 0.4685\n",
      "Epoch 33/40 - RMSE: 0.4561\n",
      "Epoch 34/40 - RMSE: 0.4441\n",
      "Epoch 35/40 - RMSE: 0.4327\n",
      "Epoch 36/40 - RMSE: 0.4217\n",
      "Epoch 37/40 - RMSE: 0.4113\n",
      "Epoch 38/40 - RMSE: 0.4013\n",
      "Epoch 39/40 - RMSE: 0.3916\n",
      "Epoch 40/40 - RMSE: 0.3824\n",
      "(factors=100, lr=0.005, reg=0.01, ep=40) - RMSE=0.9884834493281596\n",
      "Epoch 1/50 - RMSE: 1.0471\n",
      "Epoch 2/50 - RMSE: 0.9740\n",
      "Epoch 3/50 - RMSE: 0.9437\n",
      "Epoch 4/50 - RMSE: 0.9239\n",
      "Epoch 5/50 - RMSE: 0.9080\n",
      "Epoch 6/50 - RMSE: 0.8940\n",
      "Epoch 7/50 - RMSE: 0.8807\n",
      "Epoch 8/50 - RMSE: 0.8676\n",
      "Epoch 9/50 - RMSE: 0.8542\n",
      "Epoch 10/50 - RMSE: 0.8402\n",
      "Epoch 11/50 - RMSE: 0.8254\n",
      "Epoch 12/50 - RMSE: 0.8098\n",
      "Epoch 13/50 - RMSE: 0.7932\n",
      "Epoch 14/50 - RMSE: 0.7758\n",
      "Epoch 15/50 - RMSE: 0.7577\n",
      "Epoch 16/50 - RMSE: 0.7391\n",
      "Epoch 17/50 - RMSE: 0.7201\n",
      "Epoch 18/50 - RMSE: 0.7010\n",
      "Epoch 19/50 - RMSE: 0.6818\n",
      "Epoch 20/50 - RMSE: 0.6627\n",
      "Epoch 21/50 - RMSE: 0.6440\n",
      "Epoch 22/50 - RMSE: 0.6255\n",
      "Epoch 23/50 - RMSE: 0.6074\n",
      "Epoch 24/50 - RMSE: 0.5898\n",
      "Epoch 25/50 - RMSE: 0.5726\n",
      "Epoch 26/50 - RMSE: 0.5560\n",
      "Epoch 27/50 - RMSE: 0.5400\n",
      "Epoch 28/50 - RMSE: 0.5245\n",
      "Epoch 29/50 - RMSE: 0.5096\n",
      "Epoch 30/50 - RMSE: 0.4953\n",
      "Epoch 31/50 - RMSE: 0.4816\n",
      "Epoch 32/50 - RMSE: 0.4684\n",
      "Epoch 33/50 - RMSE: 0.4558\n",
      "Epoch 34/50 - RMSE: 0.4437\n",
      "Epoch 35/50 - RMSE: 0.4321\n",
      "Epoch 36/50 - RMSE: 0.4211\n",
      "Epoch 37/50 - RMSE: 0.4105\n",
      "Epoch 38/50 - RMSE: 0.4004\n",
      "Epoch 39/50 - RMSE: 0.3907\n",
      "Epoch 40/50 - RMSE: 0.3814\n",
      "Epoch 41/50 - RMSE: 0.3726\n",
      "Epoch 42/50 - RMSE: 0.3641\n",
      "Epoch 43/50 - RMSE: 0.3560\n",
      "Epoch 44/50 - RMSE: 0.3483\n",
      "Epoch 45/50 - RMSE: 0.3408\n",
      "Epoch 46/50 - RMSE: 0.3336\n",
      "Epoch 47/50 - RMSE: 0.3268\n",
      "Epoch 48/50 - RMSE: 0.3202\n",
      "Epoch 49/50 - RMSE: 0.3140\n",
      "Epoch 50/50 - RMSE: 0.3079\n",
      "(factors=100, lr=0.005, reg=0.01, ep=50) - RMSE=1.0020265001701232\n",
      "Epoch 1/10 - RMSE: 1.0483\n",
      "Epoch 2/10 - RMSE: 0.9770\n",
      "Epoch 3/10 - RMSE: 0.9497\n",
      "Epoch 4/10 - RMSE: 0.9337\n",
      "Epoch 5/10 - RMSE: 0.9229\n",
      "Epoch 6/10 - RMSE: 0.9149\n",
      "Epoch 7/10 - RMSE: 0.9087\n",
      "Epoch 8/10 - RMSE: 0.9037\n",
      "Epoch 9/10 - RMSE: 0.8994\n",
      "Epoch 10/10 - RMSE: 0.8957\n",
      "(factors=100, lr=0.005, reg=0.1, ep=10) - RMSE=0.9634642057679514\n",
      "Epoch 1/20 - RMSE: 1.0477\n",
      "Epoch 2/20 - RMSE: 0.9768\n",
      "Epoch 3/20 - RMSE: 0.9493\n",
      "Epoch 4/20 - RMSE: 0.9334\n",
      "Epoch 5/20 - RMSE: 0.9226\n",
      "Epoch 6/20 - RMSE: 0.9147\n",
      "Epoch 7/20 - RMSE: 0.9084\n",
      "Epoch 8/20 - RMSE: 0.9034\n",
      "Epoch 9/20 - RMSE: 0.8991\n",
      "Epoch 10/20 - RMSE: 0.8953\n",
      "Epoch 11/20 - RMSE: 0.8918\n",
      "Epoch 12/20 - RMSE: 0.8888\n",
      "Epoch 13/20 - RMSE: 0.8858\n",
      "Epoch 14/20 - RMSE: 0.8828\n",
      "Epoch 15/20 - RMSE: 0.8799\n",
      "Epoch 16/20 - RMSE: 0.8772\n",
      "Epoch 17/20 - RMSE: 0.8742\n",
      "Epoch 18/20 - RMSE: 0.8714\n",
      "Epoch 19/20 - RMSE: 0.8683\n",
      "Epoch 20/20 - RMSE: 0.8653\n",
      "(factors=100, lr=0.005, reg=0.1, ep=20) - RMSE=0.9533145481268268\n",
      "Epoch 1/30 - RMSE: 1.0485\n",
      "Epoch 2/30 - RMSE: 0.9774\n",
      "Epoch 3/30 - RMSE: 0.9500\n",
      "Epoch 4/30 - RMSE: 0.9340\n",
      "Epoch 5/30 - RMSE: 0.9232\n",
      "Epoch 6/30 - RMSE: 0.9151\n",
      "Epoch 7/30 - RMSE: 0.9090\n",
      "Epoch 8/30 - RMSE: 0.9039\n",
      "Epoch 9/30 - RMSE: 0.8996\n",
      "Epoch 10/30 - RMSE: 0.8957\n",
      "Epoch 11/30 - RMSE: 0.8923\n",
      "Epoch 12/30 - RMSE: 0.8892\n",
      "Epoch 13/30 - RMSE: 0.8861\n",
      "Epoch 14/30 - RMSE: 0.8832\n",
      "Epoch 15/30 - RMSE: 0.8803\n",
      "Epoch 16/30 - RMSE: 0.8774\n",
      "Epoch 17/30 - RMSE: 0.8746\n",
      "Epoch 18/30 - RMSE: 0.8717\n",
      "Epoch 19/30 - RMSE: 0.8686\n",
      "Epoch 20/30 - RMSE: 0.8653\n",
      "Epoch 21/30 - RMSE: 0.8622\n",
      "Epoch 22/30 - RMSE: 0.8589\n",
      "Epoch 23/30 - RMSE: 0.8556\n",
      "Epoch 24/30 - RMSE: 0.8520\n",
      "Epoch 25/30 - RMSE: 0.8485\n",
      "Epoch 26/30 - RMSE: 0.8448\n",
      "Epoch 27/30 - RMSE: 0.8411\n",
      "Epoch 28/30 - RMSE: 0.8374\n",
      "Epoch 29/30 - RMSE: 0.8336\n",
      "Epoch 30/30 - RMSE: 0.8297\n",
      "(factors=100, lr=0.005, reg=0.1, ep=30) - RMSE=0.941717941844724\n",
      "Epoch 1/40 - RMSE: 1.0479\n",
      "Epoch 2/40 - RMSE: 0.9769\n",
      "Epoch 3/40 - RMSE: 0.9495\n",
      "Epoch 4/40 - RMSE: 0.9337\n",
      "Epoch 5/40 - RMSE: 0.9228\n",
      "Epoch 6/40 - RMSE: 0.9149\n",
      "Epoch 7/40 - RMSE: 0.9087\n",
      "Epoch 8/40 - RMSE: 0.9036\n",
      "Epoch 9/40 - RMSE: 0.8993\n",
      "Epoch 10/40 - RMSE: 0.8956\n",
      "Epoch 11/40 - RMSE: 0.8921\n",
      "Epoch 12/40 - RMSE: 0.8889\n",
      "Epoch 13/40 - RMSE: 0.8860\n",
      "Epoch 14/40 - RMSE: 0.8831\n",
      "Epoch 15/40 - RMSE: 0.8802\n",
      "Epoch 16/40 - RMSE: 0.8774\n",
      "Epoch 17/40 - RMSE: 0.8745\n",
      "Epoch 18/40 - RMSE: 0.8715\n",
      "Epoch 19/40 - RMSE: 0.8684\n",
      "Epoch 20/40 - RMSE: 0.8653\n",
      "Epoch 21/40 - RMSE: 0.8622\n",
      "Epoch 22/40 - RMSE: 0.8589\n",
      "Epoch 23/40 - RMSE: 0.8555\n",
      "Epoch 24/40 - RMSE: 0.8519\n",
      "Epoch 25/40 - RMSE: 0.8484\n",
      "Epoch 26/40 - RMSE: 0.8447\n",
      "Epoch 27/40 - RMSE: 0.8410\n",
      "Epoch 28/40 - RMSE: 0.8373\n",
      "Epoch 29/40 - RMSE: 0.8334\n",
      "Epoch 30/40 - RMSE: 0.8295\n",
      "Epoch 31/40 - RMSE: 0.8258\n",
      "Epoch 32/40 - RMSE: 0.8219\n",
      "Epoch 33/40 - RMSE: 0.8181\n",
      "Epoch 34/40 - RMSE: 0.8142\n",
      "Epoch 35/40 - RMSE: 0.8107\n",
      "Epoch 36/40 - RMSE: 0.8067\n",
      "Epoch 37/40 - RMSE: 0.8030\n",
      "Epoch 38/40 - RMSE: 0.7994\n",
      "Epoch 39/40 - RMSE: 0.7957\n",
      "Epoch 40/40 - RMSE: 0.7920\n",
      "(factors=100, lr=0.005, reg=0.1, ep=40) - RMSE=0.9335821464234322\n",
      "Epoch 1/50 - RMSE: 1.0478\n",
      "Epoch 2/50 - RMSE: 0.9769\n",
      "Epoch 3/50 - RMSE: 0.9496\n",
      "Epoch 4/50 - RMSE: 0.9337\n",
      "Epoch 5/50 - RMSE: 0.9230\n",
      "Epoch 6/50 - RMSE: 0.9151\n",
      "Epoch 7/50 - RMSE: 0.9088\n",
      "Epoch 8/50 - RMSE: 0.9038\n",
      "Epoch 9/50 - RMSE: 0.8994\n",
      "Epoch 10/50 - RMSE: 0.8958\n",
      "Epoch 11/50 - RMSE: 0.8924\n",
      "Epoch 12/50 - RMSE: 0.8891\n",
      "Epoch 13/50 - RMSE: 0.8862\n",
      "Epoch 14/50 - RMSE: 0.8834\n",
      "Epoch 15/50 - RMSE: 0.8805\n",
      "Epoch 16/50 - RMSE: 0.8776\n",
      "Epoch 17/50 - RMSE: 0.8748\n",
      "Epoch 18/50 - RMSE: 0.8718\n",
      "Epoch 19/50 - RMSE: 0.8690\n",
      "Epoch 20/50 - RMSE: 0.8658\n",
      "Epoch 21/50 - RMSE: 0.8626\n",
      "Epoch 22/50 - RMSE: 0.8593\n",
      "Epoch 23/50 - RMSE: 0.8559\n",
      "Epoch 24/50 - RMSE: 0.8525\n",
      "Epoch 25/50 - RMSE: 0.8489\n",
      "Epoch 26/50 - RMSE: 0.8453\n",
      "Epoch 27/50 - RMSE: 0.8417\n",
      "Epoch 28/50 - RMSE: 0.8378\n",
      "Epoch 29/50 - RMSE: 0.8341\n",
      "Epoch 30/50 - RMSE: 0.8304\n",
      "Epoch 31/50 - RMSE: 0.8264\n",
      "Epoch 32/50 - RMSE: 0.8226\n",
      "Epoch 33/50 - RMSE: 0.8187\n",
      "Epoch 34/50 - RMSE: 0.8149\n",
      "Epoch 35/50 - RMSE: 0.8112\n",
      "Epoch 36/50 - RMSE: 0.8073\n",
      "Epoch 37/50 - RMSE: 0.8036\n",
      "Epoch 38/50 - RMSE: 0.7999\n",
      "Epoch 39/50 - RMSE: 0.7963\n",
      "Epoch 40/50 - RMSE: 0.7926\n",
      "Epoch 41/50 - RMSE: 0.7890\n",
      "Epoch 42/50 - RMSE: 0.7854\n",
      "Epoch 43/50 - RMSE: 0.7818\n",
      "Epoch 44/50 - RMSE: 0.7785\n",
      "Epoch 45/50 - RMSE: 0.7749\n",
      "Epoch 46/50 - RMSE: 0.7717\n",
      "Epoch 47/50 - RMSE: 0.7683\n",
      "Epoch 48/50 - RMSE: 0.7651\n",
      "Epoch 49/50 - RMSE: 0.7619\n",
      "Epoch 50/50 - RMSE: 0.7586\n",
      "(factors=100, lr=0.005, reg=0.1, ep=50) - RMSE=0.9284029945117999\n",
      "Epoch 1/10 - RMSE: 1.0202\n",
      "Epoch 2/10 - RMSE: 0.9417\n",
      "Epoch 3/10 - RMSE: 0.9076\n",
      "Epoch 4/10 - RMSE: 0.8789\n",
      "Epoch 5/10 - RMSE: 0.8489\n",
      "Epoch 6/10 - RMSE: 0.8154\n",
      "Epoch 7/10 - RMSE: 0.7776\n",
      "Epoch 8/10 - RMSE: 0.7366\n",
      "Epoch 9/10 - RMSE: 0.6940\n",
      "Epoch 10/10 - RMSE: 0.6514\n",
      "(factors=100, lr=0.01, reg=0.001, ep=10) - RMSE=0.9706466393908845\n",
      "Epoch 1/20 - RMSE: 1.0200\n",
      "Epoch 2/20 - RMSE: 0.9415\n",
      "Epoch 3/20 - RMSE: 0.9074\n",
      "Epoch 4/20 - RMSE: 0.8784\n",
      "Epoch 5/20 - RMSE: 0.8482\n",
      "Epoch 6/20 - RMSE: 0.8145\n",
      "Epoch 7/20 - RMSE: 0.7768\n",
      "Epoch 8/20 - RMSE: 0.7361\n",
      "Epoch 9/20 - RMSE: 0.6939\n",
      "Epoch 10/20 - RMSE: 0.6519\n",
      "Epoch 11/20 - RMSE: 0.6110\n",
      "Epoch 12/20 - RMSE: 0.5722\n",
      "Epoch 13/20 - RMSE: 0.5358\n",
      "Epoch 14/20 - RMSE: 0.5021\n",
      "Epoch 15/20 - RMSE: 0.4712\n",
      "Epoch 16/20 - RMSE: 0.4428\n",
      "Epoch 17/20 - RMSE: 0.4168\n",
      "Epoch 18/20 - RMSE: 0.3931\n",
      "Epoch 19/20 - RMSE: 0.3713\n",
      "Epoch 20/20 - RMSE: 0.3515\n",
      "(factors=100, lr=0.01, reg=0.001, ep=20) - RMSE=1.0275433300617143\n",
      "Epoch 1/30 - RMSE: 1.0205\n",
      "Epoch 2/30 - RMSE: 0.9420\n",
      "Epoch 3/30 - RMSE: 0.9077\n",
      "Epoch 4/30 - RMSE: 0.8786\n",
      "Epoch 5/30 - RMSE: 0.8485\n",
      "Epoch 6/30 - RMSE: 0.8149\n",
      "Epoch 7/30 - RMSE: 0.7771\n",
      "Epoch 8/30 - RMSE: 0.7361\n",
      "Epoch 9/30 - RMSE: 0.6938\n",
      "Epoch 10/30 - RMSE: 0.6517\n",
      "Epoch 11/30 - RMSE: 0.6107\n",
      "Epoch 12/30 - RMSE: 0.5719\n",
      "Epoch 13/30 - RMSE: 0.5356\n",
      "Epoch 14/30 - RMSE: 0.5020\n",
      "Epoch 15/30 - RMSE: 0.4710\n",
      "Epoch 16/30 - RMSE: 0.4427\n",
      "Epoch 17/30 - RMSE: 0.4167\n",
      "Epoch 18/30 - RMSE: 0.3929\n",
      "Epoch 19/30 - RMSE: 0.3711\n",
      "Epoch 20/30 - RMSE: 0.3512\n",
      "Epoch 21/30 - RMSE: 0.3329\n",
      "Epoch 22/30 - RMSE: 0.3162\n",
      "Epoch 23/30 - RMSE: 0.3007\n",
      "Epoch 24/30 - RMSE: 0.2864\n",
      "Epoch 25/30 - RMSE: 0.2732\n",
      "Epoch 26/30 - RMSE: 0.2610\n",
      "Epoch 27/30 - RMSE: 0.2497\n",
      "Epoch 28/30 - RMSE: 0.2392\n",
      "Epoch 29/30 - RMSE: 0.2294\n",
      "Epoch 30/30 - RMSE: 0.2203\n",
      "(factors=100, lr=0.01, reg=0.001, ep=30) - RMSE=1.0614049363658797\n",
      "Epoch 1/40 - RMSE: 1.0202\n",
      "Epoch 2/40 - RMSE: 0.9417\n",
      "Epoch 3/40 - RMSE: 0.9072\n",
      "Epoch 4/40 - RMSE: 0.8780\n",
      "Epoch 5/40 - RMSE: 0.8479\n",
      "Epoch 6/40 - RMSE: 0.8140\n",
      "Epoch 7/40 - RMSE: 0.7760\n",
      "Epoch 8/40 - RMSE: 0.7350\n",
      "Epoch 9/40 - RMSE: 0.6924\n",
      "Epoch 10/40 - RMSE: 0.6500\n",
      "Epoch 11/40 - RMSE: 0.6089\n",
      "Epoch 12/40 - RMSE: 0.5700\n",
      "Epoch 13/40 - RMSE: 0.5336\n",
      "Epoch 14/40 - RMSE: 0.4998\n",
      "Epoch 15/40 - RMSE: 0.4687\n",
      "Epoch 16/40 - RMSE: 0.4402\n",
      "Epoch 17/40 - RMSE: 0.4143\n",
      "Epoch 18/40 - RMSE: 0.3905\n",
      "Epoch 19/40 - RMSE: 0.3689\n",
      "Epoch 20/40 - RMSE: 0.3491\n",
      "Epoch 21/40 - RMSE: 0.3309\n",
      "Epoch 22/40 - RMSE: 0.3143\n",
      "Epoch 23/40 - RMSE: 0.2990\n",
      "Epoch 24/40 - RMSE: 0.2849\n",
      "Epoch 25/40 - RMSE: 0.2719\n",
      "Epoch 26/40 - RMSE: 0.2599\n",
      "Epoch 27/40 - RMSE: 0.2487\n",
      "Epoch 28/40 - RMSE: 0.2384\n",
      "Epoch 29/40 - RMSE: 0.2287\n",
      "Epoch 30/40 - RMSE: 0.2198\n",
      "Epoch 31/40 - RMSE: 0.2114\n",
      "Epoch 32/40 - RMSE: 0.2036\n",
      "Epoch 33/40 - RMSE: 0.1963\n",
      "Epoch 34/40 - RMSE: 0.1894\n",
      "Epoch 35/40 - RMSE: 0.1829\n",
      "Epoch 36/40 - RMSE: 0.1768\n",
      "Epoch 37/40 - RMSE: 0.1711\n",
      "Epoch 38/40 - RMSE: 0.1656\n",
      "Epoch 39/40 - RMSE: 0.1606\n",
      "Epoch 40/40 - RMSE: 0.1557\n",
      "(factors=100, lr=0.01, reg=0.001, ep=40) - RMSE=1.0973522195496543\n",
      "Epoch 1/50 - RMSE: 1.0193\n",
      "Epoch 2/50 - RMSE: 0.9414\n",
      "Epoch 3/50 - RMSE: 0.9071\n",
      "Epoch 4/50 - RMSE: 0.8783\n",
      "Epoch 5/50 - RMSE: 0.8479\n",
      "Epoch 6/50 - RMSE: 0.8142\n",
      "Epoch 7/50 - RMSE: 0.7764\n",
      "Epoch 8/50 - RMSE: 0.7353\n",
      "Epoch 9/50 - RMSE: 0.6925\n",
      "Epoch 10/50 - RMSE: 0.6498\n",
      "Epoch 11/50 - RMSE: 0.6084\n",
      "Epoch 12/50 - RMSE: 0.5692\n",
      "Epoch 13/50 - RMSE: 0.5327\n",
      "Epoch 14/50 - RMSE: 0.4989\n",
      "Epoch 15/50 - RMSE: 0.4681\n",
      "Epoch 16/50 - RMSE: 0.4398\n",
      "Epoch 17/50 - RMSE: 0.4140\n",
      "Epoch 18/50 - RMSE: 0.3904\n",
      "Epoch 19/50 - RMSE: 0.3689\n",
      "Epoch 20/50 - RMSE: 0.3492\n",
      "Epoch 21/50 - RMSE: 0.3311\n",
      "Epoch 22/50 - RMSE: 0.3146\n",
      "Epoch 23/50 - RMSE: 0.2994\n",
      "Epoch 24/50 - RMSE: 0.2854\n",
      "Epoch 25/50 - RMSE: 0.2724\n",
      "Epoch 26/50 - RMSE: 0.2605\n",
      "Epoch 27/50 - RMSE: 0.2494\n",
      "Epoch 28/50 - RMSE: 0.2391\n",
      "Epoch 29/50 - RMSE: 0.2295\n",
      "Epoch 30/50 - RMSE: 0.2205\n",
      "Epoch 31/50 - RMSE: 0.2122\n",
      "Epoch 32/50 - RMSE: 0.2044\n",
      "Epoch 33/50 - RMSE: 0.1971\n",
      "Epoch 34/50 - RMSE: 0.1903\n",
      "Epoch 35/50 - RMSE: 0.1838\n",
      "Epoch 36/50 - RMSE: 0.1777\n",
      "Epoch 37/50 - RMSE: 0.1720\n",
      "Epoch 38/50 - RMSE: 0.1666\n",
      "Epoch 39/50 - RMSE: 0.1615\n",
      "Epoch 40/50 - RMSE: 0.1566\n",
      "Epoch 41/50 - RMSE: 0.1521\n",
      "Epoch 42/50 - RMSE: 0.1477\n",
      "Epoch 43/50 - RMSE: 0.1436\n",
      "Epoch 44/50 - RMSE: 0.1397\n",
      "Epoch 45/50 - RMSE: 0.1360\n",
      "Epoch 46/50 - RMSE: 0.1324\n",
      "Epoch 47/50 - RMSE: 0.1290\n",
      "Epoch 48/50 - RMSE: 0.1258\n",
      "Epoch 49/50 - RMSE: 0.1227\n",
      "Epoch 50/50 - RMSE: 0.1197\n",
      "(factors=100, lr=0.01, reg=0.001, ep=50) - RMSE=1.1093833848224863\n",
      "Epoch 1/10 - RMSE: 1.0192\n",
      "Epoch 2/10 - RMSE: 0.9420\n",
      "Epoch 3/10 - RMSE: 0.9087\n",
      "Epoch 4/10 - RMSE: 0.8820\n",
      "Epoch 5/10 - RMSE: 0.8551\n",
      "Epoch 6/10 - RMSE: 0.8256\n",
      "Epoch 7/10 - RMSE: 0.7926\n",
      "Epoch 8/10 - RMSE: 0.7566\n",
      "Epoch 9/10 - RMSE: 0.7186\n",
      "Epoch 10/10 - RMSE: 0.6802\n",
      "(factors=100, lr=0.01, reg=0.01, ep=10) - RMSE=0.9587258597212618\n",
      "Epoch 1/20 - RMSE: 1.0195\n",
      "Epoch 2/20 - RMSE: 0.9417\n",
      "Epoch 3/20 - RMSE: 0.9089\n",
      "Epoch 4/20 - RMSE: 0.8821\n",
      "Epoch 5/20 - RMSE: 0.8556\n",
      "Epoch 6/20 - RMSE: 0.8260\n",
      "Epoch 7/20 - RMSE: 0.7933\n",
      "Epoch 8/20 - RMSE: 0.7573\n",
      "Epoch 9/20 - RMSE: 0.7197\n",
      "Epoch 10/20 - RMSE: 0.6816\n",
      "Epoch 11/20 - RMSE: 0.6438\n",
      "Epoch 12/20 - RMSE: 0.6078\n",
      "Epoch 13/20 - RMSE: 0.5734\n",
      "Epoch 14/20 - RMSE: 0.5411\n",
      "Epoch 15/20 - RMSE: 0.5111\n",
      "Epoch 16/20 - RMSE: 0.4834\n",
      "Epoch 17/20 - RMSE: 0.4579\n",
      "Epoch 18/20 - RMSE: 0.4344\n",
      "Epoch 19/20 - RMSE: 0.4130\n",
      "Epoch 20/20 - RMSE: 0.3933\n",
      "(factors=100, lr=0.01, reg=0.01, ep=20) - RMSE=0.993811999165615\n",
      "Epoch 1/30 - RMSE: 1.0196\n",
      "Epoch 2/30 - RMSE: 0.9418\n",
      "Epoch 3/30 - RMSE: 0.9088\n",
      "Epoch 4/30 - RMSE: 0.8821\n",
      "Epoch 5/30 - RMSE: 0.8552\n",
      "Epoch 6/30 - RMSE: 0.8258\n",
      "Epoch 7/30 - RMSE: 0.7930\n",
      "Epoch 8/30 - RMSE: 0.7567\n",
      "Epoch 9/30 - RMSE: 0.7184\n",
      "Epoch 10/30 - RMSE: 0.6796\n",
      "Epoch 11/30 - RMSE: 0.6413\n",
      "Epoch 12/30 - RMSE: 0.6046\n",
      "Epoch 13/30 - RMSE: 0.5698\n",
      "Epoch 14/30 - RMSE: 0.5373\n",
      "Epoch 15/30 - RMSE: 0.5070\n",
      "Epoch 16/30 - RMSE: 0.4792\n",
      "Epoch 17/30 - RMSE: 0.4536\n",
      "Epoch 18/30 - RMSE: 0.4301\n",
      "Epoch 19/30 - RMSE: 0.4085\n",
      "Epoch 20/30 - RMSE: 0.3888\n",
      "Epoch 21/30 - RMSE: 0.3708\n",
      "Epoch 22/30 - RMSE: 0.3541\n",
      "Epoch 23/30 - RMSE: 0.3389\n",
      "Epoch 24/30 - RMSE: 0.3249\n",
      "Epoch 25/30 - RMSE: 0.3120\n",
      "Epoch 26/30 - RMSE: 0.3001\n",
      "Epoch 27/30 - RMSE: 0.2892\n",
      "Epoch 28/30 - RMSE: 0.2791\n",
      "Epoch 29/30 - RMSE: 0.2697\n",
      "Epoch 30/30 - RMSE: 0.2610\n",
      "(factors=100, lr=0.01, reg=0.01, ep=30) - RMSE=1.0190932339254084\n",
      "Epoch 1/40 - RMSE: 1.0197\n",
      "Epoch 2/40 - RMSE: 0.9419\n",
      "Epoch 3/40 - RMSE: 0.9092\n",
      "Epoch 4/40 - RMSE: 0.8822\n",
      "Epoch 5/40 - RMSE: 0.8555\n",
      "Epoch 6/40 - RMSE: 0.8260\n",
      "Epoch 7/40 - RMSE: 0.7929\n",
      "Epoch 8/40 - RMSE: 0.7570\n",
      "Epoch 9/40 - RMSE: 0.7189\n",
      "Epoch 10/40 - RMSE: 0.6805\n",
      "Epoch 11/40 - RMSE: 0.6426\n",
      "Epoch 12/40 - RMSE: 0.6063\n",
      "Epoch 13/40 - RMSE: 0.5718\n",
      "Epoch 14/40 - RMSE: 0.5395\n",
      "Epoch 15/40 - RMSE: 0.5098\n",
      "Epoch 16/40 - RMSE: 0.4822\n",
      "Epoch 17/40 - RMSE: 0.4569\n",
      "Epoch 18/40 - RMSE: 0.4337\n",
      "Epoch 19/40 - RMSE: 0.4124\n",
      "Epoch 20/40 - RMSE: 0.3929\n",
      "Epoch 21/40 - RMSE: 0.3749\n",
      "Epoch 22/40 - RMSE: 0.3585\n",
      "Epoch 23/40 - RMSE: 0.3433\n",
      "Epoch 24/40 - RMSE: 0.3293\n",
      "Epoch 25/40 - RMSE: 0.3165\n",
      "Epoch 26/40 - RMSE: 0.3046\n",
      "Epoch 27/40 - RMSE: 0.2936\n",
      "Epoch 28/40 - RMSE: 0.2835\n",
      "Epoch 29/40 - RMSE: 0.2741\n",
      "Epoch 30/40 - RMSE: 0.2653\n",
      "Epoch 31/40 - RMSE: 0.2572\n",
      "Epoch 32/40 - RMSE: 0.2496\n",
      "Epoch 33/40 - RMSE: 0.2426\n",
      "Epoch 34/40 - RMSE: 0.2360\n",
      "Epoch 35/40 - RMSE: 0.2298\n",
      "Epoch 36/40 - RMSE: 0.2240\n",
      "Epoch 37/40 - RMSE: 0.2186\n",
      "Epoch 38/40 - RMSE: 0.2136\n",
      "Epoch 39/40 - RMSE: 0.2087\n",
      "Epoch 40/40 - RMSE: 0.2043\n",
      "(factors=100, lr=0.01, reg=0.01, ep=40) - RMSE=1.0246647818335324\n",
      "Epoch 1/50 - RMSE: 1.0200\n",
      "Epoch 2/50 - RMSE: 0.9419\n",
      "Epoch 3/50 - RMSE: 0.9088\n",
      "Epoch 4/50 - RMSE: 0.8821\n",
      "Epoch 5/50 - RMSE: 0.8553\n",
      "Epoch 6/50 - RMSE: 0.8261\n",
      "Epoch 7/50 - RMSE: 0.7931\n",
      "Epoch 8/50 - RMSE: 0.7572\n",
      "Epoch 9/50 - RMSE: 0.7194\n",
      "Epoch 10/50 - RMSE: 0.6811\n",
      "Epoch 11/50 - RMSE: 0.6434\n",
      "Epoch 12/50 - RMSE: 0.6070\n",
      "Epoch 13/50 - RMSE: 0.5726\n",
      "Epoch 14/50 - RMSE: 0.5402\n",
      "Epoch 15/50 - RMSE: 0.5100\n",
      "Epoch 16/50 - RMSE: 0.4823\n",
      "Epoch 17/50 - RMSE: 0.4567\n",
      "Epoch 18/50 - RMSE: 0.4333\n",
      "Epoch 19/50 - RMSE: 0.4117\n",
      "Epoch 20/50 - RMSE: 0.3921\n",
      "Epoch 21/50 - RMSE: 0.3740\n",
      "Epoch 22/50 - RMSE: 0.3575\n",
      "Epoch 23/50 - RMSE: 0.3423\n",
      "Epoch 24/50 - RMSE: 0.3283\n",
      "Epoch 25/50 - RMSE: 0.3154\n",
      "Epoch 26/50 - RMSE: 0.3036\n",
      "Epoch 27/50 - RMSE: 0.2926\n",
      "Epoch 28/50 - RMSE: 0.2825\n",
      "Epoch 29/50 - RMSE: 0.2731\n",
      "Epoch 30/50 - RMSE: 0.2644\n",
      "Epoch 31/50 - RMSE: 0.2562\n",
      "Epoch 32/50 - RMSE: 0.2487\n",
      "Epoch 33/50 - RMSE: 0.2417\n",
      "Epoch 34/50 - RMSE: 0.2351\n",
      "Epoch 35/50 - RMSE: 0.2290\n",
      "Epoch 36/50 - RMSE: 0.2232\n",
      "Epoch 37/50 - RMSE: 0.2179\n",
      "Epoch 38/50 - RMSE: 0.2128\n",
      "Epoch 39/50 - RMSE: 0.2080\n",
      "Epoch 40/50 - RMSE: 0.2036\n",
      "Epoch 41/50 - RMSE: 0.1994\n",
      "Epoch 42/50 - RMSE: 0.1954\n",
      "Epoch 43/50 - RMSE: 0.1916\n",
      "Epoch 44/50 - RMSE: 0.1882\n",
      "Epoch 45/50 - RMSE: 0.1849\n",
      "Epoch 46/50 - RMSE: 0.1817\n",
      "Epoch 47/50 - RMSE: 0.1787\n",
      "Epoch 48/50 - RMSE: 0.1759\n",
      "Epoch 49/50 - RMSE: 0.1732\n",
      "Epoch 50/50 - RMSE: 0.1706\n",
      "(factors=100, lr=0.01, reg=0.01, ep=50) - RMSE=1.0335044056722995\n",
      "Epoch 1/10 - RMSE: 1.0207\n",
      "Epoch 2/10 - RMSE: 0.9476\n",
      "Epoch 3/10 - RMSE: 0.9243\n",
      "Epoch 4/10 - RMSE: 0.9114\n",
      "Epoch 5/10 - RMSE: 0.9023\n",
      "Epoch 6/10 - RMSE: 0.8953\n",
      "Epoch 7/10 - RMSE: 0.8890\n",
      "Epoch 8/10 - RMSE: 0.8830\n",
      "Epoch 9/10 - RMSE: 0.8770\n",
      "Epoch 10/10 - RMSE: 0.8709\n",
      "(factors=100, lr=0.01, reg=0.1, ep=10) - RMSE=0.9529946778900432\n",
      "Epoch 1/20 - RMSE: 1.0203\n",
      "Epoch 2/20 - RMSE: 0.9476\n",
      "Epoch 3/20 - RMSE: 0.9244\n",
      "Epoch 4/20 - RMSE: 0.9112\n",
      "Epoch 5/20 - RMSE: 0.9025\n",
      "Epoch 6/20 - RMSE: 0.8951\n",
      "Epoch 7/20 - RMSE: 0.8891\n",
      "Epoch 8/20 - RMSE: 0.8831\n",
      "Epoch 9/20 - RMSE: 0.8771\n",
      "Epoch 10/20 - RMSE: 0.8710\n",
      "Epoch 11/20 - RMSE: 0.8643\n",
      "Epoch 12/20 - RMSE: 0.8575\n",
      "Epoch 13/20 - RMSE: 0.8504\n",
      "Epoch 14/20 - RMSE: 0.8428\n",
      "Epoch 15/20 - RMSE: 0.8352\n",
      "Epoch 16/20 - RMSE: 0.8277\n",
      "Epoch 17/20 - RMSE: 0.8202\n",
      "Epoch 18/20 - RMSE: 0.8126\n",
      "Epoch 19/20 - RMSE: 0.8053\n",
      "Epoch 20/20 - RMSE: 0.7979\n",
      "(factors=100, lr=0.01, reg=0.1, ep=20) - RMSE=0.9333913978517857\n",
      "Epoch 1/30 - RMSE: 1.0202\n",
      "Epoch 2/30 - RMSE: 0.9471\n",
      "Epoch 3/30 - RMSE: 0.9238\n",
      "Epoch 4/30 - RMSE: 0.9108\n",
      "Epoch 5/30 - RMSE: 0.9014\n",
      "Epoch 6/30 - RMSE: 0.8944\n",
      "Epoch 7/30 - RMSE: 0.8879\n",
      "Epoch 8/30 - RMSE: 0.8819\n",
      "Epoch 9/30 - RMSE: 0.8758\n",
      "Epoch 10/30 - RMSE: 0.8690\n",
      "Epoch 11/30 - RMSE: 0.8624\n",
      "Epoch 12/30 - RMSE: 0.8554\n",
      "Epoch 13/30 - RMSE: 0.8482\n",
      "Epoch 14/30 - RMSE: 0.8409\n",
      "Epoch 15/30 - RMSE: 0.8335\n",
      "Epoch 16/30 - RMSE: 0.8259\n",
      "Epoch 17/30 - RMSE: 0.8187\n",
      "Epoch 18/30 - RMSE: 0.8113\n",
      "Epoch 19/30 - RMSE: 0.8040\n",
      "Epoch 20/30 - RMSE: 0.7966\n",
      "Epoch 21/30 - RMSE: 0.7897\n",
      "Epoch 22/30 - RMSE: 0.7828\n",
      "Epoch 23/30 - RMSE: 0.7761\n",
      "Epoch 24/30 - RMSE: 0.7692\n",
      "Epoch 25/30 - RMSE: 0.7628\n",
      "Epoch 26/30 - RMSE: 0.7567\n",
      "Epoch 27/30 - RMSE: 0.7507\n",
      "Epoch 28/30 - RMSE: 0.7448\n",
      "Epoch 29/30 - RMSE: 0.7394\n",
      "Epoch 30/30 - RMSE: 0.7340\n",
      "(factors=100, lr=0.01, reg=0.1, ep=30) - RMSE=0.9259165151410936\n",
      "Epoch 1/40 - RMSE: 1.0208\n",
      "Epoch 2/40 - RMSE: 0.9479\n",
      "Epoch 3/40 - RMSE: 0.9247\n",
      "Epoch 4/40 - RMSE: 0.9116\n",
      "Epoch 5/40 - RMSE: 0.9025\n",
      "Epoch 6/40 - RMSE: 0.8955\n",
      "Epoch 7/40 - RMSE: 0.8894\n",
      "Epoch 8/40 - RMSE: 0.8834\n",
      "Epoch 9/40 - RMSE: 0.8773\n",
      "Epoch 10/40 - RMSE: 0.8712\n",
      "Epoch 11/40 - RMSE: 0.8646\n",
      "Epoch 12/40 - RMSE: 0.8577\n",
      "Epoch 13/40 - RMSE: 0.8504\n",
      "Epoch 14/40 - RMSE: 0.8430\n",
      "Epoch 15/40 - RMSE: 0.8354\n",
      "Epoch 16/40 - RMSE: 0.8278\n",
      "Epoch 17/40 - RMSE: 0.8200\n",
      "Epoch 18/40 - RMSE: 0.8125\n",
      "Epoch 19/40 - RMSE: 0.8051\n",
      "Epoch 20/40 - RMSE: 0.7976\n",
      "Epoch 21/40 - RMSE: 0.7905\n",
      "Epoch 22/40 - RMSE: 0.7834\n",
      "Epoch 23/40 - RMSE: 0.7765\n",
      "Epoch 24/40 - RMSE: 0.7698\n",
      "Epoch 25/40 - RMSE: 0.7633\n",
      "Epoch 26/40 - RMSE: 0.7571\n",
      "Epoch 27/40 - RMSE: 0.7511\n",
      "Epoch 28/40 - RMSE: 0.7454\n",
      "Epoch 29/40 - RMSE: 0.7398\n",
      "Epoch 30/40 - RMSE: 0.7344\n",
      "Epoch 31/40 - RMSE: 0.7294\n",
      "Epoch 32/40 - RMSE: 0.7246\n",
      "Epoch 33/40 - RMSE: 0.7201\n",
      "Epoch 34/40 - RMSE: 0.7156\n",
      "Epoch 35/40 - RMSE: 0.7115\n",
      "Epoch 36/40 - RMSE: 0.7073\n",
      "Epoch 37/40 - RMSE: 0.7035\n",
      "Epoch 38/40 - RMSE: 0.7000\n",
      "Epoch 39/40 - RMSE: 0.6965\n",
      "Epoch 40/40 - RMSE: 0.6932\n",
      "(factors=100, lr=0.01, reg=0.1, ep=40) - RMSE=0.9232255060327957\n",
      "Epoch 1/50 - RMSE: 1.0203\n",
      "Epoch 2/50 - RMSE: 0.9474\n",
      "Epoch 3/50 - RMSE: 0.9241\n",
      "Epoch 4/50 - RMSE: 0.9109\n",
      "Epoch 5/50 - RMSE: 0.9020\n",
      "Epoch 6/50 - RMSE: 0.8947\n",
      "Epoch 7/50 - RMSE: 0.8884\n",
      "Epoch 8/50 - RMSE: 0.8826\n",
      "Epoch 9/50 - RMSE: 0.8763\n",
      "Epoch 10/50 - RMSE: 0.8702\n",
      "Epoch 11/50 - RMSE: 0.8636\n",
      "Epoch 12/50 - RMSE: 0.8565\n",
      "Epoch 13/50 - RMSE: 0.8495\n",
      "Epoch 14/50 - RMSE: 0.8417\n",
      "Epoch 15/50 - RMSE: 0.8343\n",
      "Epoch 16/50 - RMSE: 0.8270\n",
      "Epoch 17/50 - RMSE: 0.8192\n",
      "Epoch 18/50 - RMSE: 0.8118\n",
      "Epoch 19/50 - RMSE: 0.8043\n",
      "Epoch 20/50 - RMSE: 0.7970\n",
      "Epoch 21/50 - RMSE: 0.7897\n",
      "Epoch 22/50 - RMSE: 0.7828\n",
      "Epoch 23/50 - RMSE: 0.7758\n",
      "Epoch 24/50 - RMSE: 0.7692\n",
      "Epoch 25/50 - RMSE: 0.7626\n",
      "Epoch 26/50 - RMSE: 0.7563\n",
      "Epoch 27/50 - RMSE: 0.7501\n",
      "Epoch 28/50 - RMSE: 0.7443\n",
      "Epoch 29/50 - RMSE: 0.7390\n",
      "Epoch 30/50 - RMSE: 0.7333\n",
      "Epoch 31/50 - RMSE: 0.7284\n",
      "Epoch 32/50 - RMSE: 0.7235\n",
      "Epoch 33/50 - RMSE: 0.7185\n",
      "Epoch 34/50 - RMSE: 0.7141\n",
      "Epoch 35/50 - RMSE: 0.7099\n",
      "Epoch 36/50 - RMSE: 0.7061\n",
      "Epoch 37/50 - RMSE: 0.7020\n",
      "Epoch 38/50 - RMSE: 0.6986\n",
      "Epoch 39/50 - RMSE: 0.6952\n",
      "Epoch 40/50 - RMSE: 0.6917\n",
      "Epoch 41/50 - RMSE: 0.6885\n",
      "Epoch 42/50 - RMSE: 0.6853\n",
      "Epoch 43/50 - RMSE: 0.6826\n",
      "Epoch 44/50 - RMSE: 0.6798\n",
      "Epoch 45/50 - RMSE: 0.6773\n",
      "Epoch 46/50 - RMSE: 0.6750\n",
      "Epoch 47/50 - RMSE: 0.6728\n",
      "Epoch 48/50 - RMSE: 0.6703\n",
      "Epoch 49/50 - RMSE: 0.6681\n",
      "Epoch 50/50 - RMSE: 0.6662\n",
      "(factors=100, lr=0.01, reg=0.1, ep=50) - RMSE=0.9232607854907892\n",
      "Epoch 1/10 - RMSE: nan\n",
      "Epoch 2/10 - RMSE: nan\n",
      "Epoch 3/10 - RMSE: nan\n",
      "Epoch 4/10 - RMSE: nan\n",
      "Epoch 5/10 - RMSE: nan\n",
      "Epoch 6/10 - RMSE: nan\n",
      "Epoch 7/10 - RMSE: nan\n",
      "Epoch 8/10 - RMSE: nan\n",
      "Epoch 9/10 - RMSE: nan\n",
      "Epoch 10/10 - RMSE: nan\n",
      "(factors=100, lr=0.2, reg=0.001, ep=10) - RMSE=nan\n",
      "Epoch 1/20 - RMSE: nan\n",
      "Epoch 2/20 - RMSE: nan\n",
      "Epoch 3/20 - RMSE: nan\n",
      "Epoch 4/20 - RMSE: nan\n",
      "Epoch 5/20 - RMSE: nan\n",
      "Epoch 6/20 - RMSE: nan\n",
      "Epoch 7/20 - RMSE: nan\n",
      "Epoch 8/20 - RMSE: nan\n",
      "Epoch 9/20 - RMSE: nan\n",
      "Epoch 10/20 - RMSE: nan\n",
      "Epoch 11/20 - RMSE: nan\n",
      "Epoch 12/20 - RMSE: nan\n",
      "Epoch 13/20 - RMSE: nan\n",
      "Epoch 14/20 - RMSE: nan\n",
      "Epoch 15/20 - RMSE: nan\n",
      "Epoch 16/20 - RMSE: nan\n",
      "Epoch 17/20 - RMSE: nan\n",
      "Epoch 18/20 - RMSE: nan\n",
      "Epoch 19/20 - RMSE: nan\n",
      "Epoch 20/20 - RMSE: nan\n",
      "(factors=100, lr=0.2, reg=0.001, ep=20) - RMSE=nan\n",
      "Epoch 1/30 - RMSE: nan\n",
      "Epoch 2/30 - RMSE: nan\n",
      "Epoch 3/30 - RMSE: nan\n",
      "Epoch 4/30 - RMSE: nan\n",
      "Epoch 5/30 - RMSE: nan\n",
      "Epoch 6/30 - RMSE: nan\n",
      "Epoch 7/30 - RMSE: nan\n",
      "Epoch 8/30 - RMSE: nan\n",
      "Epoch 9/30 - RMSE: nan\n",
      "Epoch 10/30 - RMSE: nan\n",
      "Epoch 11/30 - RMSE: nan\n",
      "Epoch 12/30 - RMSE: nan\n",
      "Epoch 13/30 - RMSE: nan\n",
      "Epoch 14/30 - RMSE: nan\n",
      "Epoch 15/30 - RMSE: nan\n",
      "Epoch 16/30 - RMSE: nan\n",
      "Epoch 17/30 - RMSE: nan\n",
      "Epoch 18/30 - RMSE: nan\n",
      "Epoch 19/30 - RMSE: nan\n",
      "Epoch 20/30 - RMSE: nan\n",
      "Epoch 21/30 - RMSE: nan\n",
      "Epoch 22/30 - RMSE: nan\n",
      "Epoch 23/30 - RMSE: nan\n",
      "Epoch 24/30 - RMSE: nan\n",
      "Epoch 25/30 - RMSE: nan\n",
      "Epoch 26/30 - RMSE: nan\n",
      "Epoch 27/30 - RMSE: nan\n",
      "Epoch 28/30 - RMSE: nan\n",
      "Epoch 29/30 - RMSE: nan\n",
      "Epoch 30/30 - RMSE: nan\n",
      "(factors=100, lr=0.2, reg=0.001, ep=30) - RMSE=nan\n",
      "Epoch 1/40 - RMSE: nan\n",
      "Epoch 2/40 - RMSE: nan\n",
      "Epoch 3/40 - RMSE: nan\n",
      "Epoch 4/40 - RMSE: nan\n",
      "Epoch 5/40 - RMSE: nan\n",
      "Epoch 6/40 - RMSE: nan\n",
      "Epoch 7/40 - RMSE: nan\n",
      "Epoch 8/40 - RMSE: nan\n",
      "Epoch 9/40 - RMSE: nan\n",
      "Epoch 10/40 - RMSE: nan\n",
      "Epoch 11/40 - RMSE: nan\n",
      "Epoch 12/40 - RMSE: nan\n",
      "Epoch 13/40 - RMSE: nan\n",
      "Epoch 14/40 - RMSE: nan\n",
      "Epoch 15/40 - RMSE: nan\n",
      "Epoch 16/40 - RMSE: nan\n",
      "Epoch 17/40 - RMSE: nan\n",
      "Epoch 18/40 - RMSE: nan\n",
      "Epoch 19/40 - RMSE: nan\n",
      "Epoch 20/40 - RMSE: nan\n",
      "Epoch 21/40 - RMSE: nan\n",
      "Epoch 22/40 - RMSE: nan\n",
      "Epoch 23/40 - RMSE: nan\n",
      "Epoch 24/40 - RMSE: nan\n",
      "Epoch 25/40 - RMSE: nan\n",
      "Epoch 26/40 - RMSE: nan\n",
      "Epoch 27/40 - RMSE: nan\n",
      "Epoch 28/40 - RMSE: nan\n",
      "Epoch 29/40 - RMSE: nan\n",
      "Epoch 30/40 - RMSE: nan\n",
      "Epoch 31/40 - RMSE: nan\n",
      "Epoch 32/40 - RMSE: nan\n",
      "Epoch 33/40 - RMSE: nan\n",
      "Epoch 34/40 - RMSE: nan\n",
      "Epoch 35/40 - RMSE: nan\n",
      "Epoch 36/40 - RMSE: nan\n",
      "Epoch 37/40 - RMSE: nan\n",
      "Epoch 38/40 - RMSE: nan\n",
      "Epoch 39/40 - RMSE: nan\n",
      "Epoch 40/40 - RMSE: nan\n",
      "(factors=100, lr=0.2, reg=0.001, ep=40) - RMSE=nan\n",
      "Epoch 1/50 - RMSE: nan\n",
      "Epoch 2/50 - RMSE: nan\n",
      "Epoch 3/50 - RMSE: nan\n",
      "Epoch 4/50 - RMSE: nan\n",
      "Epoch 5/50 - RMSE: nan\n",
      "Epoch 6/50 - RMSE: nan\n",
      "Epoch 7/50 - RMSE: nan\n",
      "Epoch 8/50 - RMSE: nan\n",
      "Epoch 9/50 - RMSE: nan\n",
      "Epoch 10/50 - RMSE: nan\n",
      "Epoch 11/50 - RMSE: nan\n",
      "Epoch 12/50 - RMSE: nan\n",
      "Epoch 13/50 - RMSE: nan\n",
      "Epoch 14/50 - RMSE: nan\n",
      "Epoch 15/50 - RMSE: nan\n",
      "Epoch 16/50 - RMSE: nan\n",
      "Epoch 17/50 - RMSE: nan\n",
      "Epoch 18/50 - RMSE: nan\n",
      "Epoch 19/50 - RMSE: nan\n",
      "Epoch 20/50 - RMSE: nan\n",
      "Epoch 21/50 - RMSE: nan\n",
      "Epoch 22/50 - RMSE: nan\n",
      "Epoch 23/50 - RMSE: nan\n",
      "Epoch 24/50 - RMSE: nan\n",
      "Epoch 25/50 - RMSE: nan\n",
      "Epoch 26/50 - RMSE: nan\n",
      "Epoch 27/50 - RMSE: nan\n",
      "Epoch 28/50 - RMSE: nan\n",
      "Epoch 29/50 - RMSE: nan\n",
      "Epoch 30/50 - RMSE: nan\n",
      "Epoch 31/50 - RMSE: nan\n",
      "Epoch 32/50 - RMSE: nan\n",
      "Epoch 33/50 - RMSE: nan\n",
      "Epoch 34/50 - RMSE: nan\n",
      "Epoch 35/50 - RMSE: nan\n",
      "Epoch 36/50 - RMSE: nan\n",
      "Epoch 37/50 - RMSE: nan\n",
      "Epoch 38/50 - RMSE: nan\n",
      "Epoch 39/50 - RMSE: nan\n",
      "Epoch 40/50 - RMSE: nan\n",
      "Epoch 41/50 - RMSE: nan\n",
      "Epoch 42/50 - RMSE: nan\n",
      "Epoch 43/50 - RMSE: nan\n",
      "Epoch 44/50 - RMSE: nan\n",
      "Epoch 45/50 - RMSE: nan\n",
      "Epoch 46/50 - RMSE: nan\n",
      "Epoch 47/50 - RMSE: nan\n",
      "Epoch 48/50 - RMSE: nan\n",
      "Epoch 49/50 - RMSE: nan\n",
      "Epoch 50/50 - RMSE: nan\n",
      "(factors=100, lr=0.2, reg=0.001, ep=50) - RMSE=nan\n",
      "Epoch 1/10 - RMSE: nan\n",
      "Epoch 2/10 - RMSE: nan\n",
      "Epoch 3/10 - RMSE: nan\n",
      "Epoch 4/10 - RMSE: nan\n",
      "Epoch 5/10 - RMSE: nan\n",
      "Epoch 6/10 - RMSE: nan\n",
      "Epoch 7/10 - RMSE: nan\n",
      "Epoch 8/10 - RMSE: nan\n",
      "Epoch 9/10 - RMSE: nan\n",
      "Epoch 10/10 - RMSE: nan\n",
      "(factors=100, lr=0.2, reg=0.01, ep=10) - RMSE=nan\n",
      "Epoch 1/20 - RMSE: nan\n",
      "Epoch 2/20 - RMSE: nan\n",
      "Epoch 3/20 - RMSE: nan\n",
      "Epoch 4/20 - RMSE: nan\n",
      "Epoch 5/20 - RMSE: nan\n",
      "Epoch 6/20 - RMSE: nan\n",
      "Epoch 7/20 - RMSE: nan\n",
      "Epoch 8/20 - RMSE: nan\n",
      "Epoch 9/20 - RMSE: nan\n",
      "Epoch 10/20 - RMSE: nan\n",
      "Epoch 11/20 - RMSE: nan\n",
      "Epoch 12/20 - RMSE: nan\n",
      "Epoch 13/20 - RMSE: nan\n",
      "Epoch 14/20 - RMSE: nan\n",
      "Epoch 15/20 - RMSE: nan\n",
      "Epoch 16/20 - RMSE: nan\n",
      "Epoch 17/20 - RMSE: nan\n",
      "Epoch 18/20 - RMSE: nan\n",
      "Epoch 19/20 - RMSE: nan\n",
      "Epoch 20/20 - RMSE: nan\n",
      "(factors=100, lr=0.2, reg=0.01, ep=20) - RMSE=nan\n",
      "Epoch 1/30 - RMSE: nan\n",
      "Epoch 2/30 - RMSE: nan\n",
      "Epoch 3/30 - RMSE: nan\n",
      "Epoch 4/30 - RMSE: nan\n",
      "Epoch 5/30 - RMSE: nan\n",
      "Epoch 6/30 - RMSE: nan\n",
      "Epoch 7/30 - RMSE: nan\n",
      "Epoch 8/30 - RMSE: nan\n",
      "Epoch 9/30 - RMSE: nan\n",
      "Epoch 10/30 - RMSE: nan\n",
      "Epoch 11/30 - RMSE: nan\n",
      "Epoch 12/30 - RMSE: nan\n",
      "Epoch 13/30 - RMSE: nan\n",
      "Epoch 14/30 - RMSE: nan\n",
      "Epoch 15/30 - RMSE: nan\n",
      "Epoch 16/30 - RMSE: nan\n",
      "Epoch 17/30 - RMSE: nan\n",
      "Epoch 18/30 - RMSE: nan\n",
      "Epoch 19/30 - RMSE: nan\n",
      "Epoch 20/30 - RMSE: nan\n",
      "Epoch 21/30 - RMSE: nan\n",
      "Epoch 22/30 - RMSE: nan\n",
      "Epoch 23/30 - RMSE: nan\n",
      "Epoch 24/30 - RMSE: nan\n",
      "Epoch 25/30 - RMSE: nan\n",
      "Epoch 26/30 - RMSE: nan\n",
      "Epoch 27/30 - RMSE: nan\n",
      "Epoch 28/30 - RMSE: nan\n",
      "Epoch 29/30 - RMSE: nan\n",
      "Epoch 30/30 - RMSE: nan\n",
      "(factors=100, lr=0.2, reg=0.01, ep=30) - RMSE=nan\n",
      "Epoch 1/40 - RMSE: nan\n",
      "Epoch 2/40 - RMSE: nan\n",
      "Epoch 3/40 - RMSE: nan\n",
      "Epoch 4/40 - RMSE: nan\n",
      "Epoch 5/40 - RMSE: nan\n",
      "Epoch 6/40 - RMSE: nan\n",
      "Epoch 7/40 - RMSE: nan\n",
      "Epoch 8/40 - RMSE: nan\n",
      "Epoch 9/40 - RMSE: nan\n",
      "Epoch 10/40 - RMSE: nan\n",
      "Epoch 11/40 - RMSE: nan\n",
      "Epoch 12/40 - RMSE: nan\n",
      "Epoch 13/40 - RMSE: nan\n",
      "Epoch 14/40 - RMSE: nan\n",
      "Epoch 15/40 - RMSE: nan\n",
      "Epoch 16/40 - RMSE: nan\n",
      "Epoch 17/40 - RMSE: nan\n",
      "Epoch 18/40 - RMSE: nan\n",
      "Epoch 19/40 - RMSE: nan\n",
      "Epoch 20/40 - RMSE: nan\n",
      "Epoch 21/40 - RMSE: nan\n",
      "Epoch 22/40 - RMSE: nan\n",
      "Epoch 23/40 - RMSE: nan\n",
      "Epoch 24/40 - RMSE: nan\n",
      "Epoch 25/40 - RMSE: nan\n",
      "Epoch 26/40 - RMSE: nan\n",
      "Epoch 27/40 - RMSE: nan\n",
      "Epoch 28/40 - RMSE: nan\n",
      "Epoch 29/40 - RMSE: nan\n",
      "Epoch 30/40 - RMSE: nan\n",
      "Epoch 31/40 - RMSE: nan\n",
      "Epoch 32/40 - RMSE: nan\n",
      "Epoch 33/40 - RMSE: nan\n",
      "Epoch 34/40 - RMSE: nan\n",
      "Epoch 35/40 - RMSE: nan\n",
      "Epoch 36/40 - RMSE: nan\n",
      "Epoch 37/40 - RMSE: nan\n",
      "Epoch 38/40 - RMSE: nan\n",
      "Epoch 39/40 - RMSE: nan\n",
      "Epoch 40/40 - RMSE: nan\n",
      "(factors=100, lr=0.2, reg=0.01, ep=40) - RMSE=nan\n",
      "Epoch 1/50 - RMSE: nan\n",
      "Epoch 2/50 - RMSE: nan\n",
      "Epoch 3/50 - RMSE: nan\n",
      "Epoch 4/50 - RMSE: nan\n",
      "Epoch 5/50 - RMSE: nan\n",
      "Epoch 6/50 - RMSE: nan\n",
      "Epoch 7/50 - RMSE: nan\n",
      "Epoch 8/50 - RMSE: nan\n",
      "Epoch 9/50 - RMSE: nan\n",
      "Epoch 10/50 - RMSE: nan\n",
      "Epoch 11/50 - RMSE: nan\n",
      "Epoch 12/50 - RMSE: nan\n",
      "Epoch 13/50 - RMSE: nan\n",
      "Epoch 14/50 - RMSE: nan\n",
      "Epoch 15/50 - RMSE: nan\n",
      "Epoch 16/50 - RMSE: nan\n",
      "Epoch 17/50 - RMSE: nan\n",
      "Epoch 18/50 - RMSE: nan\n",
      "Epoch 19/50 - RMSE: nan\n",
      "Epoch 20/50 - RMSE: nan\n",
      "Epoch 21/50 - RMSE: nan\n",
      "Epoch 22/50 - RMSE: nan\n",
      "Epoch 23/50 - RMSE: nan\n",
      "Epoch 24/50 - RMSE: nan\n",
      "Epoch 25/50 - RMSE: nan\n",
      "Epoch 26/50 - RMSE: nan\n",
      "Epoch 27/50 - RMSE: nan\n",
      "Epoch 28/50 - RMSE: nan\n",
      "Epoch 29/50 - RMSE: nan\n",
      "Epoch 30/50 - RMSE: nan\n",
      "Epoch 31/50 - RMSE: nan\n",
      "Epoch 32/50 - RMSE: nan\n",
      "Epoch 33/50 - RMSE: nan\n",
      "Epoch 34/50 - RMSE: nan\n",
      "Epoch 35/50 - RMSE: nan\n",
      "Epoch 36/50 - RMSE: nan\n",
      "Epoch 37/50 - RMSE: nan\n",
      "Epoch 38/50 - RMSE: nan\n",
      "Epoch 39/50 - RMSE: nan\n",
      "Epoch 40/50 - RMSE: nan\n",
      "Epoch 41/50 - RMSE: nan\n",
      "Epoch 42/50 - RMSE: nan\n",
      "Epoch 43/50 - RMSE: nan\n",
      "Epoch 44/50 - RMSE: nan\n",
      "Epoch 45/50 - RMSE: nan\n",
      "Epoch 46/50 - RMSE: nan\n",
      "Epoch 47/50 - RMSE: nan\n",
      "Epoch 48/50 - RMSE: nan\n",
      "Epoch 49/50 - RMSE: nan\n",
      "Epoch 50/50 - RMSE: nan\n",
      "(factors=100, lr=0.2, reg=0.01, ep=50) - RMSE=nan\n",
      "Epoch 1/10 - RMSE: nan\n",
      "Epoch 2/10 - RMSE: nan\n",
      "Epoch 3/10 - RMSE: nan\n",
      "Epoch 4/10 - RMSE: nan\n",
      "Epoch 5/10 - RMSE: nan\n",
      "Epoch 6/10 - RMSE: nan\n",
      "Epoch 7/10 - RMSE: nan\n",
      "Epoch 8/10 - RMSE: nan\n",
      "Epoch 9/10 - RMSE: nan\n",
      "Epoch 10/10 - RMSE: nan\n",
      "(factors=100, lr=0.2, reg=0.1, ep=10) - RMSE=nan\n",
      "Epoch 1/20 - RMSE: nan\n",
      "Epoch 2/20 - RMSE: nan\n",
      "Epoch 3/20 - RMSE: nan\n",
      "Epoch 4/20 - RMSE: nan\n",
      "Epoch 5/20 - RMSE: nan\n",
      "Epoch 6/20 - RMSE: nan\n",
      "Epoch 7/20 - RMSE: nan\n",
      "Epoch 8/20 - RMSE: nan\n",
      "Epoch 9/20 - RMSE: nan\n",
      "Epoch 10/20 - RMSE: nan\n",
      "Epoch 11/20 - RMSE: nan\n",
      "Epoch 12/20 - RMSE: nan\n",
      "Epoch 13/20 - RMSE: nan\n",
      "Epoch 14/20 - RMSE: nan\n",
      "Epoch 15/20 - RMSE: nan\n",
      "Epoch 16/20 - RMSE: nan\n",
      "Epoch 17/20 - RMSE: nan\n",
      "Epoch 18/20 - RMSE: nan\n",
      "Epoch 19/20 - RMSE: nan\n",
      "Epoch 20/20 - RMSE: nan\n",
      "(factors=100, lr=0.2, reg=0.1, ep=20) - RMSE=nan\n",
      "Epoch 1/30 - RMSE: nan\n",
      "Epoch 2/30 - RMSE: nan\n",
      "Epoch 3/30 - RMSE: nan\n",
      "Epoch 4/30 - RMSE: nan\n",
      "Epoch 5/30 - RMSE: nan\n",
      "Epoch 6/30 - RMSE: nan\n",
      "Epoch 7/30 - RMSE: nan\n",
      "Epoch 8/30 - RMSE: nan\n",
      "Epoch 9/30 - RMSE: nan\n",
      "Epoch 10/30 - RMSE: nan\n",
      "Epoch 11/30 - RMSE: nan\n",
      "Epoch 12/30 - RMSE: nan\n",
      "Epoch 13/30 - RMSE: nan\n",
      "Epoch 14/30 - RMSE: nan\n",
      "Epoch 15/30 - RMSE: nan\n",
      "Epoch 16/30 - RMSE: nan\n",
      "Epoch 17/30 - RMSE: nan\n",
      "Epoch 18/30 - RMSE: nan\n",
      "Epoch 19/30 - RMSE: nan\n",
      "Epoch 20/30 - RMSE: nan\n",
      "Epoch 21/30 - RMSE: nan\n",
      "Epoch 22/30 - RMSE: nan\n",
      "Epoch 23/30 - RMSE: nan\n",
      "Epoch 24/30 - RMSE: nan\n",
      "Epoch 25/30 - RMSE: nan\n",
      "Epoch 26/30 - RMSE: nan\n",
      "Epoch 27/30 - RMSE: nan\n",
      "Epoch 28/30 - RMSE: nan\n",
      "Epoch 29/30 - RMSE: nan\n",
      "Epoch 30/30 - RMSE: nan\n",
      "(factors=100, lr=0.2, reg=0.1, ep=30) - RMSE=nan\n",
      "Epoch 1/40 - RMSE: nan\n",
      "Epoch 2/40 - RMSE: nan\n",
      "Epoch 3/40 - RMSE: nan\n",
      "Epoch 4/40 - RMSE: nan\n",
      "Epoch 5/40 - RMSE: nan\n",
      "Epoch 6/40 - RMSE: nan\n",
      "Epoch 7/40 - RMSE: nan\n",
      "Epoch 8/40 - RMSE: nan\n",
      "Epoch 9/40 - RMSE: nan\n",
      "Epoch 10/40 - RMSE: nan\n",
      "Epoch 11/40 - RMSE: nan\n",
      "Epoch 12/40 - RMSE: nan\n",
      "Epoch 13/40 - RMSE: nan\n",
      "Epoch 14/40 - RMSE: nan\n",
      "Epoch 15/40 - RMSE: nan\n",
      "Epoch 16/40 - RMSE: nan\n",
      "Epoch 17/40 - RMSE: nan\n",
      "Epoch 18/40 - RMSE: nan\n",
      "Epoch 19/40 - RMSE: nan\n",
      "Epoch 20/40 - RMSE: nan\n",
      "Epoch 21/40 - RMSE: nan\n",
      "Epoch 22/40 - RMSE: nan\n",
      "Epoch 23/40 - RMSE: nan\n",
      "Epoch 24/40 - RMSE: nan\n",
      "Epoch 25/40 - RMSE: nan\n",
      "Epoch 26/40 - RMSE: nan\n",
      "Epoch 27/40 - RMSE: nan\n",
      "Epoch 28/40 - RMSE: nan\n",
      "Epoch 29/40 - RMSE: nan\n",
      "Epoch 30/40 - RMSE: nan\n",
      "Epoch 31/40 - RMSE: nan\n",
      "Epoch 32/40 - RMSE: nan\n",
      "Epoch 33/40 - RMSE: nan\n",
      "Epoch 34/40 - RMSE: nan\n",
      "Epoch 35/40 - RMSE: nan\n",
      "Epoch 36/40 - RMSE: nan\n",
      "Epoch 37/40 - RMSE: nan\n",
      "Epoch 38/40 - RMSE: nan\n",
      "Epoch 39/40 - RMSE: nan\n",
      "Epoch 40/40 - RMSE: nan\n",
      "(factors=100, lr=0.2, reg=0.1, ep=40) - RMSE=nan\n",
      "Epoch 1/50 - RMSE: nan\n",
      "Epoch 2/50 - RMSE: nan\n",
      "Epoch 3/50 - RMSE: nan\n",
      "Epoch 4/50 - RMSE: nan\n",
      "Epoch 5/50 - RMSE: nan\n",
      "Epoch 6/50 - RMSE: nan\n",
      "Epoch 7/50 - RMSE: nan\n",
      "Epoch 8/50 - RMSE: nan\n",
      "Epoch 9/50 - RMSE: nan\n",
      "Epoch 10/50 - RMSE: nan\n",
      "Epoch 11/50 - RMSE: nan\n",
      "Epoch 12/50 - RMSE: nan\n",
      "Epoch 13/50 - RMSE: nan\n",
      "Epoch 14/50 - RMSE: nan\n",
      "Epoch 15/50 - RMSE: nan\n",
      "Epoch 16/50 - RMSE: nan\n",
      "Epoch 17/50 - RMSE: nan\n",
      "Epoch 18/50 - RMSE: nan\n",
      "Epoch 19/50 - RMSE: nan\n",
      "Epoch 20/50 - RMSE: nan\n",
      "Epoch 21/50 - RMSE: nan\n",
      "Epoch 22/50 - RMSE: nan\n",
      "Epoch 23/50 - RMSE: nan\n",
      "Epoch 24/50 - RMSE: nan\n",
      "Epoch 25/50 - RMSE: nan\n",
      "Epoch 26/50 - RMSE: nan\n",
      "Epoch 27/50 - RMSE: nan\n",
      "Epoch 28/50 - RMSE: nan\n",
      "Epoch 29/50 - RMSE: nan\n",
      "Epoch 30/50 - RMSE: nan\n",
      "Epoch 31/50 - RMSE: nan\n",
      "Epoch 32/50 - RMSE: nan\n",
      "Epoch 33/50 - RMSE: nan\n",
      "Epoch 34/50 - RMSE: nan\n",
      "Epoch 35/50 - RMSE: nan\n",
      "Epoch 36/50 - RMSE: nan\n",
      "Epoch 37/50 - RMSE: nan\n",
      "Epoch 38/50 - RMSE: nan\n",
      "Epoch 39/50 - RMSE: nan\n",
      "Epoch 40/50 - RMSE: nan\n",
      "Epoch 41/50 - RMSE: nan\n",
      "Epoch 42/50 - RMSE: nan\n",
      "Epoch 43/50 - RMSE: nan\n",
      "Epoch 44/50 - RMSE: nan\n",
      "Epoch 45/50 - RMSE: nan\n",
      "Epoch 46/50 - RMSE: nan\n",
      "Epoch 47/50 - RMSE: nan\n",
      "Epoch 48/50 - RMSE: nan\n",
      "Epoch 49/50 - RMSE: nan\n",
      "Epoch 50/50 - RMSE: nan\n",
      "(factors=100, lr=0.2, reg=0.1, ep=50) - RMSE=nan\n",
      "\n",
      "Best matrix factorization RMSE: 0.9232255060327957\n",
      "Best setting: {'n_factors': 100, 'lr': 0.01, 'reg': 0.1, 'ep': 40}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "hybrid_prediction() missing 2 required positional arguments: 'item_k' and 'mf_setting'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 109\u001b[0m\n\u001b[0;32m    105\u001b[0m best_rmse_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatrixFactorization\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m best_rmse\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Hybrid Model\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m hybrid_pred \u001b[38;5;241m=\u001b[39m \u001b[43mhybrid_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_cb_setting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_user_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_item_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_mf_setting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m rmse_hybrid \u001b[38;5;241m=\u001b[39m RMSE(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m], hybrid_pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHybrid_pred\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHybrid model RMSE: \u001b[39m\u001b[38;5;124m'\u001b[39m, rmse_hybrid)\n",
      "\u001b[1;31mTypeError\u001b[0m: hybrid_prediction() missing 2 required positional arguments: 'item_k' and 'mf_setting'"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "def MSE(actual_rating, pred_rating):\n",
    "    \"\"\"\n",
    "    Calculate the mean squared error (MSE) between actual ratings and predicted ratings where both arguments are arrays.\n",
    "    \"\"\"\n",
    "    result = np.mean((actual_rating - pred_rating) ** 2)\n",
    "\n",
    "    return result\n",
    "\n",
    "def RMSE(actual_rating, pred_rating):\n",
    "    \"\"\"\n",
    "    Calculate the root mean squared error (RMSE) between actual ratings and predicted ratings where both arguments are arrays.\n",
    "    \"\"\"\n",
    "    result = np.sqrt(MSE(actual_rating, pred_rating))\n",
    "\n",
    "    return result\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "\n",
    "# Content-based Model\n",
    "content_types = ['title_genres', 'description', 'full']\n",
    "aggregation_methods = ['avg', 'weighted_avg', 'avg_pos']\n",
    "\n",
    "best_rmse_dict = {}\n",
    "\n",
    "# Content-based\n",
    "best_rmse = float('inf')\n",
    "best_cb_setting = {}\n",
    "\n",
    "for ctype in content_types:\n",
    "    for method in aggregation_methods:\n",
    "        predictions = content_based_rating_prediction(train_data, test_data, ctype, method)\n",
    "        rmse_cb = RMSE(predictions['rating'], predictions[f'CB_pred_{ctype}_{method}'])\n",
    "        print(f\"({ctype}, {method}) - RMSE={rmse_cb}\")\n",
    "\n",
    "        if rmse_cb < best_rmse:\n",
    "            best_rmse = rmse_cb\n",
    "            best_cb_setting = {'content_type': ctype, 'method': method}\n",
    "\n",
    "print(\"\\nBest content-based RMSE:\", best_rmse)\n",
    "print(\"Best setting:\", best_cb_setting, \"\\n\")\n",
    "best_rmse_dict['Content-Based'] = best_rmse\n",
    "\n",
    "\n",
    "# User-based KNN\n",
    "best_rmse = float('inf')\n",
    "best_user_k = None\n",
    "\n",
    "for k in [5, 10, 20, 30, 40, 50]:\n",
    "    predictions = user_based_rating_prediction(train_data, user_similarity_matrix, test_data, k)\n",
    "    rmse_user = RMSE(predictions['rating'], predictions[f'UserKNN_pred_{k}'])\n",
    "    print(f\"k={k} - RMSE={rmse_user}\")\n",
    "\n",
    "    if rmse_user < best_rmse:\n",
    "        best_rmse = rmse_user\n",
    "        best_user_k = k\n",
    "\n",
    "print(f\"\\nBest user-based KNN RMSE: {best_rmse} at k={best_user_k}\\n\")\n",
    "best_rmse_dict['UserKNN'] = best_rmse\n",
    "\n",
    "\n",
    "# Item-based KNN\n",
    "best_rmse = float('inf')\n",
    "best_item_k = None\n",
    "\n",
    "for k in [5, 10, 20, 25, 30, 40, 50, 100]:\n",
    "    predictions = item_based_rating_prediction(train_data, item_similarity_matrix, test_data, k)\n",
    "    rmse_item = RMSE(predictions['rating'], predictions[f'ItemKNN_pred_{k}'])\n",
    "    print(f\"k={k} - RMSE={rmse_item}\")\n",
    "\n",
    "    if rmse_item < best_rmse:\n",
    "        best_rmse = rmse_item\n",
    "        best_item_k = k\n",
    "\n",
    "print(f\"\\nBest item-based KNN RMSE: {best_rmse} at k={best_item_k}\\n\")\n",
    "best_rmse_dict['ItemKNN'] = best_rmse\n",
    "\n",
    "\n",
    "# Matrix Factorization\n",
    "best_rmse = float('inf')\n",
    "best_mf_setting = {}\n",
    "\n",
    "for factors in [20, 50, 100]:\n",
    "    for lr in [0.001, 0.005, 0.01, 0.2]:\n",
    "        for reg in [0.001, 0.01, 0.1]:\n",
    "            for ep in [10, 20, 30, 40, 50]:\n",
    "                mf = MatrixFactorizationSGD(\n",
    "                    n_factors=factors,\n",
    "                    learning_rate=lr,\n",
    "                    regularization=reg,\n",
    "                    n_epochs=ep\n",
    "                )\n",
    "                mf.fit(train_data)\n",
    "                predictions = mf_rating_prediction(mf, test_data)\n",
    "                \n",
    "                rmse_mf = RMSE(predictions['rating'], predictions[f'MF_pred_{factors}_{lr}_{reg}_{ep}'])\n",
    "                print(f\"(factors={factors}, lr={lr}, reg={reg}, ep={ep}) - RMSE={rmse_mf}\")\n",
    "\n",
    "                if rmse_mf < best_rmse:\n",
    "                    best_rmse = rmse_mf\n",
    "                    best_mf_setting = {'n_factors': factors, 'lr': lr, 'reg': reg, 'ep': ep}\n",
    "\n",
    "print(\"\\nBest matrix factorization RMSE:\", best_rmse)\n",
    "print(\"Best setting:\", best_mf_setting)\n",
    "best_rmse_dict['MatrixFactorization'] = best_rmse\n",
    "\n",
    "\n",
    "# Hybrid Model\n",
    "hybrid_pred = hybrid_prediction(train_data, test_data, best_cb_setting, best_user_k, best_item_k, best_mf_setting)\n",
    "rmse_hybrid = RMSE(test_data['rating'], hybrid_pred['Hybrid_pred'])\n",
    "print('\\nHybrid model RMSE: ', rmse_hybrid)\n",
    "best_rmse_dict['Hybrid'] = rmse_hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878099cc",
   "metadata": {},
   "source": [
    "### B. Ranking Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440a0be",
   "metadata": {},
   "source": [
    "#### B.1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72c8ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "def Precision(ground_truth, rec_list):\n",
    "    # Implement a function that computes Precision across ground truth data and recommendation list generated for each user. \n",
    "    # Note that ground_truth and rec_list contain the list of items for all users, e.g., 2-dimensional arrays.\n",
    "\n",
    "    precisions = []\n",
    "    for gt, rec in zip(ground_truth, rec_list):\n",
    "        if len(rec) == 0:\n",
    "            precisions.append(0)\n",
    "        else:\n",
    "            hit_count = len(set(gt) & set(rec))\n",
    "            precisions.append(hit_count / len(rec))\n",
    "    result = np.mean(precisions)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def Recall(ground_truth, rec_list):\n",
    "    # Implement a function that computes Recall across ground truth data and recommendation list generated for each user. \n",
    "    # Note that ground_truth and rec_list contain the list of items for all users, e.g., 2-dimensional arrays.\n",
    "\n",
    "    recalls = []\n",
    "    for gt, rec in zip(ground_truth, rec_list):\n",
    "        if len(gt) == 0:\n",
    "            recalls.append(0)\n",
    "        else:\n",
    "            hit_count = len(set(gt) & set(rec))\n",
    "            recalls.append(hit_count / len(gt))\n",
    "    result = np.mean(recalls)\n",
    "\n",
    "    return result\n",
    "\n",
    "def NDCG(ground_truth, rec_list):\n",
    "    # Implement a function that computes NDCG across ground truth data and recommendation list generated for each user. \n",
    "    # Note that ground_truth and rec_list contain the list of items for all users, e.g., 2-dimensional arrays.\n",
    "\n",
    "    ndcgs = []\n",
    "    for gt, rec in zip(ground_truth, rec_list):\n",
    "        dcg = 0.0\n",
    "        for i, item in enumerate(rec):\n",
    "            if item in gt:\n",
    "                dcg += 1.0 / np.log2(i + 2)  # log base 2, rank i+1\n",
    "        idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(gt), len(rec))))\n",
    "        ndcgs.append(dcg / idcg if idcg > 0 else 0)\n",
    "    result = np.mean(ndcgs)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9e7b6",
   "metadata": {},
   "source": [
    "#### B.2 Content Based Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dab17400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of content-based recommender for content type = title+genres and aggregation method = avg:\n",
      "Precision=0.0228, Recall=0.00424, NDCG=0.02364\n"
     ]
    }
   ],
   "source": [
    "def evaluate_content_based_ranking(train_data, test_data, content_type, aggregation_method):\n",
    "    users_emb = []\n",
    "    users = list(train_data['user_id'].unique())\n",
    "    for user in users:\n",
    "        users_emb.append(get_user_emb(train_data, user, content_type, aggregation_method))\n",
    "\n",
    "    ground_truth, rec_list = [], []\n",
    "    \n",
    "    all_items = set(train_data['item_id'].unique()) | set(test_data['item_id'].unique())\n",
    "    for idx, user in enumerate(users):\n",
    "        user_emb = users_emb[idx]\n",
    "        if user_emb is None:\n",
    "            continue\n",
    "\n",
    "        interacted_items = set(train_data[train_data['user_id'] == user]['item_id'].tolist())\n",
    "        unseen_items = list(all_items - interacted_items)\n",
    "\n",
    "        item_scores = [(item, float(np.dot(user_emb, get_item_emb(item, content_type)))) for item in unseen_items]\n",
    "\n",
    "        top_items = [item for item, score in sorted(item_scores, key=lambda x: x[1], reverse=True)[:10]]\n",
    "\n",
    "        rec_list.append(top_items)\n",
    "\n",
    "        gt_items = test_data[test_data['user_id'] == user]['item_id'].tolist()\n",
    "        ground_truth.append(gt_items)\n",
    "\n",
    "    precision_value, recall_value, ndcg_value = 0.0, 0.0, 0.0\n",
    "\n",
    "    precision_value = Precision(ground_truth, rec_list)\n",
    "    recall_value = Recall(ground_truth, rec_list)\n",
    "    ndcg_value = NDCG(ground_truth, rec_list)\n",
    "\n",
    "\n",
    "    return precision_value, recall_value, ndcg_value\n",
    "\n",
    "print('Performance of content-based recommender for content type = title+genres and aggregation method = avg:')\n",
    "precision_value, recall_value, ndcg_value = evaluate_content_based_ranking(train_data, test_data, 'title_genres', 'avg')\n",
    "print('Precision='+str(round(precision_value,5))+', Recall='+str(round(recall_value,5))+', NDCG='+str(round(ndcg_value,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17906f63",
   "metadata": {},
   "source": [
    "#### B.3 User-Based Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21a521d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed in 71.62 seconds.\n",
      "Precision@10: 0.0688\n",
      "Recall@10:    0.0136\n",
      "NDCG@10:      0.0724\n",
      "{'Precision': np.float64(0.06884531590413943), 'Recall': np.float64(0.013629265165165418), 'NDCG': np.float64(0.07235461731810341)}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_user_based_ranking(train_data, test_data, user_similarity_matrix, k=30, top_n=10):\n",
    "    \"\"\"\n",
    "    Evaluate user-based recommender across all users using Precision, Recall, and NDCG.\n",
    "    \n",
    "    Args:\n",
    "        train_data (pd.DataFrame): training set with [user_id, item_id, rating]\n",
    "        test_data (pd.DataFrame): test set with [user_id, item_id, rating]\n",
    "        user_similarity_matrix (pd.DataFrame): precomputed user-user similarity matrix\n",
    "        k (int): number of nearest neighbors\n",
    "        top_n (int): number of top items to recommend per user\n",
    "    \n",
    "    Returns:\n",
    "        dict: containing Precision, Recall, and NDCG values\n",
    "    \"\"\"\n",
    "    all_users = test_data['user_id'].unique()\n",
    "    ground_truth = []\n",
    "    rec_lists = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for user in all_users:\n",
    "        # Ground truth items (actually rated by the user in test data)\n",
    "        gt_items = test_data[test_data['user_id'] == user]['item_id'].tolist()\n",
    "        ground_truth.append(gt_items)\n",
    "\n",
    "        # Recommended items\n",
    "        recs = recommend_topk_user_based(train_data, user_similarity_matrix, user, k, top_n)\n",
    "        rec_items = [item for item, _ in recs][:top_n]\n",
    "        rec_lists.append(rec_items)\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = Precision(ground_truth, rec_lists)\n",
    "    recall = Recall(ground_truth, rec_lists)\n",
    "    ndcg = NDCG(ground_truth, rec_lists)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Evaluation completed in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Precision@{top_n}: {precision:.4f}\")\n",
    "    print(f\"Recall@{top_n}:    {recall:.4f}\")\n",
    "    print(f\"NDCG@{top_n}:      {ndcg:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'NDCG': ndcg\n",
    "    }\n",
    "\n",
    "results_user_based = evaluate_user_based_ranking(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    user_similarity_matrix=user_similarity_matrix,\n",
    "    k=30,\n",
    "    top_n=10\n",
    ")\n",
    "print(results_user_based)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664f6e6",
   "metadata": {},
   "source": [
    "#### B.4 Item-Based Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b243211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed in 9.68 seconds.\n",
      "Precision@10: 0.0231\n",
      "Recall@10:    0.0036\n",
      "NDCG@10:      0.0195\n",
      "{'Precision': np.float64(0.02309368191721133), 'Recall': np.float64(0.0035951155267602837), 'NDCG': np.float64(0.019475822073115684)}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_item_based_ranking(train_data, test_data, item_similarity_matrix, k=50, top_n=10):\n",
    "    \"\"\"\n",
    "    Evaluate item-based recommender using Precision, Recall, and NDCG.\n",
    "    \n",
    "    Args:\n",
    "        train_data (pd.DataFrame): training data [user_id, item_id, rating]\n",
    "        test_data (pd.DataFrame): test data [user_id, item_id, rating]\n",
    "        item_similarity_matrix (pd.DataFrame): item-item similarity matrix\n",
    "        k (int): number of neighbors for item-based CF\n",
    "        top_n (int): number of top recommendations per user\n",
    "\n",
    "    Returns:\n",
    "        dict: {'Precision': ..., 'Recall': ..., 'NDCG': ...}\n",
    "    \"\"\"\n",
    "    all_users = test_data['user_id'].unique()\n",
    "    ground_truth, rec_lists = [], []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for user in all_users:\n",
    "        gt_items = test_data[test_data['user_id'] == user]['item_id'].tolist()\n",
    "        ground_truth.append(gt_items)\n",
    "\n",
    "        recs = recommend_topk_item_based(train_data, item_similarity_matrix, user, k, top_n)\n",
    "        rec_items = [item for item, _ in recs]\n",
    "        rec_lists.append(rec_items)\n",
    "\n",
    "    precision = Precision(ground_truth, rec_lists)\n",
    "    recall = Recall(ground_truth, rec_lists)\n",
    "    ndcg = NDCG(ground_truth, rec_lists)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Evaluation completed in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Precision@{top_n}: {precision:.4f}\")\n",
    "    print(f\"Recall@{top_n}:    {recall:.4f}\")\n",
    "    print(f\"NDCG@{top_n}:      {ndcg:.4f}\")\n",
    "\n",
    "    return {'Precision': precision, 'Recall': recall, 'NDCG': ndcg}\n",
    "\n",
    "results_item_based = evaluate_item_based_ranking(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    item_similarity_matrix=item_similarity_matrix,\n",
    "    k=50,\n",
    "    top_n=10\n",
    ")\n",
    "print(results_item_based)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa623a",
   "metadata": {},
   "source": [
    "#### B.5 Matrix Factorization Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d27701b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - RMSE: 1.0179\n",
      "Epoch 2/10 - RMSE: 0.9497\n",
      "Epoch 3/10 - RMSE: 0.9263\n",
      "Epoch 4/10 - RMSE: 0.9104\n",
      "Epoch 5/10 - RMSE: 0.8960\n",
      "Epoch 6/10 - RMSE: 0.8813\n",
      "Epoch 7/10 - RMSE: 0.8648\n",
      "Epoch 8/10 - RMSE: 0.8458\n",
      "Epoch 9/10 - RMSE: 0.8243\n",
      "Epoch 10/10 - RMSE: 0.8007\n",
      "[MF Evaluation] Completed in 1.27 seconds.\n",
      "Precision@10: 0.1355\n",
      "Recall@10:    0.0283\n",
      "NDCG@10:      0.1385\n"
     ]
    }
   ],
   "source": [
    "def evaluate_mf_ranking(model, train_data, test_data, top_n=10):\n",
    "    \"\"\"\n",
    "    Evaluate a Matrix Factorization (SGD) model using Precision, Recall, and NDCG.\n",
    "    \n",
    "    Args:\n",
    "        model (MatrixFactorizationSGD): trained model instance\n",
    "        train_data (pd.DataFrame): training ratings [user_id, item_id, rating]\n",
    "        test_data (pd.DataFrame): test ratings [user_id, item_id, rating]\n",
    "        top_n (int): number of top recommendations per user\n",
    "\n",
    "    Returns:\n",
    "        dict: {'Precision': ..., 'Recall': ..., 'NDCG': ...}\n",
    "    \"\"\"\n",
    "    all_users = test_data['user_id'].unique()\n",
    "    ground_truth, rec_lists = [], []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for user in all_users:\n",
    "        # Ground truth = items user actually interacted with in test set\n",
    "        gt_items = test_data[test_data['user_id'] == user]['item_id'].tolist()\n",
    "        ground_truth.append(gt_items)\n",
    "\n",
    "        # Get top-N recommendations from the model\n",
    "        recs = model.recommend_topk(user, train_data=train_data, n=top_n, exclude_seen=True)\n",
    "        rec_items = [item for item, _ in recs]\n",
    "        rec_lists.append(rec_items)\n",
    "\n",
    "    precision = Precision(ground_truth, rec_lists)\n",
    "    recall = Recall(ground_truth, rec_lists)\n",
    "    ndcg = NDCG(ground_truth, rec_lists)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"[MF Evaluation] Completed in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Precision@{top_n}: {precision:.4f}\")\n",
    "    print(f\"Recall@{top_n}:    {recall:.4f}\")\n",
    "    print(f\"NDCG@{top_n}:      {ndcg:.4f}\")\n",
    "\n",
    "    return {'Precision': precision, 'Recall': recall, 'NDCG': ndcg}\n",
    "\n",
    "mf_model = MatrixFactorizationSGD(n_factors=50, n_epochs=10).fit(train_data)\n",
    "mf_results = evaluate_mf_ranking(mf_model, train_data, test_data, top_n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2a348d",
   "metadata": {},
   "source": [
    "#### B.6 BPR Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f7dcb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.6914\n",
      "Epoch 2/10 - Loss: 0.6850\n",
      "Epoch 3/10 - Loss: 0.6749\n",
      "Epoch 4/10 - Loss: 0.6547\n",
      "Epoch 5/10 - Loss: 0.6156\n",
      "Epoch 6/10 - Loss: 0.5590\n",
      "Epoch 7/10 - Loss: 0.4976\n",
      "Epoch 8/10 - Loss: 0.4407\n",
      "Epoch 9/10 - Loss: 0.3931\n",
      "Epoch 10/10 - Loss: 0.3552\n",
      "[BPR Evaluation] Completed in 1.40 seconds.\n",
      "Precision@10: 0.3377\n",
      "Recall@10:    0.1347\n",
      "NDCG@10:      0.3803\n"
     ]
    }
   ],
   "source": [
    "def evaluate_bpr_ranking(model, train_data, test_data, top_n=10):\n",
    "    \"\"\"\n",
    "    Evaluate a Bayesian Personalized Ranking (BPR) model using Precision, Recall, and NDCG.\n",
    "    \n",
    "    Args:\n",
    "        model (BayesianPersonalizedRanking): trained model\n",
    "        train_data (pd.DataFrame): training data [user_id, item_id]\n",
    "        test_data (pd.DataFrame): test data [user_id, item_id]\n",
    "        top_n (int): number of top recommendations per user\n",
    "\n",
    "    Returns:\n",
    "        dict: {'Precision': ..., 'Recall': ..., 'NDCG': ...}\n",
    "    \"\"\"\n",
    "    all_users = test_data['user_id'].unique()\n",
    "    ground_truth, rec_lists = [], []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for user in all_users:\n",
    "        gt_items = test_data[test_data['user_id'] == user]['item_id'].tolist()\n",
    "        ground_truth.append(gt_items)\n",
    "\n",
    "        recs = model.recommend_topk(user, train_data=train_data, k=top_n)\n",
    "        rec_items = [item for item, _ in recs]\n",
    "        rec_lists.append(rec_items)\n",
    "\n",
    "    precision = Precision(ground_truth, rec_lists)\n",
    "    recall = Recall(ground_truth, rec_lists)\n",
    "    ndcg = NDCG(ground_truth, rec_lists)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"[BPR Evaluation] Completed in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Precision@{top_n}: {precision:.4f}\")\n",
    "    print(f\"Recall@{top_n}:    {recall:.4f}\")\n",
    "    print(f\"NDCG@{top_n}:      {ndcg:.4f}\")\n",
    "\n",
    "    return {'Precision': precision, 'Recall': recall, 'NDCG': ndcg}\n",
    "\n",
    "bpr_model = BayesianPersonalizedRanking(n_factors=50, n_epochs=10)\n",
    "bpr_model.fit(train_data)\n",
    "bpr_results = evaluate_bpr_ranking(bpr_model, train_data, test_data, top_n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a758e",
   "metadata": {},
   "source": [
    " #### B.7 Hyperparameter Tuning Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812eba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Content-Based Ranking Evaluation ===\n",
      "Content (title_genres, avg) -> Precision=0.0228, Recall=0.0042, NDCG=0.0236\n",
      "Content (title_genres, weighted_avg) -> Precision=0.0229, Recall=0.0044, NDCG=0.0240\n",
      "Content (title_genres, avg_pos) -> Precision=0.0235, Recall=0.0046, NDCG=0.0242\n",
      "Content (description, avg) -> Precision=0.0609, Recall=0.0157, NDCG=0.0578\n",
      "Content (description, weighted_avg) -> Precision=0.0589, Recall=0.0152, NDCG=0.0564\n",
      "Content (description, avg_pos) -> Precision=0.0558, Recall=0.0144, NDCG=0.0540\n",
      "Content (full, avg) -> Precision=0.0302, Recall=0.0075, NDCG=0.0311\n",
      "Content (full, weighted_avg) -> Precision=0.0302, Recall=0.0074, NDCG=0.0307\n",
      "Content (full, avg_pos) -> Precision=0.0325, Recall=0.0082, NDCG=0.0319\n",
      "best cb:  {'Precision': np.float64(0.0608695652173913), 'Recall': np.float64(0.015652084933085637), 'NDCG': np.float64(0.057784245487442557)}\n",
      "\n",
      "=== User-Based KNN Ranking Evaluation ===\n",
      "Evaluation completed in 25.38 seconds.\n",
      "Precision@10: 0.0974\n",
      "Recall@10:    0.0227\n",
      "NDCG@10:      0.1066\n",
      "User-KNN (k=5) -> Precision=0.0974, Recall=0.0227, NDCG=0.1066\n",
      "Evaluation completed in 42.94 seconds.\n",
      "Precision@10: 0.0876\n",
      "Recall@10:    0.0184\n",
      "NDCG@10:      0.0967\n",
      "User-KNN (k=10) -> Precision=0.0876, Recall=0.0184, NDCG=0.0967\n",
      "Evaluation completed in 53.71 seconds.\n",
      "Precision@10: 0.0797\n",
      "Recall@10:    0.0151\n",
      "NDCG@10:      0.0821\n",
      "User-KNN (k=20) -> Precision=0.0797, Recall=0.0151, NDCG=0.0821\n",
      "Evaluation completed in 81.78 seconds.\n",
      "Precision@10: 0.0688\n",
      "Recall@10:    0.0136\n",
      "NDCG@10:      0.0724\n",
      "User-KNN (k=30) -> Precision=0.0688, Recall=0.0136, NDCG=0.0724\n",
      "Evaluation completed in 79.04 seconds.\n",
      "Precision@10: 0.0621\n",
      "Recall@10:    0.0111\n",
      "NDCG@10:      0.0636\n",
      "User-KNN (k=40) -> Precision=0.0621, Recall=0.0111, NDCG=0.0636\n",
      "Evaluation completed in 87.19 seconds.\n",
      "Precision@10: 0.0573\n",
      "Recall@10:    0.0098\n",
      "NDCG@10:      0.0595\n",
      "User-KNN (k=50) -> Precision=0.0573, Recall=0.0098, NDCG=0.0595\n",
      "best user:  {'Precision': np.float64(0.09738562091503268), 'Recall': np.float64(0.022653504658580827), 'NDCG': np.float64(0.10662341097577405)}\n",
      "\n",
      "=== Item-Based KNN Ranking Evaluation ===\n",
      "Evaluation completed in 6.24 seconds.\n",
      "Precision@10: 0.0346\n",
      "Recall@10:    0.0081\n",
      "NDCG@10:      0.0347\n",
      "Item-KNN (k=5) -> Precision=0.0346, Recall=0.0081, NDCG=0.0347\n",
      "Evaluation completed in 6.47 seconds.\n",
      "Precision@10: 0.0327\n",
      "Recall@10:    0.0065\n",
      "NDCG@10:      0.0299\n",
      "Item-KNN (k=10) -> Precision=0.0327, Recall=0.0065, NDCG=0.0299\n",
      "Evaluation completed in 7.07 seconds.\n",
      "Precision@10: 0.0355\n",
      "Recall@10:    0.0056\n",
      "NDCG@10:      0.0340\n",
      "Item-KNN (k=20) -> Precision=0.0355, Recall=0.0056, NDCG=0.0340\n",
      "Evaluation completed in 7.44 seconds.\n",
      "Precision@10: 0.0364\n",
      "Recall@10:    0.0053\n",
      "NDCG@10:      0.0330\n",
      "Item-KNN (k=25) -> Precision=0.0364, Recall=0.0053, NDCG=0.0330\n",
      "Evaluation completed in 7.62 seconds.\n",
      "Precision@10: 0.0362\n",
      "Recall@10:    0.0051\n",
      "NDCG@10:      0.0319\n",
      "Item-KNN (k=30) -> Precision=0.0362, Recall=0.0051, NDCG=0.0319\n",
      "Evaluation completed in 8.11 seconds.\n",
      "Precision@10: 0.0296\n",
      "Recall@10:    0.0046\n",
      "NDCG@10:      0.0251\n",
      "Item-KNN (k=40) -> Precision=0.0296, Recall=0.0046, NDCG=0.0251\n",
      "Evaluation completed in 12.25 seconds.\n",
      "Precision@10: 0.0231\n",
      "Recall@10:    0.0036\n",
      "NDCG@10:      0.0195\n",
      "Item-KNN (k=50) -> Precision=0.0231, Recall=0.0036, NDCG=0.0195\n",
      "Evaluation completed in 12.16 seconds.\n",
      "Precision@10: 0.0070\n",
      "Recall@10:    0.0009\n",
      "NDCG@10:      0.0051\n",
      "Item-KNN (k=100) -> Precision=0.0070, Recall=0.0009, NDCG=0.0051\n",
      "best item:  {'Precision': np.float64(0.036383442265795214), 'Recall': np.float64(0.00809908699883602), 'NDCG': np.float64(0.03468982827413708)}\n",
      "\n",
      "=== Matrix Factorization (SGD) Ranking Evaluation ===\n",
      "Training MF with params: {'n_factors': 20, 'learning_rate': 0.001, 'regularization': 0.001, 'n_epochs': 10}\n",
      "[MF Evaluation] Completed in 0.40 seconds.\n",
      "Precision@10: 0.2024\n",
      "Recall@10:    0.0528\n",
      "NDCG@10:      0.2130\n",
      "MF ({'n_factors': 20, 'learning_rate': 0.001, 'regularization': 0.001, 'n_epochs': 10}) -> Precision=0.2024, Recall=0.0528, NDCG=0.2130\n",
      "Training MF with params: {'n_factors': 20, 'learning_rate': 0.001, 'regularization': 0.001, 'n_epochs': 20}\n",
      "[MF Evaluation] Completed in 0.47 seconds.\n",
      "Precision@10: 0.1682\n",
      "Recall@10:    0.0405\n",
      "NDCG@10:      0.1699\n",
      "MF ({'n_factors': 20, 'learning_rate': 0.001, 'regularization': 0.001, 'n_epochs': 20}) -> Precision=0.1682, Recall=0.0405, NDCG=0.1699\n",
      "Training MF with params: {'n_factors': 20, 'learning_rate': 0.001, 'regularization': 0.001, 'n_epochs': 30}\n",
      "[MF Evaluation] Completed in 0.55 seconds.\n",
      "Precision@10: 0.1564\n",
      "Recall@10:    0.0355\n",
      "NDCG@10:      0.1494\n",
      "MF ({'n_factors': 20, 'learning_rate': 0.001, 'regularization': 0.001, 'n_epochs': 30}) -> Precision=0.1564, Recall=0.0355, NDCG=0.1494\n",
      "Training MF with params: {'n_factors': 20, 'learning_rate': 0.001, 'regularization': 0.001, 'n_epochs': 40}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 99\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m---> 99\u001b[0m all_ranking_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_all_models_ranking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_similarity_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_similarity_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mitem_similarity_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem_similarity_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m    105\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[89], line 67\u001b[0m, in \u001b[0;36mevaluate_all_models_ranking\u001b[1;34m(train_data, test_data, user_similarity_matrix, item_similarity_matrix, top_n)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining MF with params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m mf \u001b[38;5;241m=\u001b[39m MatrixFactorizationSGD(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 67\u001b[0m \u001b[43mmf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_mf_ranking(mf, train_data, test_data, top_n)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMF (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) -> Precision=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, NDCG=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNDCG\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 56\u001b[0m, in \u001b[0;36mMatrixFactorizationSGD.fit\u001b[1;34m(self, ratings, verbose)\u001b[0m\n\u001b[0;32m     53\u001b[0m total_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u, i, r \u001b[38;5;129;01min\u001b[39;00m training_data:\n\u001b[1;32m---> 56\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[0;32m     58\u001b[0m         pred \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_mean \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_bias[u] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_bias[i]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_all_models_ranking(train_data, test_data, \n",
    "                                user_similarity_matrix, item_similarity_matrix,\n",
    "                                top_n=10):\n",
    "    \"\"\"\n",
    "    Evaluate all recommenders (Content, UserKNN, ItemKNN, MF, BPR) \n",
    "    for ranking metrics and print metrics per configuration.\n",
    "    Also returns best configuration per model for hybrid usage.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    best_settings = {} \n",
    "\n",
    "    content_types = ['title_genres', 'description', 'full']\n",
    "    aggregation_methods = ['avg', 'weighted_avg', 'avg_pos']\n",
    "\n",
    "    # ----- Content-based -----\n",
    "    print(\"\\n=== Content-Based Ranking Evaluation ===\")\n",
    "    best_cb = {\"Precision\": 0, \"Recall\": 0, \"NDCG\": 0}\n",
    "    best_cb_setting = None\n",
    "    for ctype in content_types:\n",
    "        for method in aggregation_methods:\n",
    "            precision, recall, ndcg = evaluate_content_based_ranking(\n",
    "                train_data, test_data, ctype, method\n",
    "            )\n",
    "            print(f\"Content ({ctype}, {method}) -> Precision={precision:.4f}, Recall={recall:.4f}, NDCG={ndcg:.4f}\")\n",
    "            for metric in [\"Precision\", \"Recall\", \"NDCG\"]:\n",
    "                if locals()[metric.lower()] > best_cb[metric]:\n",
    "                    best_cb[metric] = locals()[metric.lower()]\n",
    "                    best_cb_setting = {'content_type': ctype, 'method': method}\n",
    "    results[\"Content-Based\"] = best_cb\n",
    "    best_settings[\"Content-Based\"] = best_cb_setting\n",
    "\n",
    "    # ----- User-based KNN -----\n",
    "    print(\"\\n=== User-Based KNN Ranking Evaluation ===\")\n",
    "    best_user = {\"Precision\": 0, \"Recall\": 0, \"NDCG\": 0}\n",
    "    best_user_k = None\n",
    "    for k in [5, 10, 20, 30, 40, 50]:\n",
    "        metrics = evaluate_user_based_ranking(train_data, test_data, user_similarity_matrix, k=k, top_n=top_n)\n",
    "        print(f\"User-KNN (k={k}) -> Precision={metrics['Precision']:.4f}, Recall={metrics['Recall']:.4f}, NDCG={metrics['NDCG']:.4f}\")\n",
    "        for metric in metrics:\n",
    "            if metrics[metric] > best_user[metric]:\n",
    "                best_user[metric] = metrics[metric]\n",
    "                best_user_k = k\n",
    "    results[\"User-KNN\"] = best_user\n",
    "    best_settings[\"User-KNN\"] = best_user_k\n",
    "\n",
    "    # ----- Item-based KNN -----\n",
    "    print(\"\\n=== Item-Based KNN Ranking Evaluation ===\")\n",
    "    best_item = {\"Precision\": 0, \"Recall\": 0, \"NDCG\": 0}\n",
    "    best_item_k = None\n",
    "    for k in [5, 10, 20, 25, 30, 40, 50, 100]:\n",
    "        metrics = evaluate_item_based_ranking(train_data, test_data, item_similarity_matrix, k=k, top_n=top_n)\n",
    "        print(f\"Item-KNN (k={k}) -> Precision={metrics['Precision']:.4f}, Recall={metrics['Recall']:.4f}, NDCG={metrics['NDCG']:.4f}\")\n",
    "        for metric in metrics:\n",
    "            if metrics[metric] > best_item[metric]:\n",
    "                best_item[metric] = metrics[metric]\n",
    "                best_item_k = k\n",
    "    results[\"Item-KNN\"] = best_item\n",
    "    best_settings[\"Item-KNN\"] = best_item_k\n",
    "\n",
    "    # ----- Matrix Factorization -----\n",
    "    print(\"\\n=== Matrix Factorization (SGD) Ranking Evaluation ===\")\n",
    "    best_mf = {\"Precision\": 0, \"Recall\": 0, \"NDCG\": 0}\n",
    "    best_mf_setting = None\n",
    "    for factors in [20, 50, 100]:\n",
    "        for lr in [0.001, 0.005, 0.01, 0.2]:\n",
    "            for reg in [0.001, 0.01, 0.1]:\n",
    "                for ep in [10, 20, 30, 40, 50]:\n",
    "                    params = {'n_factors': factors, 'learning_rate': lr, 'regularization': reg, 'n_epochs': ep}\n",
    "                    print(f\"Training MF with params: {params}\")\n",
    "                    mf = MatrixFactorizationSGD(**params)\n",
    "                    mf.fit(train_data, verbose=False)\n",
    "                    metrics = evaluate_mf_ranking(mf, train_data, test_data, top_n)\n",
    "                    print(f\"MF ({params}) -> Precision={metrics['Precision']:.4f}, Recall={metrics['Recall']:.4f}, NDCG={metrics['NDCG']:.4f}\")\n",
    "                    for metric in metrics:\n",
    "                        if metrics[metric] > best_mf[metric]:\n",
    "                            best_mf[metric] = metrics[metric]\n",
    "                            best_mf_setting = params\n",
    "    results[\"MF\"] = best_mf\n",
    "    best_settings[\"MF\"] = best_mf_setting\n",
    "\n",
    "    # ----- BPR -----\n",
    "    print(\"\\n=== Bayesian Personalized Ranking (BPR) Evaluation ===\")\n",
    "    best_bpr = {\"Precision\": 0, \"Recall\": 0, \"NDCG\": 0}\n",
    "    best_bpr_setting = None\n",
    "    for factors in [20, 50, 100]:\n",
    "        for lr in [0.001, 0.005, 0.01, 0.2]:\n",
    "            for reg in [0.001, 0.01, 0.1]:\n",
    "                for ep in [10, 20, 30, 40, 50]:\n",
    "                    params = {'n_factors': factors, 'learning_rate': lr, 'regularization': reg, 'n_epochs': ep}\n",
    "                    print(f\"Training BPR with params: {params}\")\n",
    "                    bpr = BayesianPersonalizedRanking(**params)\n",
    "                    bpr.fit(train_data, verbose=False)\n",
    "                    metrics = evaluate_bpr_ranking(bpr, train_data, test_data, top_n)\n",
    "                    print(f\"BPR ({params}) -> Precision={metrics['Precision']:.4f}, Recall={metrics['Recall']:.4f}, NDCG={metrics['NDCG']:.4f}\")\n",
    "                    for metric in metrics:\n",
    "                        if metrics[metric] > best_bpr[metric]:\n",
    "                            best_bpr[metric] = metrics[metric]\n",
    "                            best_bpr_setting = params\n",
    "    results[\"BPR\"] = best_bpr\n",
    "    best_settings[\"BPR\"] = best_bpr_setting\n",
    "\n",
    "    # ----- Summary -----\n",
    "    print(\"\\n===== BEST RANKING METRICS PER MODEL =====\")\n",
    "    for model, scores in results.items():\n",
    "        print(f\"{model}: {scores}\")\n",
    "    print(\"\\n===== BEST SETTINGS PER MODEL =====\")\n",
    "    for model, setting in best_settings.items():\n",
    "        print(f\"{model}: {setting}\")\n",
    "\n",
    "    return results, best_settings\n",
    "\n",
    "\n",
    "all_ranking_results, best_settings = evaluate_all_models_ranking(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    user_similarity_matrix=user_similarity_matrix,\n",
    "    item_similarity_matrix=item_similarity_matrix,\n",
    "    top_n=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3a3e5-adef-4144-b7ad-f5f55696972d",
   "metadata": {},
   "source": [
    "# Task 3) Implement baselines for both rating prediction and ranking tasks, and perform experiments with those baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be29653",
   "metadata": {},
   "source": [
    "### A. Prediction Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc5710b4-8bae-42f0-8990-41e7cec7433f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 RMSE Summary:\n",
      "Content-Based: 1.2151\n",
      "UserKNN: 0.9849\n",
      "ItemKNN: 1.0560\n",
      "MatrixFactorization: 0.9232\n",
      "Hybrid: 0.9375\n",
      "ItemAvgBaseline: 1.0334\n",
      "MeanHybridBaseline: 0.9930\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_item_average(train_data, test_data):\n",
    "    item_avg = train_data.groupby('item_id')['rating'].mean()\n",
    "    global_mean = train_data['rating'].mean()\n",
    "    preds = test_data['item_id'].map(item_avg).fillna(global_mean)\n",
    "    return preds\n",
    "\n",
    "def predict_mean_hybrid(test_data, \n",
    "                        best_cb_setting, \n",
    "                        best_user_k, \n",
    "                        best_item_k, \n",
    "                        best_mf_setting):\n",
    "    \"\"\"\n",
    "    Compute mean hybrid predictions for test_data using the best components.\n",
    "    \n",
    "    Args:\n",
    "        test_data: pd.DataFrame containing all prediction columns\n",
    "        best_cb_setting: dict with 'content_type' and 'method'\n",
    "        best_user_k: int, best k for user-based KNN\n",
    "        best_item_k: int, best k for item-based KNN\n",
    "        best_mf_setting: dict with n_factors, lr, reg, ep\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series with hybrid predicted ratings for test_data\n",
    "    \"\"\"\n",
    "    cb_col = f\"CB_pred_{best_cb_setting['content_type']}_{best_cb_setting['method']}\"\n",
    "    user_col = f\"UserKNN_pred_{best_user_k}\"\n",
    "    item_col = f\"ItemKNN_pred_{best_item_k}\"\n",
    "    mf_col = f\"MF_pred_{best_mf_setting['n_factors']}_{best_mf_setting['lr']}_{best_mf_setting['reg']}_{best_mf_setting['ep']}\"\n",
    "\n",
    "    # Fill missing values with global mean\n",
    "    global_mean = test_data['rating'].mean()\n",
    "    for col in [cb_col, user_col, item_col, mf_col]:\n",
    "        test_data[col] = test_data[col].fillna(global_mean)\n",
    "\n",
    "    # Compute mean across the best predictions\n",
    "    test_data['Mean_Hybrid_pred'] = test_data[[cb_col, user_col, item_col, mf_col]].mean(axis=1)\n",
    "\n",
    "    return test_data['Mean_Hybrid_pred']\n",
    "\n",
    "\n",
    "item_avg_baseline = predict_item_average(train_data, test_data)\n",
    "rmse_item_avg_baseline = RMSE(test_data['rating'], item_avg_baseline)\n",
    "\n",
    "mean_hybrid_baseline = predict_mean_hybrid(test_data, best_cb_setting, best_user_k, best_item_k, best_mf_setting)\n",
    "rmse_hybrid_baseline = RMSE(test_data['rating'], mean_hybrid_baseline)\n",
    "\n",
    "# Add to dict\n",
    "best_rmse_dict['ItemAvgBaseline'] = rmse_item_avg_baseline\n",
    "best_rmse_dict['MeanHybridBaseline'] = rmse_hybrid_baseline\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(best_rmse_dict.keys(), best_rmse_dict.values(), color='skyblue')\n",
    "plt.title('Best RMSE per Recommender Type')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 RMSE Summary:\")\n",
    "for model, rmse in best_rmse_dict.items():\n",
    "    print(f\"{model}: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe265a",
   "metadata": {},
   "source": [
    "### B. Ranking Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b696759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def evaluate_random_baseline(train_data, test_data):    \n",
    "    ground_truth, rec_list = [], []\n",
    "    \n",
    "    users = list(train_data['user_id'].unique())\n",
    "    all_items = set(train_data['item_id'].unique()) | set(test_data['item_id'].unique())\n",
    "\n",
    "    for user in users:\n",
    "        interacted_items = set(train_data[train_data['user_id'] == user]['item_id'].tolist())\n",
    "        unseen_items = list(all_items - interacted_items)\n",
    "        rec_items = random.sample(unseen_items, min(10, len(unseen_items)))\n",
    "\n",
    "        rec_list.append(rec_items)\n",
    "        gt_items = test_data[test_data['user_id'] == user]['item_id'].tolist()\n",
    "        ground_truth.append(gt_items)\n",
    "\n",
    "    precision_value, recall_value, ndcg_value = 0.0, 0.0, 0.0\n",
    "\n",
    "    precision_value = Precision(ground_truth, rec_list)\n",
    "    recall_value = Recall(ground_truth, rec_list)\n",
    "    ndcg_value = NDCG(ground_truth, rec_list)\n",
    "\n",
    "    return precision_value, recall_value, ndcg_value\n",
    "\n",
    "def evaluate_popular_baseline(train_data, test_data):    \n",
    "    ground_truth, rec_list = [], []\n",
    "    \n",
    "    popular_items = train_data.groupby('item_id').size().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "    users = list(train_data['user_id'].unique())\n",
    "\n",
    "    for user in users:\n",
    "        interacted_items = set(train_data[train_data['user_id'] == user]['item_id'].tolist())\n",
    "        rec_items = [item for item in popular_items if item not in interacted_items][:10]\n",
    "\n",
    "        rec_list.append(rec_items)\n",
    "        gt_items = test_data[test_data['user_id'] == user]['item_id'].tolist()\n",
    "        ground_truth.append(gt_items)\n",
    "\n",
    "    precision_value, recall_value, ndcg_value = 0.0, 0.0, 0.0\n",
    "\n",
    "    precision_value = Precision(ground_truth, rec_list)\n",
    "    recall_value = Recall(ground_truth, rec_list)\n",
    "    ndcg_value = NDCG(ground_truth, rec_list)\n",
    "\n",
    "    return precision_value, recall_value, ndcg_value\n",
    "\n",
    "def evaluate_mean_hybrid(test_data, \n",
    "                         best_cb_setting, \n",
    "                         best_user_k, \n",
    "                         best_item_k, \n",
    "                         best_mf_setting,\n",
    "                         best_bpr_setting,\n",
    "                         top_n=10):\n",
    "    \"\"\"\n",
    "    Compute top-N recommendations for mean hybrid baseline by averaging relevance scores\n",
    "    from best-performing models and evaluate Precision, Recall, and NDCG.\n",
    "    \"\"\"\n",
    "    all_users = test_data['user_id'].unique()\n",
    "    ground_truth, rec_lists = [], []\n",
    "\n",
    "    # Build column names for best settings\n",
    "    cb_col = f\"CB_pred_{best_cb_setting['content_type']}_{best_cb_setting['method']}\"\n",
    "    user_col = f\"UserKNN_pred_{best_user_k}\"\n",
    "    item_col = f\"ItemKNN_pred_{best_item_k}\"\n",
    "    mf_col = f\"MF_pred_{best_mf_setting['n_factors']}_{best_mf_setting['learning_rate']}_{best_mf_setting['regularization']}_{best_mf_setting['n_epochs']}\"\n",
    "    bpr_col = f\"BPR_pred_{best_bpr_setting['n_factors']}_{best_bpr_setting['learning_rate']}_{best_bpr_setting['regularization']}_{best_bpr_setting['n_epochs']}\"\n",
    "\n",
    "    for user in all_users:\n",
    "        ground_items = test_data[test_data['user_id'] == user]['item_id'].tolist()\n",
    "        ground_truth.append(ground_items)\n",
    "\n",
    "        # Get all items for this user\n",
    "        user_items = test_data[test_data['user_id'] == user]\n",
    "\n",
    "        # Compute mean relevance\n",
    "        user_items['mean_score'] = user_items[[cb_col, user_col, item_col, mf_col, bpr_col]].mean(axis=1)\n",
    "\n",
    "        # Top-N recommendations\n",
    "        top_items = user_items.sort_values('mean_score', ascending=False)['item_id'].tolist()[:top_n]\n",
    "        rec_lists.append(top_items)\n",
    "\n",
    "    # Compute ranking metrics\n",
    "    precision = Precision(ground_truth, rec_lists)\n",
    "    recall = Recall(ground_truth, rec_lists)\n",
    "    ndcg = NDCG(ground_truth, rec_lists)\n",
    "\n",
    "    print(f\"Mean Hybrid -> Precision={precision:.4f}, Recall={recall:.4f}, NDCG={ndcg:.4f}\")\n",
    "    return {'Precision': precision, 'Recall': recall, 'NDCG': ndcg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bed04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Baselines ===\n",
      "Random Baseline -> Precision=0.0176, Recall=0.0042, NDCG=0.0170\n",
      "Popular Baseline -> Precision=0.1479, Recall=0.0474, NDCG=0.1581\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_mean_hybrid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPopular Baseline -> Precision=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpop_precision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpop_recall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, NDCG=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpop_ndcg\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Mean hybrid baseline\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m mean_hybrid_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_mean_hybrid\u001b[49m(\n\u001b[0;32m     18\u001b[0m     test_data\u001b[38;5;241m=\u001b[39mtest_data,\n\u001b[0;32m     19\u001b[0m     best_cb_setting\u001b[38;5;241m=\u001b[39mbest_settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Based\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     20\u001b[0m     best_user_k\u001b[38;5;241m=\u001b[39mbest_settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-KNN\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     21\u001b[0m     best_item_k\u001b[38;5;241m=\u001b[39mbest_settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mItem-KNN\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     22\u001b[0m     best_mf_setting\u001b[38;5;241m=\u001b[39mbest_settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMF\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     23\u001b[0m     best_bpr_setting\u001b[38;5;241m=\u001b[39mbest_settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBPR\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     24\u001b[0m     top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     26\u001b[0m results_baselines[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean-Hybrid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m mean_hybrid_metrics\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Combine baseline results with hyperparameter-tuned models\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate_mean_hybrid' is not defined"
     ]
    }
   ],
   "source": [
    "# ----- Evaluate baselines and add them to results -----\n",
    "print(\"\\n=== Evaluating Baselines ===\")\n",
    "\n",
    "# Random baseline\n",
    "rand_precision, rand_recall, rand_ndcg = evaluate_random_baseline(train_data, test_data)\n",
    "results_baselines = {\n",
    "    \"Random\": {\"Precision\": rand_precision, \"Recall\": rand_recall, \"NDCG\": rand_ndcg}\n",
    "}\n",
    "print(f\"Random Baseline -> Precision={rand_precision:.4f}, Recall={rand_recall:.4f}, NDCG={rand_ndcg:.4f}\")\n",
    "\n",
    "# Most popular baseline\n",
    "pop_precision, pop_recall, pop_ndcg = evaluate_popular_baseline(train_data, test_data)\n",
    "results_baselines[\"Popular\"] = {\"Precision\": pop_precision, \"Recall\": pop_recall, \"NDCG\": pop_ndcg}\n",
    "print(f\"Popular Baseline -> Precision={pop_precision:.4f}, Recall={pop_recall:.4f}, NDCG={pop_ndcg:.4f}\")\n",
    "\n",
    "# Mean hybrid baseline\n",
    "mean_hybrid_metrics = evaluate_mean_hybrid(\n",
    "    test_data=test_data,\n",
    "    best_cb_setting=best_settings['Content-Based'],\n",
    "    best_user_k=best_settings['User-KNN'],\n",
    "    best_item_k=best_settings['Item-KNN'],\n",
    "    best_mf_setting=best_settings['MF'],\n",
    "    best_bpr_setting=best_settings['BPR'],\n",
    "    top_n=10\n",
    ")\n",
    "results_baselines[\"Mean-Hybrid\"] = mean_hybrid_metrics\n",
    "\n",
    "# Combine baseline results with hyperparameter-tuned models\n",
    "all_ranking_results.update(results_baselines)\n",
    "\n",
    "print(\"\\n===== ALL RANKING RESULTS INCLUDING BASELINES =====\")\n",
    "for model, scores in all_ranking_results.items():\n",
    "    print(f\"{model}: {scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20fd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metrics to plot\n",
    "metrics = [\"Precision\", \"Recall\", \"NDCG\"]\n",
    "\n",
    "# Models in the order we want to show them\n",
    "models = list(all_ranking_results.keys())\n",
    "\n",
    "# Prepare data for plotting\n",
    "precision_vals = [all_ranking_results[m][\"Precision\"] for m in models]\n",
    "recall_vals = [all_ranking_results[m][\"Recall\"] for m in models]\n",
    "ndcg_vals = [all_ranking_results[m][\"NDCG\"] for m in models]\n",
    "\n",
    "x = np.arange(len(models))  # the label locations\n",
    "width = 0.25  # width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot bars\n",
    "rects1 = ax.bar(x - width, precision_vals, width, label='Precision')\n",
    "rects2 = ax.bar(x, recall_vals, width, label='Recall')\n",
    "rects3 = ax.bar(x + width, ndcg_vals, width, label='NDCG')\n",
    "\n",
    "# Labels, title, legend\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Ranking Metrics for All Models and Baselines')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Display values on top of bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0,3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10661af-16f3-41f1-b09a-0307f70c344f",
   "metadata": {},
   "source": [
    "# Task 4) Analysis of recommendation models. Analyzing the coefficients of hybrid model and the success of recommendation models for different users' groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23377855-cc19-4b06-a497-712a892ae3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9914f603-b893-471c-9622-4437855dd8fa",
   "metadata": {},
   "source": [
    "# Task 5) Evaluation of beyond accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a0e4b-adbe-4673-82f8-a75761666fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
